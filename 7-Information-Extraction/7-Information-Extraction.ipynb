{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - Information Extraction\n",
    "\n",
    "\n",
    "This week, we move from arbitrary textual classification to the use of computation and linguistic models to parse precise claims from documents. Rather than focusing on simply the *ideas* in a corpus, here we focus on understanding and extracting its precise *claims*. This process involves a sequential pipeline of classifying and structuring tokens from text, each of which generates potentially useful data for the content analyst. Steps in this process, which we examine in this notebook, include: 1) tagging words by their part of speech (POS) to reveal the linguistic role they play in the sentence (e.g., Verb, Noun, Adjective, etc.); 2) tagging words as named entities (NER) such as places or organizations; 3) structuring or \"parsing\" sentences into nested phrases that are local to, describe or depend on one another; and 4) extracting informational claims from those phrases, like the Subject-Verb-Object (SVO) triples we extract here. While much of this can be done directly in the python package NLTK that we introduced in week 2, here we use NLTK bindings to the Stanford NLP group's open software, written in Java. Try typing a sentence into the online version [here](http://nlp.stanford.edu:8080/corenlp/) to get a sense of its potential. It is superior in performance to NLTK's implementations, but takes time to run, and so for these exercises we will parse and extract information for a very small text corpus. Of course, for final projects that draw on these tools, we encourage you to install the software on your own machines or shared servers at the university (RCC, SSRC) in order to perform these operations on much more text. \n",
    "\n",
    "For this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "#For NLP\n",
    "import nltk\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "\n",
    "#Displays the graphs\n",
    "import graphviz #You also need to install the command line graphviz\n",
    "\n",
    "#These are from the standard library\n",
    "import os.path\n",
    "import zipfile\n",
    "import subprocess\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to run this _once_ to download everything, you will also need [Java 1.8+](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) if you are using Windows or MacOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting downloads, this will take 5-10 minutes\n",
      "../stanford-NLP/parser already exists, skipping download\n",
      "../stanford-NLP/ner already exists, skipping download\n",
      "../stanford-NLP/postagger already exists, skipping download\n",
      "../stanford-NLP/core already exists, skipping download\n",
      "[100%]Done setting up the Stanford NLP collection\n"
     ]
    }
   ],
   "source": [
    "lucem_illud.setupStanfordNLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have stanford-NLP setup before importing, so we are doing the import here. IF you have stanford-NLP working, you can import at the beginning like you would with any other library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/Anaconda3-5.0.0.1-el7-x86_64/lib/python3.6/site-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n",
      "/software/Anaconda3-5.0.0.1-el7-x86_64/lib/python3.6/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import lucem_illud.stanford as stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Information Extraction is a module packaged within the Stanford Core NLP package, but it is not yet supported by `nltk`. As a result, we have defining our own `lucem_illud` function that runs the Stanford Core NLP java code right here. For other projects, it is often useful to use Java or other programs (in C, C++) within a python workflow, and this is an example. `stanford.openIE()` takes in a string or list of strings and then produces as output all the subject, verb, object (SVO) triples Stanford Corenlp can find, as a DataFrame. You can do this through links to the Stanford Core NLP project that we provide here, or play with their interface directly (in the penultimate cell of this notebook), which produces data in \"pretty graphics\" like this example parsing of the first sentence in the \"Shooting of Trayvon Martin\" Wikipedia article:\n",
    "\n",
    "![Output 1](../data/stanford_core1.png)\n",
    "![Output 2](../data/stanford_core2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will illustrate these tools on some *very* short examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the elephant in my pajamas.\n",
      "The quick brown fox jumped over the lazy dog.\n",
      "While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.\n",
      "Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.\n",
      "Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\n"
     ]
    }
   ],
   "source": [
    "text = ['I saw the elephant in my pajamas.', 'The quick brown fox jumped over the lazy dog.', 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.', 'Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.', 'Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo']\n",
    "tokenized_text = [nltk.word_tokenize(t) for t in text]\n",
    "print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In POS tagging, we classify each word by its semantic role in a sentence. The Stanford POS tagger uses the [Penn Treebank tag set]('http://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports') to POS tag words from input sentences. As discussed in the second assignment, this is a relatively precise tagset, which allows more informative tags, and also more opportunities to err :-).\n",
    "\n",
    "|#. |Tag |Description |\n",
    "|---|----|------------|\n",
    "|1.\t|CC\t|Coordinating conjunction\n",
    "|2.\t|CD\t|Cardinal number\n",
    "|3.\t|DT\t|Determiner\n",
    "|4.\t|EX\t|Existential there\n",
    "|5.\t|FW\t|Foreign word\n",
    "|6.\t|IN\t|Preposition or subordinating conjunction\n",
    "|7.\t|JJ\t|Adjective\n",
    "|8.\t|JJR|\tAdjective, comparative\n",
    "|9.\t|JJS|\tAdjective, superlative\n",
    "|10.|\tLS\t|List item marker\n",
    "|11.|\tMD\t|Modal\n",
    "|12.|\tNN\t|Noun, singular or mass\n",
    "|13.|\tNNS\t|Noun, plural\n",
    "|14.|\tNNP\t|Proper noun, singular\n",
    "|15.|\tNNPS|\tProper noun, plural\n",
    "|16.|\tPDT\t|Predeterminer\n",
    "|17.|\tPOS\t|Possessive ending\n",
    "|18.|\tPRP\t|Personal pronoun\n",
    "|19.|\tPRP\\$|\tPossessive pronoun\n",
    "|20.|\tRB\t|Adverb\n",
    "|21.|\tRBR\t|Adverb, comparative\n",
    "|22.|\tRBS\t|Adverb, superlative\n",
    "|23.|\tRP\t|Particle\n",
    "|24.|\tSYM\t|Symbol\n",
    "|25.|\tTO\t|to\n",
    "|26.|\tUH\t|Interjection\n",
    "|27.|\tVB\t|Verb, base form\n",
    "|28.|\tVBD\t|Verb, past tense\n",
    "|29.|\tVBG\t|Verb, gerund or present participle\n",
    "|30.|\tVBN\t|Verb, past participle\n",
    "|31.|\tVBP\t|Verb, non-3rd person singular present\n",
    "|32.|\tVBZ\t|Verb, 3rd person singular present\n",
    "|33.|\tWDT\t|Wh-determiner\n",
    "|34.|\tWP\t|Wh-pronoun\n",
    "|35.|\tWP$\t|Possessive wh-pronoun\n",
    "|36.|\tWRB\t|Wh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('elephant', 'NN'), ('in', 'IN'), ('my', 'PRP$'), ('pajamas', 'NNS'), ('.', '.')], [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')], [('While', 'IN'), ('in', 'IN'), ('France', 'NNP'), (',', ','), ('Christine', 'NNP'), ('Lagarde', 'NNP'), ('discussed', 'VBD'), ('short-term', 'JJ'), ('stimulus', 'NN'), ('efforts', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('recent', 'JJ'), ('interview', 'NN'), ('with', 'IN'), ('the', 'DT'), ('Wall', 'NNP'), ('Street', 'NNP'), ('Journal', 'NNP'), ('.', '.')], [('Trayvon', 'NNP'), ('Benjamin', 'NNP'), ('Martin', 'NNP'), ('was', 'VBD'), ('an', 'DT'), ('African', 'NNP'), ('American', 'NNP'), ('from', 'IN'), ('Miami', 'NNP'), ('Gardens', 'NNP'), (',', ','), ('Florida', 'NNP'), (',', ','), ('who', 'WP'), (',', ','), ('at', 'IN'), ('17', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('was', 'VBD'), ('fatally', 'RB'), ('shot', 'VBN'), ('by', 'IN'), ('George', 'NNP'), ('Zimmerman', 'NNP'), (',', ','), ('a', 'DT'), ('neighborhood', 'NN'), ('watch', 'NN'), ('volunteer', 'NN'), (',', ','), ('in', 'IN'), ('Sanford', 'NNP'), (',', ','), ('Florida', 'NNP'), ('.', '.')], [('Buffalo', 'NNP'), ('buffalo', 'NN'), ('Buffalo', 'NNP'), ('buffalo', 'NN'), ('buffalo', 'NN'), ('buffalo', 'NN'), ('Buffalo', 'NNP'), ('buffalo', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "pos_sents = stanford.postTagger.tag_sents(tokenized_text)\n",
    "print(pos_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite good. Now we will try POS tagging with a somewhat larger corpus. We consider a few of the top posts from the reddit data we used last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditDF = pandas.read_csv('../data/reddit.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing the 10 highest scoring posts and tokenizing the sentences. Once again, notice that we aren't going to do any kind of stemming this week (although *semantic* normalization may be performed where we translate synonyms into the same focal word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>over_18</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goldie-gold</td>\n",
       "      <td>False</td>\n",
       "      <td>12650</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>This just happened...  So, I had a laptop syst...</td>\n",
       "      <td>Engineer is doing drugs!! No. No they aren't.</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[This, just, happened, ...], [So, ,, I, had, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheDroolinFool</td>\n",
       "      <td>False</td>\n",
       "      <td>13152</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>Another tale from the out of hours IT desk... ...</td>\n",
       "      <td>\"I need you to fix Google Bing immediately!\"</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[Another, tale, from, the, out, of, hours, IT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clickity_clickity</td>\n",
       "      <td>False</td>\n",
       "      <td>13404</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>[Part 1](http://www.reddit.com/r/talesfromtech...</td>\n",
       "      <td>Jack, the Worst End User, Part 4</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[[, Part, 1, ], (, http, :, //www.reddit.com/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECGaz</td>\n",
       "      <td>False</td>\n",
       "      <td>13724</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>&gt; $Me  - Hello, IT.   &gt; $Usr - Hi, I am still ...</td>\n",
       "      <td>Hi, I am still off sick but I am not.</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[&gt;, $, Me, -, Hello, ,, IT, .], [&gt;, $, Usr, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guitarsdontdance</td>\n",
       "      <td>False</td>\n",
       "      <td>14089</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>So my story starts on what was a normal day ta...</td>\n",
       "      <td>\"Don't bother sending a tech, I'll be dead by ...</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[So, my, story, starts, on, what, was, a, nor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author  over_18  score                subreddit  \\\n",
       "4        goldie-gold    False  12650  Tales From Tech Support   \n",
       "3     TheDroolinFool    False  13152  Tales From Tech Support   \n",
       "2  Clickity_clickity    False  13404  Tales From Tech Support   \n",
       "1             SECGaz    False  13724  Tales From Tech Support   \n",
       "0   guitarsdontdance    False  14089  Tales From Tech Support   \n",
       "\n",
       "                                                text  \\\n",
       "4  This just happened...  So, I had a laptop syst...   \n",
       "3  Another tale from the out of hours IT desk... ...   \n",
       "2  [Part 1](http://www.reddit.com/r/talesfromtech...   \n",
       "1  > $Me  - Hello, IT.   > $Usr - Hi, I am still ...   \n",
       "0  So my story starts on what was a normal day ta...   \n",
       "\n",
       "                                               title  \\\n",
       "4      Engineer is doing drugs!! No. No they aren't.   \n",
       "3       \"I need you to fix Google Bing immediately!\"   \n",
       "2                   Jack, the Worst End User, Part 4   \n",
       "1              Hi, I am still off sick but I am not.   \n",
       "0  \"Don't bother sending a tech, I'll be dead by ...   \n",
       "\n",
       "                                                 url  \\\n",
       "4  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "3  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "2  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "0  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "\n",
       "                                           sentences  \n",
       "4  [[This, just, happened, ...], [So, ,, I, had, ...  \n",
       "3  [[Another, tale, from, the, out, of, hours, IT...  \n",
       "2  [[[, Part, 1, ], (, http, :, //www.reddit.com/...  \n",
       "1  [[>, $, Me, -, Hello, ,, IT, .], [>, $, Usr, -...  \n",
       "0  [[So, my, story, starts, on, what, was, a, nor...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores = redditDF.sort_values('score')[-10:]\n",
    "redditTopScores['sentences'] = redditTopScores['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "redditTopScores.index = range(len(redditTopScores) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "redditTopScores[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditTopScores['POS_sents'] = redditTopScores['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    [[(Last, JJ), (year, NN), (,, ,), (Help, NN), ...\n",
       "8    [[(First, JJ), (post, NN), (in, IN), (quite, R...\n",
       "7    [[([, NNP), (Original, NNP), (Post, NNP), (], ...\n",
       "6    [[(I, PRP), (witnessed, VBD), (this, DT), (ast...\n",
       "5    [[(I, PRP), (work, VBP), (Helpdesk, NNP), (for...\n",
       "4    [[(This, DT), (just, RB), (happened, VBN), (.....\n",
       "3    [[(Another, DT), (tale, NN), (from, IN), (the,...\n",
       "2    [[([, NNP), (Part, NNP), (1, CD), (], FW), ((,...\n",
       "1    [[(>, JJR), ($, $), (Me, PRP), (-, :), (Hello,...\n",
       "0    [[(So, RB), (my, PRP$), (story, NN), (starts, ...\n",
       "Name: POS_sents, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores['POS_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And count the number of `NN` (nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('password', 21),\n",
       " ('(', 19),\n",
       " ('time', 14),\n",
       " (')', 14),\n",
       " ('lot', 12),\n",
       " ('computer', 12),\n",
       " ('life', 11),\n",
       " ('email', 11),\n",
       " ('**Genius**', 10),\n",
       " ('message', 9),\n",
       " ('**Me**', 9),\n",
       " ('system', 9),\n",
       " ('day', 9),\n",
       " ('call', 8),\n",
       " ('laptop', 8),\n",
       " ('office', 8),\n",
       " ('part', 8),\n",
       " ('today', 8),\n",
       " ('story', 8),\n",
       " ('user', 7)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the number of top verbs (`VB`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 18),\n",
       " ('have', 17),\n",
       " ('get', 14),\n",
       " ('do', 11),\n",
       " ('change', 9),\n",
       " ('make', 8),\n",
       " ('know', 7),\n",
       " ('say', 7),\n",
       " ('help', 6),\n",
       " ('look', 6),\n",
       " ('tell', 6),\n",
       " ('send', 6),\n",
       " ('go', 5),\n",
       " ('work', 4),\n",
       " ('use', 4),\n",
       " ('receive', 4),\n",
       " ('thank', 4),\n",
       " ('feel', 4),\n",
       " ('want', 4),\n",
       " ('call', 4)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'VB'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the adjectives that modify the word, \"computer\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'own', 'unrestricted'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'computer'\n",
    "NResults = set()\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating POS tagger\n",
    "\n",
    "We can check the POS tagger by running it on a manually tagged corpus and identifying a reasonable error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeBank = nltk.corpus.treebank\n",
    "treeBank.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeBank.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanfordTags = stanford.postTagger.tag_sents(treeBank.sents()[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Dutch  \tStanford: JJ\tTreebank: NNP\n",
      "Word: publishing  \tStanford: NN\tTreebank: VBG\n",
      "Word: used  \tStanford: VBD\tTreebank: VBN\n",
      "Word: more  \tStanford: JJR\tTreebank: RBR\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: later  \tStanford: RB\tTreebank: JJ\n",
      "Word: New  \tStanford: NNP\tTreebank: JJ\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: more  \tStanford: JJR\tTreebank: RBR\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: replaced  \tStanford: VBD\tTreebank: VBN\n",
      "Word: more  \tStanford: JJR\tTreebank: JJ\n",
      "Word: expected  \tStanford: VBD\tTreebank: VBN\n",
      "Word: study  \tStanford: VBD\tTreebank: VBP\n",
      "Word: studied  \tStanford: VBD\tTreebank: VBN\n",
      "Word: industrialized  \tStanford: JJ\tTreebank: VBN\n",
      "Word: Lorillard  \tStanford: NNP\tTreebank: NN\n",
      "Word: found  \tStanford: VBD\tTreebank: VBN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: rejected  \tStanford: VBD\tTreebank: VBN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: poured  \tStanford: VBN\tTreebank: VBD\n",
      "Word: in  \tStanford: IN\tTreebank: RP\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "The Precision is 96.547%\n"
     ]
    }
   ],
   "source": [
    "NumDiffs = 0\n",
    "for sentIndex in range(len(stanfordTags)):\n",
    "    for wordIndex in range(len(stanfordTags[sentIndex])):\n",
    "        if stanfordTags[sentIndex][wordIndex][1] != treeBank.tagged_sents()[sentIndex][wordIndex][1]:\n",
    "            if treeBank.tagged_sents()[sentIndex][wordIndex][1] != '-NONE-':\n",
    "                print(\"Word: {}  \\tStanford: {}\\tTreebank: {}\".format(stanfordTags[sentIndex][wordIndex][0], stanfordTags[sentIndex][wordIndex][1], treeBank.tagged_sents()[sentIndex][wordIndex][1]))\n",
    "                NumDiffs += 1\n",
    "total = sum([len(s) for s in stanfordTags])\n",
    "print(\"The Precision is {:.3f}%\".format((total-NumDiffs)/total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the stanford POS tagger is quite good. Nevertheless, for a 20 word sentence, we only have a 66% chance ($1-.96^{20}$) of tagging (and later parsing) it correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform POS tagging on a meaningful (but modest) subset of a corpus associated with your final project. Examine the list of words associated with at least three different parts of speech. Consider conditional frequencies (e.g., adjectives associated with nouns of interest or adverbs with verbs of interest). What do these distributions suggest about your corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the base dataframe from PLoS Sample\n",
    "plos_complete_df = pandas.read_pickle('../data/plos_analysis/plos_sample.pk1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Contents</th>\n",
       "      <th>Copyright Year</th>\n",
       "      <th>Journal Title</th>\n",
       "      <th>Titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The study of animal communication is a complex...</td>\n",
       "      <td>2011</td>\n",
       "      <td>plos one</td>\n",
       "      <td>UV-Deprived Coloration Reduces Success in Mate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aneurysms in general represent a Damocles swor...</td>\n",
       "      <td>2017</td>\n",
       "      <td>plos one</td>\n",
       "      <td>Metabolomic profiling of ascending thoracic ao...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prognostic information about life expectancy i...</td>\n",
       "      <td>2013</td>\n",
       "      <td>plos one</td>\n",
       "      <td>Predictive Value of a Profile of Routine Blood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Interleukin (IL)-23 has been associated with t...</td>\n",
       "      <td>2017</td>\n",
       "      <td>plos one</td>\n",
       "      <td>Continuous IL-23 stimulation drives ILC3 deple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Labor represents a stress test for the fetus. ...</td>\n",
       "      <td>2014</td>\n",
       "      <td>plos one</td>\n",
       "      <td>Assessment of Coupling between Trans-Abdominal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Article Contents Copyright Year  \\\n",
       "0  The study of animal communication is a complex...           2011   \n",
       "1  Aneurysms in general represent a Damocles swor...           2017   \n",
       "2  Prognostic information about life expectancy i...           2013   \n",
       "3  Interleukin (IL)-23 has been associated with t...           2017   \n",
       "4  Labor represents a stress test for the fetus. ...           2014   \n",
       "\n",
       "  Journal Title                                             Titles  \n",
       "0      plos one  UV-Deprived Coloration Reduces Success in Mate...  \n",
       "1      plos one  Metabolomic profiling of ascending thoracic ao...  \n",
       "2      plos one  Predictive Value of a Profile of Routine Blood...  \n",
       "3      plos one  Continuous IL-23 stimulation drives ILC3 deple...  \n",
       "4      plos one  Assessment of Coupling between Trans-Abdominal...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reduce the dataframe to the first five entries to keep it from being too big for the kernel\n",
    "plos_df = plos_complete_df.iloc[0:5]\n",
    "plos_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/Anaconda3-5.0.0.1-el7-x86_64/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#tokenize the sentences. \n",
    "plos_df['sentences'] = plos_df['Article Contents'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/Anaconda3-5.0.0.1-el7-x86_64/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#run the POS tagger over the PLoS Sentences. \n",
    "plos_df['POS_sents'] = plos_df['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'DT'),\n",
       "  ('study', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('animal', 'NN'),\n",
       "  ('communication', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('complex', 'JJ'),\n",
       "  ('science', 'NN'),\n",
       "  ('addressing', 'VBG'),\n",
       "  ('a', 'DT'),\n",
       "  ('wide', 'JJ'),\n",
       "  ('range', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('multi-layered', 'JJ'),\n",
       "  ('questions', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('such', 'JJ'),\n",
       "  ('as', 'IN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('a', 'DT'),\n",
       "  ('signal', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('emitted', 'VBN'),\n",
       "  ('(', 'FW'),\n",
       "  ('e.g.', 'FW'),\n",
       "  (',', ','),\n",
       "  ('visually', 'RB'),\n",
       "  (',', ','),\n",
       "  ('acoustically', 'RB'),\n",
       "  ('etc', 'FW'),\n",
       "  (')', 'FW'),\n",
       "  (',', ','),\n",
       "  ('how', 'WRB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('perceived', 'VBN'),\n",
       "  ('(', 'FW'),\n",
       "  ('e.g', 'FW'),\n",
       "  ('.', '.')],\n",
       " [('the', 'DT'),\n",
       "  ('spectral', 'JJ'),\n",
       "  ('range', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('sensitivity', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('colour', 'NN'),\n",
       "  ('vision', 'NN'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('and', 'CC'),\n",
       "  ('of', 'IN'),\n",
       "  ('course', 'NN'),\n",
       "  ('the', 'DT'),\n",
       "  ('adaptive', 'JJ'),\n",
       "  ('reasons', 'NNS'),\n",
       "  ('for', 'IN'),\n",
       "  ('signaling', 'VBG'),\n",
       "  ('(', 'FW'),\n",
       "  ('e.g.', 'FW'),\n",
       "  (',', ','),\n",
       "  ('deterring', 'VBG'),\n",
       "  ('rivals', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('attracting', 'VBG'),\n",
       "  ('mates', 'NNS'),\n",
       "  (')', 'CD'),\n",
       "  ('.', '.')],\n",
       " [('Any', 'DT'),\n",
       "  ('and', 'CC'),\n",
       "  ('all', 'DT'),\n",
       "  ('of', 'IN'),\n",
       "  ('these', 'DT'),\n",
       "  ('factors', 'NNS'),\n",
       "  ('interact', 'VBP'),\n",
       "  ('to', 'TO'),\n",
       "  ('mold', 'NN'),\n",
       "  ('signal', 'NN'),\n",
       "  ('selection', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('wild', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('and', 'CC'),\n",
       "  ('mediate', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('type', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('degree', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('honest', 'JJ'),\n",
       "  ('indication', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('some', 'DT'),\n",
       "  ('aspect', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('sender', 'FW'),\n",
       "  ('‘', 'FW'),\n",
       "  ('quality', 'NN'),\n",
       "  ('’', 'NN'),\n",
       "  (',', ','),\n",
       "  ('which', 'WDT'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('expected', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('any', 'DT'),\n",
       "  ('evolutionarily', 'RB'),\n",
       "  ('stable', 'JJ'),\n",
       "  ('signal', 'NN'),\n",
       "  ('trait', 'NN'),\n",
       "  ('[', 'NN'),\n",
       "  ('1', 'CD'),\n",
       "  (']', 'FW'),\n",
       "  ('–', 'FW'),\n",
       "  ('[', 'NN'),\n",
       "  ('4', 'CD'),\n",
       "  (']', 'FW'),\n",
       "  ('.Early', 'FW'),\n",
       "  ('work', 'NN'),\n",
       "  ('suggested', 'VBD'),\n",
       "  ('that', 'IN'),\n",
       "  ('‘', 'CD'),\n",
       "  ('badges', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('status', 'NN'),\n",
       "  ('’', 'NN'),\n",
       "  ('would', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('beneficial', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('both', 'DT'),\n",
       "  ('signalers', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('receivers', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('since', 'IN'),\n",
       "  ('they', 'PRP'),\n",
       "  ('would', 'MD'),\n",
       "  ('cut', 'VB'),\n",
       "  ('costs', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('contests', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('both', 'DT'),\n",
       "  ('contestants', 'NNS'),\n",
       "  ('if', 'IN'),\n",
       "  ('an', 'DT'),\n",
       "  ('outcome', 'NN'),\n",
       "  ('would', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('predictable', 'JJ'),\n",
       "  ('[', 'NN'),\n",
       "  ('5', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Subsequent', 'JJ'),\n",
       "  ('work', 'NN'),\n",
       "  ('has', 'VBZ'),\n",
       "  ('debated', 'VBN'),\n",
       "  ('whether', 'IN'),\n",
       "  ('merely', 'RB'),\n",
       "  ('‘', 'CD'),\n",
       "  ('social', 'JJ'),\n",
       "  ('costs', 'NNS'),\n",
       "  ('’', 'CD'),\n",
       "  (',', ','),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('absence', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('developmental', 'JJ'),\n",
       "  ('costs', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('really', 'RB'),\n",
       "  ('are', 'VBP'),\n",
       "  ('sufficient', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('guarantee', 'VB'),\n",
       "  ('honest', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('evolutionarily', 'RB'),\n",
       "  ('stable', 'JJ'),\n",
       "  ('signaling', 'NN'),\n",
       "  (',', ','),\n",
       "  ('conveying', 'VBG'),\n",
       "  ('aspects', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('‘', 'NN'),\n",
       "  ('quality', 'NN'),\n",
       "  ('’', 'CD'),\n",
       "  (',', ','),\n",
       "  ('such', 'JJ'),\n",
       "  ('as', 'IN'),\n",
       "  ('fighting', 'VBG'),\n",
       "  ('or', 'CC'),\n",
       "  ('parental', 'JJ'),\n",
       "  ('ability', 'NN'),\n",
       "  ('[', 'NN'),\n",
       "  ('6', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  (',', ','),\n",
       "  ('[', 'NN'),\n",
       "  ('7', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Nevertheless', 'RB'),\n",
       "  (',', ','),\n",
       "  ('there', 'EX'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('no', 'DT'),\n",
       "  ('lack', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('examples', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('seemingly', 'RB'),\n",
       "  ('‘', 'JJ'),\n",
       "  ('cheap', 'JJ'),\n",
       "  ('’', 'NN'),\n",
       "  ('yet', 'RB'),\n",
       "  ('adaptive', 'JJ'),\n",
       "  ('badges', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('recent', 'JJ'),\n",
       "  ('literature', 'NN'),\n",
       "  (',', ','),\n",
       "  ('such', 'JJ'),\n",
       "  ('as', 'IN'),\n",
       "  ('wing', 'NN'),\n",
       "  ('epaulettes', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('birds', 'NNS'),\n",
       "  ('[', 'CD'),\n",
       "  ('8', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('badges', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('bright', 'JJ'),\n",
       "  ('colours', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('lizards', 'FW'),\n",
       "  ('[', 'FW'),\n",
       "  ('9', 'CD'),\n",
       "  (']', 'FW'),\n",
       "  ('.The', 'FW'),\n",
       "  ('bright', 'JJ'),\n",
       "  ('green', 'JJ'),\n",
       "  ('colour', 'NN'),\n",
       "  ('‘', 'FW'),\n",
       "  ('badge', 'FW'),\n",
       "  ('’', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('our', 'PRP$'),\n",
       "  ('model', 'NN'),\n",
       "  ('species', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('Swedish', 'JJ'),\n",
       "  ('sand', 'NN'),\n",
       "  ('lizard', 'FW'),\n",
       "  ('(', 'FW'),\n",
       "  ('Lacerta', 'FW'),\n",
       "  ('agilis', 'FW'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('shows', 'VBZ'),\n",
       "  ('spectral', 'JJ'),\n",
       "  ('reflectance', 'NN'),\n",
       "  ('peaks', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('both', 'CC'),\n",
       "  ('green', 'JJ'),\n",
       "  ('(', 'NN'),\n",
       "  ('ca', 'MD'),\n",
       "  ('540', 'CD'),\n",
       "  ('nm', 'FW'),\n",
       "  (')', 'FW'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('UV', 'NN'),\n",
       "  ('(', 'NN'),\n",
       "  ('ca', 'MD'),\n",
       "  ('340', 'CD'),\n",
       "  ('nm', 'FW'),\n",
       "  (')', 'FW'),\n",
       "  ('.', '.')],\n",
       " [('However', 'RB'),\n",
       "  (',', ','),\n",
       "  ('when', 'WRB'),\n",
       "  ('we', 'PRP'),\n",
       "  ('first', 'RB'),\n",
       "  ('investigated', 'VBD'),\n",
       "  ('these', 'DT'),\n",
       "  ('traits', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('mid', 'JJ'),\n",
       "  ('90', 'CD'),\n",
       "  (\"'s\", 'POS'),\n",
       "  (',', ','),\n",
       "  ('neither', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('UV-component', 'JJ'),\n",
       "  ('nor', 'CC'),\n",
       "  ('other', 'JJ'),\n",
       "  ('aspects', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('colour', 'NN'),\n",
       "  ('as', 'IN'),\n",
       "  ('such', 'JJ'),\n",
       "  ('(', 'JJ'),\n",
       "  ('spectral', 'JJ'),\n",
       "  ('shape', 'NN'),\n",
       "  (')', 'NN'),\n",
       "  ('were', 'VBD'),\n",
       "  ('identified', 'VBN'),\n",
       "  (';', ':'),\n",
       "  ('in', 'IN'),\n",
       "  ('particular', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('we', 'PRP'),\n",
       "  ('dismissed', 'VBD'),\n",
       "  ('the', 'DT'),\n",
       "  ('UV', 'NN'),\n",
       "  ('effect', 'NN'),\n",
       "  ('based', 'VBN'),\n",
       "  ('on', 'IN'),\n",
       "  ('data', 'NNS'),\n",
       "  ('from', 'IN'),\n",
       "  ('males', 'NNS'),\n",
       "  ('caught', 'VBN'),\n",
       "  ('at', 'IN'),\n",
       "  ('spring', 'NN'),\n",
       "  ('emergence', 'NN'),\n",
       "  ('from', 'IN'),\n",
       "  ('hibernation', 'NN'),\n",
       "  ('(', 'NN'),\n",
       "  ('with', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('rationale', 'NN'),\n",
       "  ('that', 'IN'),\n",
       "  ('signals', 'NNS'),\n",
       "  ('present', 'JJ'),\n",
       "  ('at', 'IN'),\n",
       "  ('this', 'DT'),\n",
       "  ('time', 'NN'),\n",
       "  ('are', 'VBP'),\n",
       "  ('the', 'DT'),\n",
       "  ('most', 'RBS'),\n",
       "  ('likely', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('reflect', 'VB'),\n",
       "  ('early', 'JJ'),\n",
       "  ('male', 'JJ'),\n",
       "  ('resource', 'NN'),\n",
       "  ('holding', 'VBG'),\n",
       "  ('power', 'NN'),\n",
       "  ('when', 'WRB'),\n",
       "  ('core', 'NN'),\n",
       "  ('areas', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('male', 'JJ'),\n",
       "  ('home', 'NN'),\n",
       "  ('ranges', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('contested', 'VBN'),\n",
       "  ('[', 'NN'),\n",
       "  ('9', 'CD'),\n",
       "  (']', 'FW'),\n",
       "  (')', 'FW'),\n",
       "  ('.', '.')],\n",
       " [('However', 'RB'),\n",
       "  (',', ','),\n",
       "  ('this', 'DT'),\n",
       "  ('ignores', 'VBZ'),\n",
       "  ('that', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('exuvia', 'NN'),\n",
       "  ('may', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('show', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('same', 'JJ'),\n",
       "  ('spectroradiometry', 'NN'),\n",
       "  ('characteristics', 'NNS'),\n",
       "  ('as', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('newly', 'RB'),\n",
       "  ('shed', 'VBN'),\n",
       "  ('animal', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('maximal', 'JJ'),\n",
       "  ('brightness', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Instead', 'RB'),\n",
       "  (',', ','),\n",
       "  ('our', 'PRP$'),\n",
       "  ('signaling', 'NN'),\n",
       "  ('work', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('sand', 'NN'),\n",
       "  ('lizards', 'NNS'),\n",
       "  ('has', 'VBZ'),\n",
       "  ('since', 'IN'),\n",
       "  ('mostly', 'RB'),\n",
       "  ('focused', 'VBN'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('link', 'NN'),\n",
       "  ('between', 'IN'),\n",
       "  ('badge', 'NN'),\n",
       "  ('size', 'NN'),\n",
       "  ('(', 'NN'),\n",
       "  ('proportion', 'NN'),\n",
       "  ('green', 'JJ'),\n",
       "  ('colour', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('male', 'NN'),\n",
       "  (\"'s\", 'POS'),\n",
       "  ('body', 'NN'),\n",
       "  ('sides', 'NNS'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('signaling', 'NN'),\n",
       "  (',', ','),\n",
       "  ('and', 'CC'),\n",
       "  ('fitness', 'NN'),\n",
       "  ('parameters', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('field', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('laboratory', 'NN'),\n",
       "  ('studies', 'NNS'),\n",
       "  ('[', 'CD'),\n",
       "  ('9', 'CD'),\n",
       "  (']', 'FW'),\n",
       "  ('–', 'FW'),\n",
       "  ('[', 'NN'),\n",
       "  ('12', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('This', 'DT'),\n",
       "  ('work', 'NN'),\n",
       "  ('showed', 'VBD'),\n",
       "  (',', ','),\n",
       "  ('for', 'IN'),\n",
       "  ('example', 'NN'),\n",
       "  (',', ','),\n",
       "  ('that', 'IN'),\n",
       "  ('badges', 'NNS'),\n",
       "  ('contribute', 'VBP'),\n",
       "  ('to', 'TO'),\n",
       "  ('mate', 'VB'),\n",
       "  ('acquisition', 'NN'),\n",
       "  (';', ':'),\n",
       "  ('in', 'IN'),\n",
       "  ('smaller', 'JJR'),\n",
       "  ('than', 'IN'),\n",
       "  ('average', 'JJ'),\n",
       "  ('males', 'NNS'),\n",
       "  ('experimental', 'JJ'),\n",
       "  ('increment', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('badge', 'NN'),\n",
       "  ('size', 'NN'),\n",
       "  ('increased', 'VBD'),\n",
       "  ('mate', 'NN'),\n",
       "  ('acquisition', 'NN'),\n",
       "  ('by', 'IN'),\n",
       "  ('400', 'CD'),\n",
       "  ('percent', 'NN'),\n",
       "  ('[', 'NN'),\n",
       "  ('12', 'CD'),\n",
       "  (']', 'FW'),\n",
       "  ('.Given', 'FW'),\n",
       "  ('the', 'DT'),\n",
       "  ('renewed', 'VBN'),\n",
       "  ('interest', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('UV', 'NN'),\n",
       "  ('signaling', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('both', 'DT'),\n",
       "  ('vertebrates', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('invertebrates', 'NNS'),\n",
       "  ('(', 'NNP'),\n",
       "  ('e.g.', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('[', 'NNP'),\n",
       "  ('13', 'CD'),\n",
       "  (']', 'NNP'),\n",
       "  ('–', 'NNP'),\n",
       "  ('[', 'NNP'),\n",
       "  ('21', 'CD'),\n",
       "  (']', 'FW'),\n",
       "  (')', 'FW'),\n",
       "  (',', ','),\n",
       "  ('we', 'PRP'),\n",
       "  ('revisited', 'VBD'),\n",
       "  ('this', 'DT'),\n",
       "  ('research', 'NN'),\n",
       "  ('area', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('2007', 'CD'),\n",
       "  ('.', '.')],\n",
       " [('A', 'DT'),\n",
       "  ('pilot', 'NN'),\n",
       "  ('study', 'NN'),\n",
       "  ('confirmed', 'VBD'),\n",
       "  ('that', 'IN'),\n",
       "  ('recently', 'RB'),\n",
       "  ('shed', 'VBN'),\n",
       "  ('males', 'NNS'),\n",
       "  ('indeed', 'RB'),\n",
       "  ('showed', 'VBD'),\n",
       "  ('a', 'DT'),\n",
       "  ('much', 'RB'),\n",
       "  ('stronger', 'JJR'),\n",
       "  ('reflectance', 'NN'),\n",
       "  ('peak', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('UV', 'NN'),\n",
       "  ('spectrum', 'NN'),\n",
       "  ('(', 'NN'),\n",
       "  ('ca', 'MD'),\n",
       "  ('340', 'CD'),\n",
       "  ('nm', 'NN'),\n",
       "  (',', ','),\n",
       "  ('Fig', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('1', 'CD'), (')', 'NN'), ('.', '.')],\n",
       " [('This', 'DT'),\n",
       "  ('agrees', 'VBZ'),\n",
       "  ('with', 'IN'),\n",
       "  ('reflectance', 'NN'),\n",
       "  ('data', 'NNS'),\n",
       "  ('from', 'IN'),\n",
       "  ('male', 'JJ'),\n",
       "  ('Lacerta', 'FW'),\n",
       "  ('agilis', 'FW'),\n",
       "  ('in', 'IN'),\n",
       "  ('Pyrenees', 'NNP'),\n",
       "  ('populations', 'NNS'),\n",
       "  ('during', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('mating', 'NN'),\n",
       "  ('season', 'NN'),\n",
       "  ('[', 'NN'),\n",
       "  ('19', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('We', 'PRP'),\n",
       "  ('therefore', 'RB'),\n",
       "  ('designed', 'VBD'),\n",
       "  ('an', 'DT'),\n",
       "  ('experiment', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('2008', 'CD'),\n",
       "  ('to', 'TO'),\n",
       "  ('test', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('proposition', 'NN'),\n",
       "  ('that', 'IN'),\n",
       "  ('UV', 'NN'),\n",
       "  ('blockage', 'NN'),\n",
       "  ('would', 'MD'),\n",
       "  ('interfere', 'VB'),\n",
       "  ('with', 'IN'),\n",
       "  ('rival', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('partner', 'NN'),\n",
       "  ('communication', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('compromise', 'NN'),\n",
       "  ('mate', 'VBP'),\n",
       "  ('acquisition.Note', 'NN'),\n",
       "  ('the', 'DT'),\n",
       "  ('considerable', 'JJ'),\n",
       "  ('UV-reduction', 'NN'),\n",
       "  ('also', 'RB'),\n",
       "  ('after', 'IN'),\n",
       "  ('30', 'CD'),\n",
       "  ('days', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('wild', 'JJ'),\n",
       "  ('.', '.')],\n",
       " [('Spectra', 'NNP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('set', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('equal', 'JJ'),\n",
       "  ('brightness', 'NN'),\n",
       "  (',', ','),\n",
       "  ('in', 'IN'),\n",
       "  ('order', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('see', 'VB'),\n",
       "  ('spectral', 'JJ'),\n",
       "  ('shape', 'NN'),\n",
       "  ('(', 'FW'),\n",
       "  ('i.e.', 'FW'),\n",
       "  (',', ','),\n",
       "  ('colour', 'NN'),\n",
       "  (')', 'CD'),\n",
       "  ('more', 'JJR'),\n",
       "  ('precisely.The', 'NN'),\n",
       "  ('field', 'NN'),\n",
       "  ('work', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('this', 'DT'),\n",
       "  ('population', 'NN'),\n",
       "  ('(', 'NNP'),\n",
       "  ('Asketunnan', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('Sweden', 'NNP'),\n",
       "  ('∼N57°22′', 'NNP'),\n",
       "  ('E11°58′', 'NNP'),\n",
       "  (')', 'NNP'),\n",
       "  ('follows', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('well-established', 'JJ'),\n",
       "  ('protocol', 'NN'),\n",
       "  ('that', 'WDT'),\n",
       "  ('has', 'VBZ'),\n",
       "  ('been', 'VBN'),\n",
       "  ('reported', 'VBN'),\n",
       "  ('on', 'IN'),\n",
       "  ('in', 'IN'),\n",
       "  ('previous', 'JJ'),\n",
       "  ('work', 'NN'),\n",
       "  ('(', 'NNP'),\n",
       "  ('e.g.', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('[', 'NNP'),\n",
       "  ('9', 'CD'),\n",
       "  (']', 'FW'),\n",
       "  ('–', 'FW'),\n",
       "  ('[', 'NN'),\n",
       "  ('12', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  (',', ','),\n",
       "  ('[', 'NN'),\n",
       "  ('22', 'CD'),\n",
       "  (']', 'FW'),\n",
       "  (')', 'FW'),\n",
       "  ('.', '.')],\n",
       " [('In', 'IN'),\n",
       "  ('short', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('sand', 'NN'),\n",
       "  ('lizards', 'NNS'),\n",
       "  ('(', 'FW'),\n",
       "  ('Lacerta', 'FW'),\n",
       "  ('agilis', 'FW'),\n",
       "  (')', 'FW'),\n",
       "  ('are', 'VBP'),\n",
       "  ('small', 'JJ'),\n",
       "  ('(', 'CD'),\n",
       "  ('to', 'TO'),\n",
       "  ('20', 'CD'),\n",
       "  ('g', 'NN'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('ground-dwelling', 'JJ'),\n",
       "  ('lizards', 'NNS'),\n",
       "  ('.', '.')],\n",
       " [('Eighty', 'CD'),\n",
       "  ('five', 'CD'),\n",
       "  ('males', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('eighty', 'NN'),\n",
       "  ('females', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('individually', 'RB'),\n",
       "  ('marked', 'JJ'),\n",
       "  ('short', 'JJ'),\n",
       "  ('term', 'NN'),\n",
       "  ('by', 'IN'),\n",
       "  ('putting', 'VBG'),\n",
       "  ('a', 'DT'),\n",
       "  ('uniquely', 'RB'),\n",
       "  ('numbered', 'VBN'),\n",
       "  ('cloth', 'NN'),\n",
       "  ('tape', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('their', 'PRP$'),\n",
       "  ('backs', 'NNS'),\n",
       "  ('.', '.')],\n",
       " [('Males', 'NNS'),\n",
       "  ('observed', 'VBD'),\n",
       "  ('courting', 'VBG'),\n",
       "  (',', ','),\n",
       "  ('copulating', 'VBG'),\n",
       "  ('or', 'CC'),\n",
       "  ('mate', 'VB'),\n",
       "  ('guarding', 'VBG'),\n",
       "  ('females', 'NNS'),\n",
       "  ('were', 'VBD'),\n",
       "  ('classified', 'VBN'),\n",
       "  ('as', 'IN'),\n",
       "  ('partners', 'NNS'),\n",
       "  ('.', '.')],\n",
       " [('The', 'DT'),\n",
       "  ('sex', 'NN'),\n",
       "  ('ratio', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('Asketunnan', 'NNP'),\n",
       "  ('population', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('approximately', 'RB'),\n",
       "  ('1∶1', 'CD'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('capture', 'NN'),\n",
       "  ('rate', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('adults', 'NNS'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('>', 'JJR'),\n",
       "  ('90', 'CD'),\n",
       "  ('%', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  (',', ','),\n",
       "  ('thus', 'RB'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('observations', 'NNS'),\n",
       "  ('made', 'VBN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('lizards', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('current', 'JJ'),\n",
       "  ('paper', 'NN'),\n",
       "  ('are', 'VBP'),\n",
       "  ('based', 'VBN'),\n",
       "  ('on', 'IN'),\n",
       "  ('nearly', 'RB'),\n",
       "  ('complete', 'JJ'),\n",
       "  ('coverage', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('adult', 'JJ'),\n",
       "  ('population', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('That', 'DT'),\n",
       "  ('said', 'VBD'),\n",
       "  (',', ','),\n",
       "  ('scored', 'VBD'),\n",
       "  ('mating', 'NN'),\n",
       "  ('success', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('adult', 'JJ'),\n",
       "  ('males', 'NNS'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('known', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('covary', 'VB'),\n",
       "  ('with', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('times', 'NNS'),\n",
       "  ('males', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('observed', 'VBN'),\n",
       "  (',', ','),\n",
       "  ('which', 'WDT'),\n",
       "  ('was', 'VBD'),\n",
       "  ('therefore', 'RB'),\n",
       "  ('controlled', 'VBN'),\n",
       "  ('for', 'IN'),\n",
       "  ('in', 'IN'),\n",
       "  ('our', 'PRP$'),\n",
       "  ('analyses', 'NNS'),\n",
       "  ('.', '.')],\n",
       " [('All', 'DT'),\n",
       "  ('adults', 'NNS'),\n",
       "  ('were', 'VBD'),\n",
       "  ('weighed', 'VBN'),\n",
       "  ('(', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('the', 'DT'),\n",
       "  ('nearest', 'JJS'),\n",
       "  ('0.1', 'CD'),\n",
       "  ('g', 'NN'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('measured', 'VBN'),\n",
       "  ('(', 'NN'),\n",
       "  ('snout-vent', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('total', 'JJ'),\n",
       "  ('length', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('the', 'DT'),\n",
       "  ('nearest', 'JJS'),\n",
       "  ('1', 'CD'),\n",
       "  ('mm', 'NN'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('and', 'CC'),\n",
       "  ('a', 'DT'),\n",
       "  ('50', 'CD'),\n",
       "  ('µl', 'NN'),\n",
       "  ('blood', 'NN'),\n",
       "  ('sample', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('taken', 'VBN'),\n",
       "  ('from', 'IN'),\n",
       "  ('vena', 'FW'),\n",
       "  ('angularis', 'FW'),\n",
       "  ('(', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('corner', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('mouth', 'NN'),\n",
       "  (')', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('both', 'DT'),\n",
       "  ('males', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('females', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('stored', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('70', 'CD'),\n",
       "  ('%', 'NN'),\n",
       "  ('alcohol', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('later', 'RB'),\n",
       "  ('molecular', 'JJ'),\n",
       "  ('genetic', 'JJ'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Males', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('females', 'NNS'),\n",
       "  ('were', 'VBD'),\n",
       "  ('then', 'RB'),\n",
       "  ('released', 'VBN'),\n",
       "  ('at', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('place', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('capture', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('monitored', 'VBN'),\n",
       "  ('during', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('mating', 'NN'),\n",
       "  ('season', 'NN'),\n",
       "  ('(', 'NN'),\n",
       "  ('ca', 'MD'),\n",
       "  ('seven', 'CD'),\n",
       "  ('weeks', 'NNS'),\n",
       "  (')', 'VBP'),\n",
       "  ('every', 'DT'),\n",
       "  ('day', 'NN'),\n",
       "  ('that', 'IN'),\n",
       "  ('weather', 'NN'),\n",
       "  ('permitted.Females', 'NNS'),\n",
       "  ('were', 'VBD'),\n",
       "  ('immediately', 'RB'),\n",
       "  ('released', 'VBN'),\n",
       "  ('at', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('place', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('captured', 'VBN'),\n",
       "  ('.', '.')],\n",
       " [('Males', 'NNS'),\n",
       "  ('were', 'VBD'),\n",
       "  ('accumulated', 'VBN'),\n",
       "  ('at', 'IN'),\n",
       "  ('daily', 'JJ'),\n",
       "  ('field', 'NN'),\n",
       "  ('captures', 'VBZ'),\n",
       "  ('over', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('ten', 'CD'),\n",
       "  ('day', 'NN'),\n",
       "  ('period', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('stored', 'VBD'),\n",
       "  ('at', 'IN'),\n",
       "  ('+8°C', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('constant', 'JJ'),\n",
       "  ('temperature', 'NN'),\n",
       "  ('room', 'NN'),\n",
       "  (',', ','),\n",
       "  ('awaiting', 'VBG'),\n",
       "  ('a', 'DT'),\n",
       "  ('synchronized', 'VBN'),\n",
       "  ('release', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('all', 'DT'),\n",
       "  ('males', 'NNS'),\n",
       "  ('immediately', 'RB'),\n",
       "  ('after', 'IN'),\n",
       "  ('being', 'VBG'),\n",
       "  ('weighed', 'VBN'),\n",
       "  (',', ','),\n",
       "  ('measured', 'VBN'),\n",
       "  (',', ','),\n",
       "  ('marked', 'VBN'),\n",
       "  ('and', 'CC'),\n",
       "  ('treated', 'VBN'),\n",
       "  ('with', 'IN'),\n",
       "  ('UV', 'NN'),\n",
       "  ('blocker', 'NN'),\n",
       "  ('(', 'NN'),\n",
       "  ('released', 'VBN'),\n",
       "  ('2', 'CD'),\n",
       "  ('May', 'NNP'),\n",
       "  ('2008', 'CD'),\n",
       "  (')', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Representative', 'JJ'),\n",
       "  ('radiospectrometric', 'JJ'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('UV', 'NN'),\n",
       "  ('blockage', 'NN'),\n",
       "  ('effects', 'NNS'),\n",
       "  ('were', 'VBD'),\n",
       "  ('performed', 'VBN'),\n",
       "  ('at', 'IN'),\n",
       "  ('release', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('after', 'IN'),\n",
       "  ('three', 'CD'),\n",
       "  ('weeks', 'NNS'),\n",
       "  ('(', 'NNP'),\n",
       "  ('Fig', 'NNP'),\n",
       "  ('.', '.')],\n",
       " [('1', 'CD'),\n",
       "  (')', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('verify', 'VB'),\n",
       "  ('that', 'DT'),\n",
       "  ('our', 'PRP$'),\n",
       "  ('UV', 'NN'),\n",
       "  ('blockage', 'NN'),\n",
       "  ('had', 'VBD'),\n",
       "  ('the', 'DT'),\n",
       "  ('desired', 'VBN'),\n",
       "  ('long-term', 'JJ'),\n",
       "  ('effects', 'NNS'),\n",
       "  ('.', '.')],\n",
       " [('The', 'DT'),\n",
       "  ('second', 'JJ'),\n",
       "  ('measure', 'NN'),\n",
       "  ('after', 'IN'),\n",
       "  ('three', 'CD'),\n",
       "  ('weeks', 'NNS'),\n",
       "  ('was', 'VBD'),\n",
       "  ('virtually', 'RB'),\n",
       "  ('indistinguishable', 'JJ'),\n",
       "  ('from', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('first', 'JJ'),\n",
       "  ('(', 'NNP'),\n",
       "  ('Fig', 'NNP'),\n",
       "  ('.', '.')],\n",
       " [('1', 'CD'), (')', 'NN'), ('.', '.')],\n",
       " [('The', 'DT'),\n",
       "  ('UV', 'NN'),\n",
       "  ('blockage', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('performed', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('gently', 'RB'),\n",
       "  ('rubbing', 'VBG'),\n",
       "  ('+50', 'CD'),\n",
       "  ('SPF', 'NNP'),\n",
       "  ('(', 'NNP'),\n",
       "  ('‘', 'NNP'),\n",
       "  ('sun', 'NN'),\n",
       "  ('protection', 'NN'),\n",
       "  ('factor', 'NN'),\n",
       "  ('’', 'NN'),\n",
       "  (';', ':'),\n",
       "  ('Vichy', 'NNP'),\n",
       "  ('Laboratoires', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('Capital', 'NNP'),\n",
       "  ('Soleil', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('Very', 'RB'),\n",
       "  ('High', 'NNP'),\n",
       "  ('Protection', 'NNP'),\n",
       "  (']', 'NNP'),\n",
       "  ('on', 'IN'),\n",
       "  ('every', 'DT'),\n",
       "  ('second', 'JJ'),\n",
       "  ('male', 'JJ'),\n",
       "  ('(', 'NN'),\n",
       "  ('n', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('43', 'CD'),\n",
       "  (',', ','),\n",
       "  ('for', 'IN'),\n",
       "  ('controls', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('n', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('42', 'CD'),\n",
       "  (')', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('an', 'DT'),\n",
       "  ('Excel', 'NNP'),\n",
       "  ('size-sorted', 'JJ'),\n",
       "  ('data', 'NNS'),\n",
       "  ('set', 'VBN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('captured', 'VBN'),\n",
       "  ('males', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('storage', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('This', 'DT'),\n",
       "  ('ensured', 'VBD'),\n",
       "  ('that', 'IN'),\n",
       "  ('UV-blocked', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('control', 'NN'),\n",
       "  ('males', 'NNS'),\n",
       "  ('did', 'VBD'),\n",
       "  ('not', 'RB'),\n",
       "  ('differ', 'VB'),\n",
       "  ('in', 'IN'),\n",
       "  ('snout-vent', 'JJ'),\n",
       "  ('length', 'NN'),\n",
       "  (',', ','),\n",
       "  ('mass', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('body', 'NN'),\n",
       "  ('condition', 'NN'),\n",
       "  ('(', 'CD'),\n",
       "  ('p', 'NN'),\n",
       "  ('>', 'JJR'),\n",
       "  ('0.14', 'CD'),\n",
       "  ('for', 'IN'),\n",
       "  ('all', 'DT'),\n",
       "  ('three', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('these', 'DT'),\n",
       "  ('traits', 'NNS'),\n",
       "  (')', 'CD'),\n",
       "  ('.', '.')],\n",
       " [('Thereafter', 'RB'),\n",
       "  ('every', 'DT'),\n",
       "  ('male', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('sprayed', 'VBN'),\n",
       "  ('with', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('vapour-permeable', 'JJ'),\n",
       "  ('spray', 'NN'),\n",
       "  ('dressing', 'VBG'),\n",
       "  (',', ','),\n",
       "  ('used', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('treat', 'VB'),\n",
       "  ('superficial', 'JJ'),\n",
       "  ('human', 'JJ'),\n",
       "  ('wounds', 'NNS'),\n",
       "  ('(', 'NNP'),\n",
       "  ('Smith', 'NNP'),\n",
       "  ('&', 'CC'),\n",
       "  ('Nephew', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('Hull', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('England', 'NNP'),\n",
       "  (')', 'NNP'),\n",
       "  ('.', '.')],\n",
       " [('Neither', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('storage', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('cool', 'JJ'),\n",
       "  ('temperatures', 'NNS'),\n",
       "  ('nor', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('spray', 'NN'),\n",
       "  ('dressing', 'VBG'),\n",
       "  ('have', 'VBP'),\n",
       "  ('any', 'DT'),\n",
       "  ('detrimental', 'JJ'),\n",
       "  ('effects', 'NNS'),\n",
       "  ('[', 'VBP'),\n",
       "  ('12', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  (',', ','),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('UV', 'NN'),\n",
       "  ('block', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('developed', 'VBN'),\n",
       "  ('for', 'IN'),\n",
       "  ('humans', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('appeared', 'VBD'),\n",
       "  ('biologically', 'RB'),\n",
       "  ('inert', 'JJ'),\n",
       "  ('on', 'IN'),\n",
       "  ('lizard', 'NN'),\n",
       "  ('skin', 'NN'),\n",
       "  ('(', 'NN'),\n",
       "  ('no', 'DT'),\n",
       "  ('apparent', 'JJ'),\n",
       "  ('fading', 'JJ'),\n",
       "  ('or', 'CC'),\n",
       "  ('discoloring', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('observed', 'VBN'),\n",
       "  (')', 'FW'),\n",
       "  ('.After', 'FW'),\n",
       "  ('the', 'DT'),\n",
       "  ('morphology', 'NN'),\n",
       "  ('data', 'NNS'),\n",
       "  ('had', 'VBD'),\n",
       "  ('been', 'VBN'),\n",
       "  ('collected', 'VBN'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('lizards', 'NNS'),\n",
       "  ('treated', 'VBN'),\n",
       "  (',', ','),\n",
       "  ('they', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('released', 'VBN'),\n",
       "  ('at', 'IN'),\n",
       "  ('random', 'JJ'),\n",
       "  ('places', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('capture', 'NN'),\n",
       "  ('(', 'FW'),\n",
       "  ('i.e.', 'FW'),\n",
       "  (',', ','),\n",
       "  ('randomizing', 'VBG'),\n",
       "  ('sites', 'NNS'),\n",
       "  ('that', 'WDT'),\n",
       "  ('were', 'VBD'),\n",
       "  (',', ','),\n",
       "  ('at', 'IN'),\n",
       "  ('capture', 'NN'),\n",
       "  (',', ','),\n",
       "  ('potentially', 'RB'),\n",
       "  ('further', 'RBR'),\n",
       "  ('or', 'CC'),\n",
       "  ('closer', 'RBR'),\n",
       "  ('from', 'IN'),\n",
       "  ('females', 'NNS'),\n",
       "  (')', 'CD'),\n",
       "  ('and', 'CC'),\n",
       "  ('monitored', 'VBD'),\n",
       "  ('for', 'IN'),\n",
       "  ('associations', 'NNS'),\n",
       "  ('with', 'IN'),\n",
       "  ('partners', 'NNS'),\n",
       "  ('(', 'NN'),\n",
       "  ('facilitated', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('prolonged', 'JJ'),\n",
       "  ('mate', 'NN'),\n",
       "  ('guarding', 'VBG'),\n",
       "  (',', ','),\n",
       "  ('[', 'NN'),\n",
       "  ('22', 'CD'),\n",
       "  (']', 'FW'),\n",
       "  (')', 'FW'),\n",
       "  ('every', 'DT'),\n",
       "  ('day', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('mating', 'NN'),\n",
       "  ('season', 'NN'),\n",
       "  ('when', 'WRB'),\n",
       "  ('the', 'DT'),\n",
       "  ('weather', 'NN'),\n",
       "  ('permitted', 'VBD'),\n",
       "  ('lizard', 'NN'),\n",
       "  ('activity', 'NN'),\n",
       "  ('(', 'NN'),\n",
       "  ('3', 'CD'),\n",
       "  ('May–20', 'NN'),\n",
       "  ('June', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('observation', 'NN'),\n",
       "  ('days', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('n', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('26', 'CD'),\n",
       "  (')', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Thus', 'RB'),\n",
       "  (',', ','),\n",
       "  ('our', 'PRP$'),\n",
       "  ('procedure', 'NN'),\n",
       "  ('also', 'RB'),\n",
       "  ('eliminates', 'VBZ'),\n",
       "  ('variation', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('male', 'JJ'),\n",
       "  ('spring', 'NN'),\n",
       "  ('emergence', 'NN'),\n",
       "  ('(', 'NN'),\n",
       "  ('since', 'IN'),\n",
       "  ('all', 'DT'),\n",
       "  ('males', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('released', 'VBN'),\n",
       "  ('simultaneously', 'RB'),\n",
       "  (')', 'CD'),\n",
       "  ('.', '.')],\n",
       " [('Our', 'PRP$'),\n",
       "  ('work', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('approved', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('Animal', 'NNP'),\n",
       "  ('Ethics', 'NNPS'),\n",
       "  ('Committee', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('University', 'NNP'),\n",
       "  ('of', 'IN'),\n",
       "  ('Gothenburg.Our', 'NN'),\n",
       "  ('statistical', 'JJ'),\n",
       "  ('analyses', 'NNS'),\n",
       "  ('involved', 'VBD'),\n",
       "  ('two', 'CD'),\n",
       "  ('approaches', 'NNS'),\n",
       "  (':', ':'),\n",
       "  ('(', 'CD'),\n",
       "  ('a', 'DT'),\n",
       "  (')', 'NN'),\n",
       "  ('we', 'PRP'),\n",
       "  ('first', 'RB'),\n",
       "  ('performed', 'VBD'),\n",
       "  ('a', 'DT'),\n",
       "  ('homogeneity', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('slopes', 'NNS'),\n",
       "  ('regression', 'NN'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('with', 'IN'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('partners', 'NNS'),\n",
       "  ('as', 'IN'),\n",
       "  ('response', 'NN'),\n",
       "  ('variable', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('treatment', 'NN'),\n",
       "  ('(', 'FW'),\n",
       "  ('UV-blocked', 'FW'),\n",
       "  ('vs.', 'FW'),\n",
       "  ('controls', 'VBZ'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('observations', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('male', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('its', 'PRP$'),\n",
       "  ('interaction', 'NN'),\n",
       "  ('with', 'IN'),\n",
       "  ('treatment', 'NN'),\n",
       "  (',', ','),\n",
       "  ('and', 'CC'),\n",
       "  ('male', 'JJ'),\n",
       "  ('snout-vent', 'JJ'),\n",
       "  ('length', 'NN'),\n",
       "  ('as', 'IN'),\n",
       "  ('covariate', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('However', 'RB'),\n",
       "  (',', ','),\n",
       "  ('because', 'IN'),\n",
       "  ('of', 'IN'),\n",
       "  ('some', 'DT'),\n",
       "  ('non-normality', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('data', 'NNS'),\n",
       "  ('(', 'NN'),\n",
       "  ('over-representation', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('zero', 'CD'),\n",
       "  ('pairing', 'NN'),\n",
       "  ('success', 'NN'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('we', 'PRP'),\n",
       "  ('(', 'VBD'),\n",
       "  ('b', 'NN'),\n",
       "  (')', 'NN'),\n",
       "  ('also', 'RB'),\n",
       "  ('performed', 'VBD'),\n",
       "  ('an', 'DT'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('more', 'RBR'),\n",
       "  ('robust', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('deviation', 'NN'),\n",
       "  ('from', 'IN'),\n",
       "  ('normality', 'NN'),\n",
       "  ('using', 'VBG'),\n",
       "  ('a', 'DT'),\n",
       "  ('logistic', 'JJ'),\n",
       "  ('regression', 'NN'),\n",
       "  ('with', 'IN'),\n",
       "  ('an', 'DT'),\n",
       "  ('ordered', 'VBN'),\n",
       "  ('cumulative', 'JJ'),\n",
       "  ('logit', 'NN'),\n",
       "  ('model', 'NN'),\n",
       "  ('with', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('same', 'JJ'),\n",
       "  ('trait', 'NN'),\n",
       "  ('variables.There', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('no', 'DT'),\n",
       "  ('difference', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('mean', 'JJ'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('observations', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('UV-reduced', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('control', 'NN'),\n",
       "  ('males', 'NNS'),\n",
       "  ('(', 'VBP'),\n",
       "  ('mean', 'JJ'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('re-observations', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('2.1±0.24', 'NN'),\n",
       "  (',', ','),\n",
       "  ('range', 'NN'),\n",
       "  ('1', 'CD'),\n",
       "  ('to', 'TO'),\n",
       "  ('8', 'CD'),\n",
       "  (',', ','),\n",
       "  ('and', 'CC'),\n",
       "  ('1.93±0.24', 'CD'),\n",
       "  (',', ','),\n",
       "  ('range', 'NN'),\n",
       "  ('1', 'CD'),\n",
       "  ('to', 'TO'),\n",
       "  ('9', 'CD'),\n",
       "  (',', ','),\n",
       "  ('for', 'IN'),\n",
       "  ('control', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('UV-reduced', 'JJ'),\n",
       "  ('males', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('respectively', 'RB'),\n",
       "  (';', ':'),\n",
       "  ('T-test', 'NN'),\n",
       "  (',', ','),\n",
       "  ('t', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('0.61', 'CD'),\n",
       "  (',', ','),\n",
       "  ('P', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('0.54', 'CD'),\n",
       "  (')', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Across', 'IN'),\n",
       "  ('treatment', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('control', 'NN'),\n",
       "  ('males', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('observations', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('male', 'NN'),\n",
       "  ('after', 'IN'),\n",
       "  ('release', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('correlated', 'VBN'),\n",
       "  ('with', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('times', 'NNS'),\n",
       "  ('he', 'PRP'),\n",
       "  ('was', 'VBD'),\n",
       "  ('seen', 'VBN'),\n",
       "  ('courting', 'VBG'),\n",
       "  ('a', 'DT'),\n",
       "  ('female', 'JJ'),\n",
       "  ('(', 'NN'),\n",
       "  ('rs', 'VBZ'),\n",
       "  ('=', 'JJ'),\n",
       "  ('0.49', 'CD'),\n",
       "  (',', ','),\n",
       "  ('P', 'NN'),\n",
       "  ('<', 'VBG'),\n",
       "  ('0.0001', 'CD'),\n",
       "  (',', ','),\n",
       "  ('N', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('85', 'CD'),\n",
       "  (')', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('We', 'PRP'),\n",
       "  ('therefore', 'RB'),\n",
       "  ('incorporated', 'VBD'),\n",
       "  ('male', 'JJ'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('re-sightings', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('our', 'PRP$'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('treatment', 'NN'),\n",
       "  ('effects', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('females', 'NNS'),\n",
       "  ('paired', 'JJ'),\n",
       "  ('.', '.')],\n",
       " [('UV-blocked', 'JJ'),\n",
       "  ('males', 'NNS'),\n",
       "  ('had', 'VBD'),\n",
       "  ('an', 'DT'),\n",
       "  ('average', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('0.12', 'CD'),\n",
       "  ('female', 'JJ'),\n",
       "  ('pairing', 'VBG'),\n",
       "  ('observations', 'NNS'),\n",
       "  ('per', 'IN'),\n",
       "  ('male', 'JJ'),\n",
       "  ('(', 'FW'),\n",
       "  ('±0.049', 'FW'),\n",
       "  (',', ','),\n",
       "  ('SE', 'NN'),\n",
       "  (',', ','),\n",
       "  ('N', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('43', 'CD'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('whereas', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('corresponding', 'JJ'),\n",
       "  ('number', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('control', 'NN'),\n",
       "  ('males', 'NNS'),\n",
       "  ('was', 'VBD'),\n",
       "  ('three', 'CD'),\n",
       "  ('times', 'NNS'),\n",
       "  ('as', 'IN'),\n",
       "  ('high', 'JJ'),\n",
       "  ('(', 'NN'),\n",
       "  ('0.31±0.12', 'NN'),\n",
       "  (',', ','),\n",
       "  ('N', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('42', 'CD'),\n",
       "  (')', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('The', 'DT'),\n",
       "  ('regression', 'NN'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('globally', 'RB'),\n",
       "  ('significant', 'JJ'),\n",
       "  ('(', 'NN'),\n",
       "  ('F3', 'NN'),\n",
       "  (',', ','),\n",
       "  ('81', 'CD'),\n",
       "  ('=', 'JJ'),\n",
       "  ('40.7', 'CD'),\n",
       "  (',', ','),\n",
       "  ('P', 'NN'),\n",
       "  ('<', 'VBG'),\n",
       "  ('0.0001', 'CD'),\n",
       "  (',', ','),\n",
       "  ('R2', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('0.60', 'CD'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('and', 'CC'),\n",
       "  ('had', 'VBD'),\n",
       "  ('significant', 'JJ'),\n",
       "  ('independent', 'JJ'),\n",
       "  ('effects', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('treatment', 'NN'),\n",
       "  ('(', 'NN'),\n",
       "  ('F', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('15.4', 'CD'),\n",
       "  (',', ','),\n",
       "  ('P', 'NN'),\n",
       "  ('<', 'VBG'),\n",
       "  ('0.0002', 'CD'),\n",
       "  (',', ','),\n",
       "  ('d.f', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('=', 'JJ'),\n",
       "  ('1', 'CD'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('observations', 'NNS'),\n",
       "  ('(', 'CD'),\n",
       "  ('F', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('82.9', 'CD'),\n",
       "  (',', ','),\n",
       "  ('P', 'NN'),\n",
       "  ('<', 'VBG'),\n",
       "  ('0.0001', 'CD'),\n",
       "  (',', ','),\n",
       "  ('d.f', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('=', 'JJ'),\n",
       "  ('1', 'CD'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('and', 'CC'),\n",
       "  ('their', 'PRP$'),\n",
       "  ('interaction', 'NN'),\n",
       "  ('(', 'NN'),\n",
       "  ('F', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('39.7', 'CD'),\n",
       "  (',', ','),\n",
       "  ('P', 'NN'),\n",
       "  ('<', 'JJR'),\n",
       "  ('0.0001', 'CD'),\n",
       "  (';', ':'),\n",
       "  ('Fig', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('2', 'CD'), (')', 'NN'), ('.', '.')],\n",
       " [('Body', 'NN'),\n",
       "  ('size', 'NN'),\n",
       "  ('(', 'FW'),\n",
       "  ('SVL', 'FW'),\n",
       "  (')', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('backwards', 'RB'),\n",
       "  ('eliminated', 'VBN'),\n",
       "  ('from', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('final', 'JJ'),\n",
       "  ('model', 'NN'),\n",
       "  ('(', 'NN'),\n",
       "  ('P', 'NN'),\n",
       "  ('>', 'JJR'),\n",
       "  ('0.25', 'CD'),\n",
       "  (')', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Our', 'PRP$'),\n",
       "  ('cumulative', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('ordered', 'VBN'),\n",
       "  ('logistic', 'JJ'),\n",
       "  ('regression', 'NN'),\n",
       "  ('largely', 'RB'),\n",
       "  ('agreed', 'VBD'),\n",
       "  ('with', 'IN'),\n",
       "  ('these', 'DT'),\n",
       "  ('results', 'NNS'),\n",
       "  ('(', 'VBP'),\n",
       "  ('Global', 'JJ'),\n",
       "  ('model', 'NN'),\n",
       "  ('Likelihood', 'NN'),\n",
       "  ('ratio', 'NN'),\n",
       "  ('X2', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('35.0', 'CD'),\n",
       "  (',', ','),\n",
       "  ('P', 'NN'),\n",
       "  ('<', 'VBG'),\n",
       "  ('0.0001', 'CD'),\n",
       "  (',', ','),\n",
       "  ('d.f', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('=', 'JJ'), ('4', 'CD'), (')', 'NN'), ('.', '.')],\n",
       " [('The', 'DT'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('observations', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('male', 'NN'),\n",
       "  ('significantly', 'RB'),\n",
       "  ('affected', 'VBD'),\n",
       "  ('the', 'DT'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('females', 'NNS'),\n",
       "  ('he', 'PRP'),\n",
       "  ('was', 'VBD'),\n",
       "  ('observed', 'VBN'),\n",
       "  ('with', 'IN'),\n",
       "  ('(', 'FW'),\n",
       "  ('Wald', 'FW'),\n",
       "  ('X2', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('15.06', 'CD'),\n",
       "  (',', ','),\n",
       "  ('P', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('0.0001', 'CD'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('treatment', 'NN'),\n",
       "  ('x', 'NN'),\n",
       "  ('observation', 'NN'),\n",
       "  ('interaction', 'NN'),\n",
       "  ('remained', 'VBD'),\n",
       "  ('significant', 'JJ'),\n",
       "  ('(', 'NNP'),\n",
       "  ('Wald', 'NNP'),\n",
       "  ('X2', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('6.03', 'CD'),\n",
       "  (',', ','),\n",
       "  ('P', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('0.014', 'CD'),\n",
       "  (')', 'CD'),\n",
       "  (',', ','),\n",
       "  ('while', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('treatment', 'NN'),\n",
       "  ('effect', 'NN'),\n",
       "  ('per', 'FW'),\n",
       "  ('se', 'FW'),\n",
       "  ('fell', 'VBD'),\n",
       "  ('just', 'RB'),\n",
       "  ('short', 'JJ'),\n",
       "  ('of', 'IN'),\n",
       "  ('significant', 'JJ'),\n",
       "  ('(', 'FW'),\n",
       "  ('Wald', 'FW'),\n",
       "  ('X2', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('3.07', 'CD'),\n",
       "  (',', ','),\n",
       "  ('P', 'NN'),\n",
       "  ('=', 'JJ'),\n",
       "  ('0.079', 'CD'),\n",
       "  (';', ':'),\n",
       "  ('Fig', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('2', 'CD'),\n",
       "  (')', 'CD'),\n",
       "  ('.Increment', 'NN'),\n",
       "  ('symbol', 'NN'),\n",
       "  ('size', 'NN'),\n",
       "  ('represents', 'VBZ'),\n",
       "  ('increasing', 'VBG'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('observations', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('males', 'NNS'),\n",
       "  ('from', 'IN'),\n",
       "  ('1', 'CD'),\n",
       "  ('(', 'CD'),\n",
       "  ('smallest', 'JJS'),\n",
       "  (')', 'CD'),\n",
       "  ('to', 'TO'),\n",
       "  ('24', 'CD'),\n",
       "  ('(', 'CD'),\n",
       "  ('largest', 'JJS'),\n",
       "  (')', 'FW'),\n",
       "  ('.Our', 'FW'),\n",
       "  ('results', 'NNS'),\n",
       "  ('show', 'VBP'),\n",
       "  ('slight', 'JJ'),\n",
       "  ('discrepancy', 'NN'),\n",
       "  ('between', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('logistic', 'JJ'),\n",
       "  ('regression', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('linear', 'JJ'),\n",
       "  ('multiple', 'JJ'),\n",
       "  ('regression', 'NN'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('However', 'RB'),\n",
       "  (',', ','),\n",
       "  ('we', 'PRP'),\n",
       "  ('know', 'VBP'),\n",
       "  ('from', 'IN'),\n",
       "  ('previous', 'JJ'),\n",
       "  ('work', 'NN'),\n",
       "  ('[', 'NN'),\n",
       "  ('22', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  ('that', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('observations', 'NNS'),\n",
       "  ('per', 'IN'),\n",
       "  ('male', 'NN'),\n",
       "  ('influences', 'VBZ'),\n",
       "  ('estimates', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('mate', 'NN'),\n",
       "  ('acquisition', 'NN'),\n",
       "  ('(', 'NN'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('females', 'NNS'),\n",
       "  ('seen', 'VBN'),\n",
       "  ('courting', 'VBG'),\n",
       "  (')', 'NN'),\n",
       "  (',', ','),\n",
       "  ('and', 'CC'),\n",
       "  ('that', 'IN'),\n",
       "  ('this', 'DT'),\n",
       "  ('effect', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('modified', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('UV', 'NN'),\n",
       "  ('reduced', 'VBD'),\n",
       "  ('signaling', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Thus', 'RB'),\n",
       "  (',', ','),\n",
       "  ('it', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('argued', 'VBN'),\n",
       "  ('that', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('significant', 'JJ'),\n",
       "  ('interaction', 'NN'),\n",
       "  ('term', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('both', 'DT'),\n",
       "  ('analyses', 'NNS'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('the', 'DT'),\n",
       "  ('correct', 'JJ'),\n",
       "  ('unit', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('analysis', 'NN'),\n",
       "  (',', ','),\n",
       "  ('and', 'CC'),\n",
       "  ('it', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('significant', 'JJ'),\n",
       "  ('in', 'IN'),\n",
       "  ('both', 'DT'),\n",
       "  ('cases', 'NNS'),\n",
       "  ('.', '.')],\n",
       " [('How', 'WRB'),\n",
       "  ('robust', 'JJ'),\n",
       "  ('are', 'VBP'),\n",
       "  ('these', 'DT'),\n",
       "  ('results', 'NNS'),\n",
       "  ('?', '.')],\n",
       " [('The', 'DT'),\n",
       "  ('current', 'JJ'),\n",
       "  ('study', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('specifically', 'RB'),\n",
       "  ('aimed', 'VBN'),\n",
       "  ('at', 'IN'),\n",
       "  ('analyzing', 'VBG'),\n",
       "  ('mate', 'NN'),\n",
       "  ('acquisition', 'NN'),\n",
       "  ('success', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('relation', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('UV', 'NN'),\n",
       "  ('blockage', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Thus', 'RB'),\n",
       "  (',', ','),\n",
       "  ('analyzing', 'NN'),\n",
       "  ('access', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('females', 'NNS'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('more', 'RBR'),\n",
       "  ('appropriate', 'JJ'),\n",
       "  ('level', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('than', 'IN'),\n",
       "  ('tallying', 'VBG'),\n",
       "  ('molecularly', 'RB'),\n",
       "  ('assigned', 'VBN'),\n",
       "  ('offspring', 'NN'),\n",
       "  (',', ','),\n",
       "  ('since', 'IN'),\n",
       "  ('this', 'DT'),\n",
       "  ('level', 'NN'),\n",
       "  ('would', 'MD'),\n",
       "  ('include', 'VB'),\n",
       "  ('sperm', 'NN'),\n",
       "  ('competition', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('cryptic', 'JJ'),\n",
       "  ('female', 'JJ'),\n",
       "  ('choice', 'NN'),\n",
       "  ('effects', 'NNS'),\n",
       "  ('[', 'CD'),\n",
       "  ('9', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  (',', ','),\n",
       "  ('[', 'NN'),\n",
       "  ('23', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Regardless', 'RB'),\n",
       "  (',', ','),\n",
       "  ('male', 'JJ'),\n",
       "  ('access', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('females', 'NNS'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('tightly', 'RB'),\n",
       "  ('correlated', 'VBN'),\n",
       "  ('with', 'IN'),\n",
       "  ('probability', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('paternity', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('given', 'JJ'),\n",
       "  ('female', 'JJ'),\n",
       "  ('[', 'NN'),\n",
       "  ('24', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('hence', 'RB'),\n",
       "  ('our', 'PRP$'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('should', 'MD'),\n",
       "  ('represent', 'VB'),\n",
       "  ('fitness', 'NN'),\n",
       "  ('consequences', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('UV-signaling', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('independent', 'JJ'),\n",
       "  ('of', 'IN'),\n",
       "  ('badge', 'FW'),\n",
       "  ('signaling.Our', 'FW'),\n",
       "  ('previous', 'JJ'),\n",
       "  ('work', 'NN'),\n",
       "  ('[', 'NN'),\n",
       "  ('9', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  ('shows', 'VBZ'),\n",
       "  ('the', 'DT'),\n",
       "  ('effect', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('area', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('nuptial', 'JJ'),\n",
       "  ('coloration', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('successful', 'JJ'),\n",
       "  ('mate', 'NN'),\n",
       "  ('acquisition', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('that', 'IN'),\n",
       "  ('males', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('likely', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('use', 'VB'),\n",
       "  ('both', 'DT'),\n",
       "  ('badge', 'NN'),\n",
       "  ('cues', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('other', 'JJ'),\n",
       "  ('coloration', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('assessing', 'VBG'),\n",
       "  ('rival', 'JJ'),\n",
       "  ('fighting', 'NN'),\n",
       "  ('ability', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('to', 'TO'),\n",
       "  ('avoid', 'VB'),\n",
       "  ('repeating', 'VBG'),\n",
       "  ('contests', 'NNS'),\n",
       "  ('with', 'IN'),\n",
       "  ('other', 'JJ'),\n",
       "  ('males', 'NNS'),\n",
       "  ('[', 'CD'),\n",
       "  ('9', 'CD'),\n",
       "  (']', 'FW'),\n",
       "  ('–', 'FW'),\n",
       "  ('[', 'NN'),\n",
       "  ('11', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('How', 'WRB'),\n",
       "  ('UV', 'NN'),\n",
       "  ('signaling', 'NN'),\n",
       "  ('adds', 'VBZ'),\n",
       "  ('additional', 'JJ'),\n",
       "  ('information', 'NN'),\n",
       "  (',', ','),\n",
       "  ('or', 'CC'),\n",
       "  ('makes', 'VBZ'),\n",
       "  ('already', 'RB'),\n",
       "  ('described', 'VBN'),\n",
       "  ('traits', 'NNS'),\n",
       "  ('such', 'JJ'),\n",
       "  ('as', 'IN'),\n",
       "  ('size', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('fighting', 'VBG'),\n",
       "  ('ability', 'NN'),\n",
       "  ('more', 'JJR'),\n",
       "  ('(', 'CD'),\n",
       "  ('or', 'CC'),\n",
       "  ('less', 'JJR'),\n",
       "  (')', 'NN'),\n",
       "  ('easily', 'RB'),\n",
       "  ('or', 'CC'),\n",
       "  ('accurately', 'RB'),\n",
       "  ('perceived', 'VBN'),\n",
       "  ('can', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('be', 'VB'),\n",
       "  ('deduced', 'VBN'),\n",
       "  ('from', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('current', 'JJ'),\n",
       "  ('experiment', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('However', 'RB'),\n",
       "  (',', ','),\n",
       "  ('since', 'IN'),\n",
       "  ('UV', 'NN'),\n",
       "  ('blocking', 'NN'),\n",
       "  ('has', 'VBZ'),\n",
       "  ('an', 'DT'),\n",
       "  ('effect', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('mate', 'NN'),\n",
       "  ('acquisition', 'NN'),\n",
       "  ('across', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('male', 'JJ'),\n",
       "  ('size', 'NN'),\n",
       "  ('distribution', 'NN'),\n",
       "  ('when', 'WRB'),\n",
       "  ('badge', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('other', 'JJ'),\n",
       "  ('colour', 'NN'),\n",
       "  ('traits', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('unmanipulated', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('this', 'DT'),\n",
       "  ('seems', 'VBZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('suggest', 'VB'),\n",
       "  ('that', 'IN'),\n",
       "  ('UV', 'NN'),\n",
       "  ('signaling', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('universally', 'RB'),\n",
       "  ('employed', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('all', 'DT'),\n",
       "  ('males', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('perhaps', 'RB'),\n",
       "  ('more', 'RBR'),\n",
       "  ('important', 'JJ'),\n",
       "  ('for', 'IN'),\n",
       "  ('conveying', 'VBG'),\n",
       "  ('mere', 'JJ'),\n",
       "  ('presence', 'NN'),\n",
       "  ('than', 'IN'),\n",
       "  ('fighting', 'VBG'),\n",
       "  ('ability', 'NN'),\n",
       "  ('[', 'NN'),\n",
       "  ('9', 'CD'),\n",
       "  (']', 'FW'),\n",
       "  ('.Our', 'FW'),\n",
       "  ('results', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('interpretations', 'NNS'),\n",
       "  ('also', 'RB'),\n",
       "  ('agree', 'VBP'),\n",
       "  ('with', 'IN'),\n",
       "  ('those', 'DT'),\n",
       "  ('of', 'IN'),\n",
       "  ('two', 'CD'),\n",
       "  ('previous', 'JJ'),\n",
       "  ('studies', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('role', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('UV', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('lizard', 'NN'),\n",
       "  ('communication', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Stapley', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('Whiting', 'NNP'),\n",
       "  ('[', 'NNP'),\n",
       "  ('25', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  ('showed', 'VBD'),\n",
       "  ('with', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('field', 'NN'),\n",
       "  ('experiment', 'NN'),\n",
       "  ('that', 'IN'),\n",
       "  ('males', 'NNS'),\n",
       "  ('with', 'IN'),\n",
       "  ('reduced', 'VBN'),\n",
       "  ('UV', 'NN'),\n",
       "  ('signals', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('Platysaurus', 'FW'),\n",
       "  ('broadleyi', 'FW'),\n",
       "  ('were', 'VBD'),\n",
       "  ('more', 'RBR'),\n",
       "  ('likely', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('be', 'VB'),\n",
       "  ('challenged', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('rivals', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('and', 'CC'),\n",
       "  ('Bajer', 'NNP'),\n",
       "  ('et', 'FW'),\n",
       "  ('al', 'FW'),\n",
       "  ('.', '.')],\n",
       " [('[', 'NN'),\n",
       "  ('26', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  ('showed', 'VBD'),\n",
       "  ('that', 'IN'),\n",
       "  ('male', 'JJ'),\n",
       "  ('green', 'JJ'),\n",
       "  ('lizards', 'NNS'),\n",
       "  ('(', 'FW'),\n",
       "  ('Lacerta', 'FW'),\n",
       "  ('viridis', 'FW'),\n",
       "  (')', 'FW'),\n",
       "  ('with', 'IN'),\n",
       "  ('reduced', 'VBN'),\n",
       "  ('UV', 'NN'),\n",
       "  ('signals', 'NNS'),\n",
       "  ('were', 'VBD'),\n",
       "  ('less', 'RBR'),\n",
       "  ('spatially', 'RB'),\n",
       "  ('associated', 'VBN'),\n",
       "  ('with', 'IN'),\n",
       "  ('by', 'IN'),\n",
       "  ('females', 'NNS'),\n",
       "  ('.', '.')],\n",
       " [('In', 'IN'),\n",
       "  ('sand', 'NN'),\n",
       "  ('lizards', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('we', 'PRP'),\n",
       "  ('have', 'VBP'),\n",
       "  ('never', 'RB'),\n",
       "  ('been', 'VBN'),\n",
       "  ('able', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('demonstrate', 'VB'),\n",
       "  ('that', 'IN'),\n",
       "  ('there', 'EX'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('female', 'JJ'),\n",
       "  ('choice', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('male', 'JJ'),\n",
       "  ('colour', 'NN'),\n",
       "  ('traits', 'NNS'),\n",
       "  ('whereas', 'IN'),\n",
       "  ('there', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('strong', 'JJ'),\n",
       "  ('effects', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('male', 'JJ'),\n",
       "  ('green', 'JJ'),\n",
       "  ('badges', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('male', 'JJ'),\n",
       "  ('contest', 'NN'),\n",
       "  ('behavoiurs', 'FW'),\n",
       "  ('[', 'FW'),\n",
       "  ('9', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  (',', ','),\n",
       "  ('[', 'NN'),\n",
       "  ('10', 'CD'),\n",
       "  (']', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Thus', 'RB'),\n",
       "  (',', ','),\n",
       "  ('we', 'PRP'),\n",
       "  ('conclude', 'VBP'),\n",
       "  ('that', 'IN'),\n",
       "  ('male', 'JJ'),\n",
       "  ('UV', 'NN'),\n",
       "  ('reduction', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('this', 'DT'),\n",
       "  ('species', 'NN'),\n",
       "  ('compromises', 'VBZ'),\n",
       "  ('mate', 'NN'),\n",
       "  ('acquisition', 'NN'),\n",
       "  ('but', 'CC'),\n",
       "  ('that', 'IN'),\n",
       "  ('it', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('unresolved', 'JJ'),\n",
       "  ('in', 'IN'),\n",
       "  ('free-ranging', 'JJ'),\n",
       "  ('animals', 'NNS'),\n",
       "  ('whether', 'IN'),\n",
       "  ('this', 'DT'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('combined', 'JJ'),\n",
       "  ('effect', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('male-male', 'JJ'),\n",
       "  ('rivalry', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('female', 'JJ'),\n",
       "  ('choice', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('UV', 'NN'),\n",
       "  ('components', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('signalling.In', 'NN'),\n",
       "  ('summary', 'NN'),\n",
       "  (',', ','),\n",
       "  ('our', 'PRP$'),\n",
       "  ('field', 'NN'),\n",
       "  ('experiment', 'NN'),\n",
       "  ('demonstrates', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('technique', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('long-term', 'JJ'),\n",
       "  ('elimination', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('UV', 'NN'),\n",
       "  ('signaling', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('free-ranging', 'JJ'),\n",
       "  ('lizards', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('which', 'WDT'),\n",
       "  ('reduces', 'VBZ'),\n",
       "  ('success', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('mate', 'NN'),\n",
       "  ('acquisition', 'NN'),\n",
       "  (',', ','),\n",
       "  ('probably', 'RB'),\n",
       "  ('through', 'IN'),\n",
       "  ('reduced', 'VBN'),\n",
       "  ('deterrence', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('rivals', 'NNS'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the POS tagged sentences for the first article. \n",
    "plos_df['POS_sents'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plos_df.to_pickle('../data/plos_POS_sample.pk1') #saves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting nouns...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(')', 276),\n",
       " ('(', 137),\n",
       " ('[', 114),\n",
       " (']', 113),\n",
       " ('ILC3', 109),\n",
       " ('IL-23', 86),\n",
       " ('analysis', 78),\n",
       " ('laboratory', 65),\n",
       " ('study', 57),\n",
       " ('%', 55),\n",
       " ('mc', 42),\n",
       " ('number', 33),\n",
       " ('disease', 33),\n",
       " ('profile', 33),\n",
       " ('group', 33),\n",
       " ('tissue', 32),\n",
       " ('injection', 32),\n",
       " ('speed', 32),\n",
       " ('Fig', 31),\n",
       " ('n', 31),\n",
       " ('mortality', 31),\n",
       " ('UC', 31),\n",
       " ('risk', 29),\n",
       " ('signal', 28),\n",
       " ('BAV-A', 28),\n",
       " ('fHR', 27),\n",
       " ('TAV-A', 26),\n",
       " ('aneurysm', 25),\n",
       " ('TNFα', 25),\n",
       " ('gait', 24),\n",
       " ('staining', 24),\n",
       " ('population', 23),\n",
       " ('expression', 23),\n",
       " ('BPRSA', 23),\n",
       " ('UV', 21),\n",
       " ('frequency', 21),\n",
       " ('response', 20),\n",
       " ('model', 19),\n",
       " ('male', 19),\n",
       " ('age', 19),\n",
       " ('post', 19),\n",
       " ('time', 18),\n",
       " ('sample', 18),\n",
       " ('control', 18),\n",
       " ('p', 18),\n",
       " ('treatment', 18),\n",
       " ('loss', 18),\n",
       " ('cell', 18),\n",
       " ('C', 18),\n",
       " ('line', 18)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Counting nouns...')\n",
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in plos_df['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'radiospectrometric', 'genetic', 'off-line', 'simultaneous', 'Welch-spectral', 'additional', 'spectral', 'cross-spectral', 'metabolomic', 'univariate', 'cytometric', 'novel', 'in-depth'}\n"
     ]
    }
   ],
   "source": [
    "#checks the adjectives that modify a specified noun\n",
    "NTarget = 'JJ'\n",
    "Word = 'analysis'\n",
    "NResults = set()\n",
    "for entry in plos_df['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting adjectives...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('=', 61),\n",
       " ('aortic', 46),\n",
       " ('significant', 37),\n",
       " ('other', 24),\n",
       " ('valve-associated', 24),\n",
       " ('clinical', 23),\n",
       " ('fetal', 21),\n",
       " ('intestinal', 20),\n",
       " ('male', 19),\n",
       " ('specific', 17),\n",
       " ('thoracic', 17),\n",
       " ('tricuspid', 17),\n",
       " ('spectral', 16),\n",
       " ('such', 15),\n",
       " ('single', 15),\n",
       " ('TAV-Diss', 15),\n",
       " ('proximal', 15),\n",
       " ('high', 14),\n",
       " ('general', 14),\n",
       " ('individual', 14),\n",
       " ('similar', 14),\n",
       " ('first', 13),\n",
       " ('different', 13),\n",
       " ('low', 12),\n",
       " ('absolute', 12),\n",
       " ('female', 11),\n",
       " ('several', 11),\n",
       " ('black', 11),\n",
       " ('available', 11),\n",
       " ('sex-dependent', 11),\n",
       " ('multiple', 10),\n",
       " ('additional', 10),\n",
       " ('open', 10),\n",
       " ('potential', 10),\n",
       " ('abnormal', 10),\n",
       " ('RORγt+', 10),\n",
       " ('visual', 10),\n",
       " ('early', 9),\n",
       " ('same', 9),\n",
       " ('total', 9)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counts adjectives\n",
    "print('Counting adjectives...')\n",
    "countTarget = 'JJ'\n",
    "targetCounts = {}\n",
    "for entry in plos_df['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting verbs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('be', 68),\n",
       " ('reveal', 8),\n",
       " ('assess', 7),\n",
       " ('differ', 6),\n",
       " ('predict', 5),\n",
       " ('provide', 5),\n",
       " ('have', 4),\n",
       " ('investigate', 4),\n",
       " ('drive', 4),\n",
       " ('determine', 4),\n",
       " ('test', 3),\n",
       " ('see', 3),\n",
       " ('represent', 3),\n",
       " ('observe', 3),\n",
       " ('lead', 3),\n",
       " ('benefit', 3),\n",
       " ('identify', 3),\n",
       " ('obtain', 3),\n",
       " ('produce', 3),\n",
       " ('cut', 2),\n",
       " ('show', 2),\n",
       " ('mate', 2),\n",
       " ('verify', 2),\n",
       " ('include', 2),\n",
       " ('use', 2),\n",
       " ('avoid', 2),\n",
       " ('demonstrate', 2),\n",
       " ('define', 2),\n",
       " ('indicate', 2),\n",
       " ('increase', 2),\n",
       " ('do', 2),\n",
       " ('induce', 2),\n",
       " ('get', 2),\n",
       " ('allow', 2),\n",
       " ('result', 2),\n",
       " ('reduce', 2),\n",
       " ('focus', 2),\n",
       " ('form', 2),\n",
       " ('differentiate', 2),\n",
       " ('stratify', 2)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counts verbs\n",
    "print('Counting verbs...')\n",
    "countTarget = 'VB'\n",
    "targetCounts = {}\n",
    "for entry in plos_df['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not'}\n"
     ]
    }
   ],
   "source": [
    "#checks the adverbs that modify a specified verb\n",
    "NTarget = 'RB'\n",
    "Word = 'differ'\n",
    "NResults = set()\n",
    "for entry in plos_df['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With many of the adverbs in the dataset it seems that they are mostly used for existial modulation. As is shown immediately below the most frequent adverb is 'not' and the second is 'also' with a fairly large dropoff in the frequency of adverbs after that. So the adverbs are modulating verbs in terms of whether they exist ('not focus', 'not observe') and whether they are instantiated with other words ('also investigate', 'also reveal'), but what kind of words I cannot say. I am curious to know if this is a feature that is characteristic of the particular dataset or is a typical characteristic of adverbs in english generally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting adverbs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('not', 53),\n",
       " ('also', 31),\n",
       " ('significantly', 18),\n",
       " ('only', 14),\n",
       " ('previously', 11)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counts adverbs\n",
    "print('Counting adverbs...')\n",
    "countTarget = 'RB'\n",
    "targetCounts = {}\n",
    "for entry in plos_df['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to know the contexts in which some of these adjectives occur so I am going to use the concordances from Corpus Linguistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/Anaconda3-5.0.0.1-el7-x86_64/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/software/Anaconda3-5.0.0.1-el7-x86_64/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "plos_df['tokenized_text'] = plos_df['Article Contents'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "plos_df['word_counts'] = plos_df['tokenized_text'].apply(lambda x: len(x))\n",
    "\n",
    "plosIndex = nltk.text.ConcordanceIndex(plos_df['tokenized_text'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 53 matches:\n",
      " , this ignores that the exuvia may not show the same spectroradiometry cha\n",
      "at UV-blocked and control males did not differ in snout-vent length , mass \n",
      " easily or accurately perceived can not be deduced from the current experim\n",
      "nesis of many forms of aneurysms is not clear , which has so far hampered p\n",
      "allmarks ” of ATAA pathogenesis may not occur generally , and ( non-syndrom\n",
      "s > 30 % ) or metabolites which did not exceed a signal intensity of a mini\n",
      "ces were evident . Fig 1 shows that not a single metabolite ( out of 92 det\n",
      " comparisons for sphingomyelins did not result in further differences howev\n",
      "AV-A vs TAV-A was indicated but did not differ significantly ( p = 0.058 ) \n",
      "ples per group , the difference did not reach significance . For details , \n",
      "ids , amino acids , ceramides ) did not reveal significant differences ( se\n",
      "speculated that TAV-A formation may not be related to the metabolic factors\n",
      "sical pro-atherogenic processes may not play a role in these forms of aorti\n",
      "rosis risk factors and processes do not seem to constitute central elements\n",
      "els of ceramides in BAV-A samples ( not significant ) suggests that compare\n",
      "pal of Occam`s razor and clearly is not more than a hypothesis which needs \n",
      "y analysing the diseased tissue and not serum which mirrors the metabolome \n",
      "omarker search was accordingly , to not focus on systemic factors , but to \n",
      "ntiation of TAV-A from controls was not possible . As above mentioned , the\n",
      "y shows that single metabolites can not be used as biomarkers to differenti\n",
      " samples and healthy controls could not be achieved by single proteins , bu\n",
      "argue these differences , and could not reveal subtype specific characteris\n",
      ", due to the analysis of tissue and not serum/plasma the applicability of t\n",
      "y the aorta tissue.Finally , it can not be excluded that unknown confounder\n",
      "A . Comparisons : TAV-A versus C is not given , because no significant diff\n"
     ]
    }
   ],
   "source": [
    "plosIndex.print_concordance('not')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is also a classification task, which identifies named objects. Included with Stanford NER are a 4 class model trained on the CoNLL 2003 eng.train, a 7 class model trained on the MUC 6 and MUC 7 training data sets, and a 3 class model trained on both data sets plus some additional data (including ACE 2002 and limited data in-house) on the intersection of those class sets. \n",
    "\n",
    "**3 class**:\tLocation, Person, Organization\n",
    "\n",
    "**4 class**:\tLocation, Person, Organization, Misc\n",
    "\n",
    "**7 class**:\tLocation, Person, Organization, Money, Percent, Date, Time\n",
    "\n",
    "These models each use distributional similarity features, which provide some performance gain at the cost of increasing their size and runtime. Also available are the same models missing those features.\n",
    "\n",
    "(We note that the training data for the 3 class model does not include any material from the CoNLL eng.testa or eng.testb data sets, nor any of the MUC 6 or 7 test or devtest datasets, nor Alan Ritter's Twitter NER data, so all of these would be valid tests of its performance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we tag our first set of exemplary sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'O'), ('saw', 'O'), ('the', 'O'), ('elephant', 'O'), ('in', 'O'), ('my', 'O'), ('pajamas', 'O'), ('.', 'O')], [('The', 'O'), ('quick', 'O'), ('brown', 'O'), ('fox', 'O'), ('jumped', 'O'), ('over', 'O'), ('the', 'O'), ('lazy', 'O'), ('dog', 'O'), ('.', 'O')], [('While', 'O'), ('in', 'O'), ('France', 'LOCATION'), (',', 'O'), ('Christine', 'PERSON'), ('Lagarde', 'PERSON'), ('discussed', 'O'), ('short-term', 'O'), ('stimulus', 'O'), ('efforts', 'O'), ('in', 'O'), ('a', 'O'), ('recent', 'O'), ('interview', 'O'), ('with', 'O'), ('the', 'O'), ('Wall', 'ORGANIZATION'), ('Street', 'ORGANIZATION'), ('Journal', 'ORGANIZATION'), ('.', 'O')], [('Trayvon', 'PERSON'), ('Benjamin', 'PERSON'), ('Martin', 'PERSON'), ('was', 'O'), ('an', 'O'), ('African', 'O'), ('American', 'O'), ('from', 'O'), ('Miami', 'LOCATION'), ('Gardens', 'LOCATION'), (',', 'O'), ('Florida', 'LOCATION'), (',', 'O'), ('who', 'O'), (',', 'O'), ('at', 'O'), ('17', 'O'), ('years', 'O'), ('old', 'O'), (',', 'O'), ('was', 'O'), ('fatally', 'O'), ('shot', 'O'), ('by', 'O'), ('George', 'PERSON'), ('Zimmerman', 'PERSON'), (',', 'O'), ('a', 'O'), ('neighborhood', 'O'), ('watch', 'O'), ('volunteer', 'O'), (',', 'O'), ('in', 'O'), ('Sanford', 'LOCATION'), (',', 'O'), ('Florida', 'LOCATION'), ('.', 'O')], [('Buffalo', 'LOCATION'), ('buffalo', 'O'), ('Buffalo', 'ORGANIZATION'), ('buffalo', 'O'), ('buffalo', 'O'), ('buffalo', 'O'), ('Buffalo', 'ORGANIZATION'), ('buffalo', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "classified_sents = stanford.nerTagger.tag_sents(tokenized_text)\n",
    "print(classified_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run NER over our entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditTopScores['classified_sents'] = redditTopScores['sentences'].apply(lambda x: stanford.nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    [[(Last, O), (year, O), (,, O), (Help, O), (De...\n",
       "8    [[(First, O), (post, O), (in, O), (quite, O), ...\n",
       "7    [[([, O), (Original, O), (Post, O), (], O), ((...\n",
       "6    [[(I, O), (witnessed, O), (this, O), (astoundi...\n",
       "5    [[(I, O), (work, O), (Helpdesk, ORGANIZATION),...\n",
       "4    [[(This, O), (just, O), (happened, O), (..., O...\n",
       "3    [[(Another, O), (tale, O), (from, O), (the, O)...\n",
       "2    [[([, O), (Part, O), (1, O), (], O), ((, O), (...\n",
       "1    [[(>, O), ($, O), (Me, O), (-, O), (Hello, O),...\n",
       "0    [[(So, O), (my, O), (story, O), (starts, O), (...\n",
       "Name: classified_sents, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores['classified_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common entities (which are, of course, boring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 401),\n",
       " ('I', 245),\n",
       " ('the', 226),\n",
       " (',', 205),\n",
       " ('to', 197),\n",
       " ('a', 143),\n",
       " ('and', 135),\n",
       " ('>', 106),\n",
       " ('you', 102),\n",
       " ('of', 97)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if ent in entityCounts:\n",
    "                entityCounts[ent] += 1\n",
    "            else:\n",
    "                entityCounts[ent] = 1\n",
    "sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedEntities[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or those occurring only twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'Desk',\n",
       " 'busy',\n",
       " 'fix',\n",
       " 'received',\n",
       " 'couple',\n",
       " 'Windows',\n",
       " 'anymore',\n",
       " 'Sure',\n",
       " 'error',\n",
       " 'DVD',\n",
       " 'opened',\n",
       " 'There',\n",
       " 'upside',\n",
       " 'local',\n",
       " 'bane',\n",
       " 'existence',\n",
       " 'learn',\n",
       " 'sometimes',\n",
       " 'generic',\n",
       " 'Everyone',\n",
       " 'login',\n",
       " 'times',\n",
       " 'guy',\n",
       " 'asset',\n",
       " 'name',\n",
       " 'Computer',\n",
       " 'nothing',\n",
       " \"'P4ssword\",\n",
       " 'P',\n",
       " 'Everything',\n",
       " 'case',\n",
       " '*type',\n",
       " 'S',\n",
       " 'LOWERCASE',\n",
       " 'used',\n",
       " 'four',\n",
       " 'Original',\n",
       " 'cancer',\n",
       " 'month',\n",
       " 'live',\n",
       " 'brave',\n",
       " 'bitter',\n",
       " 'passed',\n",
       " 'ago',\n",
       " 'absolutely',\n",
       " 'ready',\n",
       " 'proud',\n",
       " 'above',\n",
       " 'completely',\n",
       " 'its',\n",
       " 'meant',\n",
       " 'both',\n",
       " 'sharing',\n",
       " 'making',\n",
       " '100',\n",
       " 'share',\n",
       " 'looking',\n",
       " 'ALL',\n",
       " 'whom',\n",
       " 'business',\n",
       " 'whose',\n",
       " 'stronger',\n",
       " 'bad',\n",
       " 'mess',\n",
       " 'turn',\n",
       " 'first',\n",
       " 'others',\n",
       " 'Here',\n",
       " 'suggested',\n",
       " 'videos',\n",
       " 'While',\n",
       " 'stand',\n",
       " 'certain',\n",
       " 'enjoy',\n",
       " 'well',\n",
       " 'drowned',\n",
       " 'soon',\n",
       " 'understand',\n",
       " 'risks',\n",
       " 'myself',\n",
       " 'point',\n",
       " 'future',\n",
       " 'avoid',\n",
       " 'thinking',\n",
       " 'information',\n",
       " 'insurance',\n",
       " 'site',\n",
       " 'step',\n",
       " 'guide',\n",
       " 'discover',\n",
       " 'order',\n",
       " '5',\n",
       " 'slightly',\n",
       " 'spent',\n",
       " 'moment',\n",
       " 'arms',\n",
       " 'idea',\n",
       " 'food',\n",
       " 'party',\n",
       " 'played',\n",
       " 'family',\n",
       " 'allowed',\n",
       " 'cry',\n",
       " 'pretty',\n",
       " 'nice',\n",
       " 'loved',\n",
       " 'mind',\n",
       " 'favor',\n",
       " 'watched',\n",
       " 'Things',\n",
       " '17',\n",
       " 'small',\n",
       " 'lived',\n",
       " 'living',\n",
       " 'themselves',\n",
       " 'potential',\n",
       " 'happiness',\n",
       " 'sound',\n",
       " 'situation',\n",
       " 'believe',\n",
       " 'mistakes',\n",
       " 'same',\n",
       " 'scenario',\n",
       " 'difference',\n",
       " 'glad',\n",
       " 'flaws',\n",
       " 'stupid',\n",
       " 'yourself',\n",
       " 'ok',\n",
       " 'In',\n",
       " 'comments',\n",
       " 'SO',\n",
       " 'random',\n",
       " 'request',\n",
       " 'Give',\n",
       " 'THIS',\n",
       " 'response',\n",
       " 'Thanks',\n",
       " 'tears',\n",
       " 'helped',\n",
       " 'reply',\n",
       " 'large',\n",
       " 'academic',\n",
       " 'organization',\n",
       " 'CEO',\n",
       " 'Of',\n",
       " 'course',\n",
       " 'mailboxes',\n",
       " 'Fail',\n",
       " '#',\n",
       " 'check',\n",
       " 'generate',\n",
       " '=',\n",
       " 'real',\n",
       " 'stopped',\n",
       " 'avalanche',\n",
       " 'died',\n",
       " 'systems',\n",
       " 'staff',\n",
       " 'brought',\n",
       " 'retail',\n",
       " 'store',\n",
       " 'plugged',\n",
       " 'hear',\n",
       " 'occasionally',\n",
       " 'operate',\n",
       " 'drawer*',\n",
       " 'try',\n",
       " 'hit',\n",
       " '*I',\n",
       " 'echo',\n",
       " 'heard',\n",
       " 'seconds',\n",
       " 'nose',\n",
       " 'working',\n",
       " 'BING',\n",
       " 'THE',\n",
       " '*Note',\n",
       " 'yes',\n",
       " 'different',\n",
       " 'search',\n",
       " 'immediately',\n",
       " 'connection',\n",
       " 'Turns',\n",
       " 'shortcut',\n",
       " 'okay',\n",
       " 'computering',\n",
       " 'taking',\n",
       " 'Steve',\n",
       " 'XYZ',\n",
       " 'however',\n",
       " 'forwarded',\n",
       " 'using',\n",
       " 'meet',\n",
       " 'ran',\n",
       " 'mouse',\n",
       " 'closed',\n",
       " 'window',\n",
       " 'revealing',\n",
       " 'me*',\n",
       " 'shaking',\n",
       " 'lunch',\n",
       " 'yelled',\n",
       " 'needed',\n",
       " 'key',\n",
       " 'week',\n",
       " 'pointed',\n",
       " 'door',\n",
       " 'blinked',\n",
       " 'Then',\n",
       " 'command',\n",
       " 'three',\n",
       " 'web',\n",
       " 'pages',\n",
       " 'person',\n",
       " 'Whatever',\n",
       " 'um',\n",
       " 'wrong',\n",
       " 'mother',\n",
       " 'done',\n",
       " 'way',\n",
       " 'weeks',\n",
       " 'since',\n",
       " 'stuff',\n",
       " 'took',\n",
       " 'Wow',\n",
       " 'gildings',\n",
       " 'One',\n",
       " 'HR',\n",
       " 'select',\n",
       " '*Are',\n",
       " 'Ca',\n",
       " 'building',\n",
       " 'holiday',\n",
       " 'pay',\n",
       " 'gods',\n",
       " 'expire',\n",
       " '60',\n",
       " 'anyway',\n",
       " 'User',\n",
       " 'DeskMugPhonePencil1',\n",
       " 'Thursday',\n",
       " 'return',\n",
       " 'calls',\n",
       " 'often',\n",
       " 'issues',\n",
       " 'service',\n",
       " 'older',\n",
       " 'types',\n",
       " 'speak',\n",
       " 'box',\n",
       " 'properly',\n",
       " 'personally',\n",
       " 'ask',\n",
       " 'supervisor',\n",
       " 'earlier',\n",
       " 'visit',\n",
       " 'terrible',\n",
       " 'willing',\n",
       " 'nasty']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in sortedEntities if x[1] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also list the most common \"non-objects\". (We note that we're not graphing these because there are so few here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jack', 17),\n",
       " ('Google', 6),\n",
       " ('Smith', 5),\n",
       " ('Steve', 2),\n",
       " ('Citrix', 1),\n",
       " ('Nono', 1),\n",
       " ('Reddit', 1),\n",
       " ('Helpdesk', 1),\n",
       " ('UK', 1),\n",
       " ('CMD', 1)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonObjCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind == 'O':\n",
    "                continue\n",
    "            elif ent in nonObjCounts:\n",
    "                nonObjCounts[ent] += 1\n",
    "            else:\n",
    "                nonObjCounts[ent] = 1\n",
    "sortedNonObj = sorted(nonObjCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedNonObj[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the Organizations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', 6), ('Citrix', 1), ('Helpdesk', 1), ('CMD', 1), ('GOOGLE', 1)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrgCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'ORGANIZATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These, of course, have much smaller counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform NER on a (modest) subset of your corpus of interest. List all of the different kinds of entities tagged? What does their distribution suggest about the focus of your corpus? For a subset of your corpus, tally at least one type of named entity and calculate the Precision, Recall and F-score for the NER classification just performed (using your own hand-codings as \"ground truth\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/Anaconda3-5.0.0.1-el7-x86_64/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "plos_df['classified_sents'] = plos_df['sentences'].apply(lambda x: stanford.nerTagger.tag_sents(x))\n",
    "#plos_df['classified_sents']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "First_Article_NER = plos_df['classified_sents'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 965),\n",
       " ('the', 728),\n",
       " ('of', 679),\n",
       " ('.', 588),\n",
       " (')', 536),\n",
       " ('and', 530),\n",
       " ('(', 518),\n",
       " ('in', 355),\n",
       " ('to', 291),\n",
       " ('a', 212)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counts all the objects, these are things that are not Organizations, Persons, or Locations\n",
    "entityCounts = {}\n",
    "for entry in plos_df['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if ent in entityCounts:\n",
    "                entityCounts[ent] += 1\n",
    "            else:\n",
    "                entityCounts[ent] = 1\n",
    "sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedEntities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ATAA', 9),\n",
       " ('UC', 9),\n",
       " ('of', 8),\n",
       " ('IADL', 8),\n",
       " ('Germany', 7),\n",
       " ('University', 6),\n",
       " ('BPRSA', 6),\n",
       " ('AAA', 5),\n",
       " ('USA', 5),\n",
       " ('Leiden', 5)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonObjCounts = {}\n",
    "for entry in plos_df['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind == 'O':\n",
    "                continue\n",
    "            elif ent in nonObjCounts:\n",
    "                nonObjCounts[ent] += 1\n",
    "            else:\n",
    "                nonObjCounts[ent] = 1\n",
    "sortedNonObj = sorted(nonObjCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedNonObj[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ATAA', 9),\n",
       " ('of', 8),\n",
       " ('IADL', 8),\n",
       " ('University', 6),\n",
       " ('UC', 6),\n",
       " ('AAA', 5),\n",
       " ('IBD', 5),\n",
       " ('Committee', 4),\n",
       " ('Medical', 4),\n",
       " ('CRP', 4)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counts the organizations\n",
    "OrgCounts = {}\n",
    "for entry in plos_df['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'ORGANIZATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counts Persons\n",
    "#counts the organizations\n",
    "OrgCounts = {}\n",
    "for entry in plos_df['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'PERSON':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Germany', 7),\n",
       " ('USA', 5),\n",
       " ('UC', 3),\n",
       " ('Berlin', 2),\n",
       " ('Innsbruck', 2),\n",
       " ('Austria', 2),\n",
       " ('Zwingen', 2),\n",
       " ('Switzerland', 2),\n",
       " ('Darmstadt', 2),\n",
       " ('Armonk', 2)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counts locations\n",
    "OrgCounts = {}\n",
    "for entry in plos_df['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'LOCATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've had some trouble figuring out how to run the precision measure's as well as comparing the NER calssified to hand codings, but inspecting the tags from the first article, printed in the next cell I can see that the tagger is getting a few things wrong, idntifying person's that are actually objects, and doing that inconsistently. It identified Lacerta Agilis as a person, though only once. Lacerta agilis being some non-object in one of the biology articles in the sub-sample. The parser missed as an Organization the Animal Ethics Committee, while considering partially idenitfying the University of Gthenburg as a organization and location. It missed Vichy Laboratores as an organization but did idntify Vichy as a location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing\n",
    "\n",
    "Here we will introduce the Stanford Parser by feeding it tokenized text from our initial example sentences. The parser is a dependency parser, but this initial program outputs a simple, self-explanatory phrase-structure representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NNP', ['Trayvon']), Tree('NNP', ['Benjamin']), Tree('NNP', ['Martin'])]), Tree('VP', [Tree('VBD', ['was']), Tree('NP', [Tree('NP', [Tree('DT', ['an']), Tree('NNP', ['African']), Tree('NNP', ['American'])]), Tree('PP', [Tree('IN', ['from']), Tree('NP', [Tree('NP', [Tree('NNP', ['Miami']), Tree('NNPS', ['Gardens'])]), Tree(',', [',']), Tree('NP', [Tree('NNP', ['Florida'])]), Tree(',', [',']), Tree('SBAR', [Tree('WHNP', [Tree('WP', ['who'])]), Tree('S', [Tree(',', [',']), Tree('PP', [Tree('IN', ['at']), Tree('ADJP', [Tree('NP', [Tree('CD', ['17']), Tree('NNS', ['years'])]), Tree('JJ', ['old'])])]), Tree(',', [',']), Tree('VP', [Tree('VBD', ['was']), Tree('ADVP', [Tree('RB', ['fatally'])]), Tree('VP', [Tree('VBN', ['shot']), Tree('PP', [Tree('IN', ['by']), Tree('NP', [Tree('NP', [Tree('NNP', ['George']), Tree('NNP', ['Zimmerman'])]), Tree(',', [',']), Tree('NP', [Tree('DT', ['a']), Tree('NN', ['neighborhood']), Tree('NN', ['watch']), Tree('NN', ['volunteer'])]), Tree(',', [','])])]), Tree('PP', [Tree('IN', ['in']), Tree('NP', [Tree('NNP', ['Sanford']), Tree(',', [',']), Tree('NNP', ['Florida'])])])])])])])])])])]), Tree('.', ['.'])])])]\n"
     ]
    }
   ],
   "source": [
    "parses = list(stanford.parser.parse_sents(tokenized_text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "fourthSentParseTree = list(parses[3]) #iterators so be careful about re-running code, without re-running this block\n",
    "print(fourthSentParseTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees are a common data structure and there are a large number of things to do with them. What we are intetered in is the relationship between different types of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treeRelation(parsetree, relationType, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retList = []\n",
    "        for subT in parsetree.subtrees():\n",
    "            if subT.label() == relationType:\n",
    "                if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                    retList.append([(subT.label(), ' '.join(subT.leaves()))])\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treeSubRelation(parsetree, relationTypeScope, relationTypeTarget, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retSet = set()\n",
    "        for subT in parsetree.subtrees():\n",
    "            if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                if subT.label() == relationTypeScope:\n",
    "                    for subsub in subT.subtrees():\n",
    "                        if subsub.label()==relationTypeTarget:\n",
    "                            retSet.add(' '.join(subsub.leaves()))\n",
    "    return retSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('NP',\n",
       "   'an African American from Miami Gardens , Florida , who , at 17 years old , was fatally shot by George Zimmerman , a neighborhood watch volunteer , in Sanford , Florida')],\n",
       " [('NP',\n",
       "   'Miami Gardens , Florida , who , at 17 years old , was fatally shot by George Zimmerman , a neighborhood watch volunteer , in Sanford , Florida')]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeRelation(fourthSentParseTree, 'NP', 'Florida', 'who')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Florida occurs twice in two different nested noun phrases in the sentence. \n",
    "\n",
    "We can also find all of the verbs within the noun phrase defined by one or more target words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shot'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeSubRelation(fourthSentParseTree, 'NP', 'VBN', 'Florida', 'who')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we want to to look at the whole tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                   ROOT                                                                                                                       \n",
      "                                                                                                                    |                                                                                                                          \n",
      "                                                                                                                    S                                                                                                                         \n",
      "            ________________________________________________________________________________________________________|_______________________________________________________________________________________________________________________   \n",
      "           |                       VP                                                                                                                                                                                                       | \n",
      "           |              _________|______________                                                                                                                                                                                          |  \n",
      "           |             |                        NP                                                                                                                                                                                        | \n",
      "           |             |          ______________|________________                                                                                                                                                                         |  \n",
      "           |             |         |                               PP                                                                                                                                                                       | \n",
      "           |             |         |               ________________|___________                                                                                                                                                             |  \n",
      "           |             |         |              |                            NP                                                                                                                                                           | \n",
      "           |             |         |              |           _________________|____________________________________                                                                                                                        |  \n",
      "           |             |         |              |          |           |     |     |                             SBAR                                                                                                                     | \n",
      "           |             |         |              |          |           |     |     |    __________________________|______________________________                                                                                         |  \n",
      "           |             |         |              |          |           |     |     |   |                                                         S                                                                                        | \n",
      "           |             |         |              |          |           |     |     |   |     ____________________________________________________|_______________________                                                                 |  \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |                                                 VP                                                               | \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |    _____________________________________________|_______                                                         |  \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |   |     |                                               VP                                                       | \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |   |     |      _________________________________________|________________________________________                |  \n",
      "           |             |         |              |          |           |     |     |   |    |           PP             |   |     |     |                      PP                                                          |               | \n",
      "           |             |         |              |          |           |     |     |   |    |    _______|____          |   |     |     |     _________________|__________                                                 |               |  \n",
      "           |             |         |              |          |           |     |     |   |    |   |           ADJP       |   |     |     |    |                            NP                                               PP              | \n",
      "           |             |         |              |          |           |     |     |   |    |   |        ____|____     |   |     |     |    |           _________________|________________________________     ___________|___            |  \n",
      "           NP            |         NP             |          NP          |     NP    |  WHNP  |   |       NP        |    |   |    ADVP   |    |          NP            |           NP                       |   |               NP          | \n",
      "    _______|_______      |    _____|_______       |      ____|_____      |     |     |   |    |   |    ___|____     |    |   |     |     |    |     _____|______       |    _______|_________________       |   |      _________|_____      |  \n",
      "  NNP     NNP     NNP   VBD  DT   NNP     NNP     IN   NNP        NNPS   ,    NNP    ,   WP   ,   IN  CD      NNS   JJ   ,  VBD    RB   VBN   IN  NNP          NNP     ,   DT      NN        NN      NN     ,   IN   NNP        ,    NNP    . \n",
      "   |       |       |     |   |     |       |      |     |          |     |     |     |   |    |   |   |        |    |    |   |     |     |    |    |            |      |   |       |         |       |      |   |     |         |     |     |  \n",
      "Trayvon Benjamin Martin was  an African American from Miami     Gardens  ,  Florida  ,  who   ,   at  17     years old   ,  was fatally shot  by George     Zimmerman  ,   a  neighborhood watch volunteer  ,   in Sanford      ,  Florida  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fourthSentParseTree[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ROOT                           \n",
      "                      |                              \n",
      "                      S                             \n",
      "       _______________|___________________________   \n",
      "      |                          VP               | \n",
      "      |                __________|___             |  \n",
      "      |               |              PP           | \n",
      "      |               |      ________|___         |  \n",
      "      NP              |     |            NP       | \n",
      "  ____|__________     |     |     _______|____    |  \n",
      " DT   JJ    JJ   NN  VBD    IN   DT      JJ   NN  . \n",
      " |    |     |    |    |     |    |       |    |   |  \n",
      "The quick brown fox jumped over the     lazy dog  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "list(parses[1])[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing and graph representations\n",
    "\n",
    "Dependency parsing was developed to robustly capture linguistic dependencies from text. The complex tags associated with these parses are detailed [here]('http://universaldependencies.org/u/overview/syntax.html'). When parsing with the dependency parser, we will work directly from the untokenized text. Note that no *processing* takes place before parsing sentences--we do not remove so-called stop words or anything that plays a syntactic role in the sentence, although anaphora resolution and related normalization may be performed before or after parsing to enhance the value of information extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x7feff7334b70>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [5]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'The'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'quick'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'brown'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'amod': [2, 3],\n",
      "                                      'det': [1]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'fox'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'VBD',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'nmod': [9],\n",
      "                                      'nsubj': [4]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'VBD',\n",
      "                 'word': 'jumped'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'case',\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'over'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'the'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'lazy'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'amod': [8],\n",
      "                                      'case': [6],\n",
      "                                      'det': [7]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nmod',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'dog'}})\n"
     ]
    }
   ],
   "source": [
    "depParses = list(stanford.depParser.raw_parse_sents(text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "secondSentDepParseTree = list(depParses[1])[0] #iterators so be careful about re-running code, without re-running this block\n",
    "print(secondSentDepParseTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a graph and we can convert it to a dot file and use that to visulize it. Try traversing the tree and extracting elements that are nearby one another. We note that unless you have the graphviz successfully installed on your computer (which is not necessary to complete this homework), the following graphviz call will trigger an error. If you are interested in installing graphviz and working on a Mac, consider installing through [homebrew](https://brew.sh), a package manager (i.e., with the command \"brew install graphviz\", once brew is installed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    secondSentGraph = graphviz.Source(secondSentDepParseTree.to_dot())\n",
    "except:\n",
    "    secondSentGraph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "secondSentGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"961pt\" height=\"566pt\"\n",
       " viewBox=\"0.00 0.00 961.00 566.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 562)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-562 957,-562 957,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"232\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node2\" class=\"node\"><title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"232\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">7 (American)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;7 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M232,-521.799C232,-510.163 232,-494.548 232,-481.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"235.5,-481.175 232,-471.175 228.5,-481.175 235.5,-481.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">root</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\"><title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"74\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">3 (Martin)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>7&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193.462,-434.963C181.635,-429.483 168.668,-423.214 157,-417 141.139,-408.552 124.062,-398.448 109.546,-389.541\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"111.218,-386.459 100.872,-384.175 107.535,-392.412 111.218,-386.459\"/>\n",
       "<text text-anchor=\"middle\" x=\"172\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\"><title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"158\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">4 (was)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>7&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M217.025,-434.799C206.293,-422.471 191.673,-405.679 179.669,-391.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"182.159,-389.42 172.953,-384.175 176.879,-394.016 182.159,-389.42\"/>\n",
       "<text text-anchor=\"middle\" x=\"210\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">cop</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\"><title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"232\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">5 (an)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>7&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M232,-434.799C232,-423.163 232,-407.548 232,-394.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"235.5,-394.175 232,-384.175 228.5,-394.175 235.5,-394.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"240.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\"><title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"316\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">6 (African)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M248.999,-434.799C261.295,-422.356 278.087,-405.364 291.784,-391.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"294.487,-393.748 299.027,-384.175 289.508,-388.828 294.487,-393.748\"/>\n",
       "<text text-anchor=\"middle\" x=\"309\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node9\" class=\"node\"><title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"417\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">10 (Gardens)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;10 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>7&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M277.249,-440.698C297.554,-434.849 321.499,-426.853 342,-417 357.504,-409.548 373.521,-399.277 386.753,-389.982\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"388.929,-392.729 395.012,-384.054 384.847,-387.042 388.929,-392.729\"/>\n",
       "<text text-anchor=\"middle\" x=\"384\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\"><title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"41\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">1 (Trayvon)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\"><title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"145\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">2 (Benjamin)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M67.322,-347.799C62.7596,-336.047 56.6219,-320.238 51.4211,-306.842\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54.5501,-305.231 47.6681,-297.175 48.0246,-307.764 54.5501,-305.231\"/>\n",
       "<text text-anchor=\"middle\" x=\"89\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M102.456,-347.911C109.458,-342.777 116.513,-336.704 122,-330 127.594,-323.167 132.146,-314.75 135.68,-306.785\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"138.96,-308.01 139.487,-297.429 132.476,-305.372 138.96,-308.01\"/>\n",
       "<text text-anchor=\"middle\" x=\"161\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\"><title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"268\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>10&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M372.659,-348.343C360.184,-343.054 346.805,-336.795 335,-330 321.639,-322.31 307.831,-312.393 296.247,-303.422\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"298.26,-300.552 288.243,-297.102 293.922,-306.046 298.26,-300.552\"/>\n",
       "<text text-anchor=\"middle\" x=\"347\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\"><title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"355\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">9 (Miami)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>10&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M388.011,-347.942C381.329,-342.879 374.786,-336.833 370,-330 365.3,-323.291 362.049,-315.086 359.809,-307.278\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"363.172,-306.293 357.412,-297.401 356.37,-307.945 363.172,-306.293\"/>\n",
       "<text text-anchor=\"middle\" x=\"399\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node12\" class=\"node\"><title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"451\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">12 (Florida)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;12 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>10&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M423.88,-347.799C428.581,-336.047 434.905,-320.238 440.263,-306.842\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"443.666,-307.76 444.13,-297.175 437.166,-305.16 443.666,-307.76\"/>\n",
       "<text text-anchor=\"middle\" x=\"453\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">appos</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node13\" class=\"node\"><title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"543\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">23 (shot)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;23 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>10&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M445.132,-347.888C454.112,-342.301 464.041,-335.991 473,-330 485.787,-321.449 499.616,-311.685 511.579,-303.068\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"513.871,-305.73 519.918,-297.03 509.766,-300.06 513.871,-305.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"517\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node14\" class=\"node\"><title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"337\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">14 (who)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;14 -->\n",
       "<g id=\"edge16\" class=\"edge\"><title>23&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M510.212,-263.981C507.126,-262.889 504.023,-261.874 501,-261 457.617,-248.462 442.222,-261.441 401,-243 386.707,-236.606 372.772,-226.351 361.573,-216.788\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"363.739,-214.031 353.937,-210.012 359.093,-219.267 363.739,-214.031\"/>\n",
       "<text text-anchor=\"middle\" x=\"427.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubjpass</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node18\" class=\"node\"><title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"418\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">19 (old)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;19 -->\n",
       "<g id=\"edge17\" class=\"edge\"><title>23&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M510.439,-262.889C499.204,-257.185 486.749,-250.298 476,-243 464.63,-235.281 452.986,-225.644 443.143,-216.896\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"445.434,-214.248 435.675,-210.124 440.732,-219.434 445.434,-214.248\"/>\n",
       "<text text-anchor=\"middle\" x=\"491\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">advcl</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node19\" class=\"node\"><title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"498\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">21 (was)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;21 -->\n",
       "<g id=\"edge18\" class=\"edge\"><title>23&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M528.132,-260.935C523.834,-255.453 519.393,-249.192 516,-243 512.054,-235.799 508.676,-227.562 505.948,-219.879\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"509.218,-218.62 502.744,-210.233 502.575,-220.826 509.218,-218.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"537.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">auxpass</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node20\" class=\"node\"><title>22</title>\n",
       "<text text-anchor=\"middle\" x=\"587\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">22 (fatally)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;22 -->\n",
       "<g id=\"edge19\" class=\"edge\"><title>23&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M551.904,-260.799C558.107,-248.817 566.492,-232.617 573.512,-219.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"576.62,-220.665 578.109,-210.175 570.404,-217.447 576.62,-220.665\"/>\n",
       "<text text-anchor=\"middle\" x=\"590.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">advmod</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node21\" class=\"node\"><title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"700\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">26 (Zimmerman)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;26 -->\n",
       "<g id=\"edge20\" class=\"edge\"><title>23&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M575.77,-263.362C588.753,-257.34 603.722,-250.116 617,-243 632.84,-234.511 649.913,-224.4 664.432,-215.499\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"666.441,-218.372 673.108,-210.138 662.761,-212.417 666.441,-218.372\"/>\n",
       "<text text-anchor=\"middle\" x=\"659\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node22\" class=\"node\"><title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"822\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">36 (Florida)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;36 -->\n",
       "<g id=\"edge21\" class=\"edge\"><title>23&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M575.639,-270.42C603.324,-263.796 644,-253.602 679,-243 709.824,-233.663 743.881,-221.817 771.024,-212\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"772.491,-215.19 780.693,-208.483 770.099,-208.612 772.491,-215.19\"/>\n",
       "<text text-anchor=\"middle\" x=\"738\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node15\" class=\"node\"><title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"358\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">16 (at)</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node16\" class=\"node\"><title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"439\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">17 (17)</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node17\" class=\"node\"><title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"439\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">18 (years)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;17 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>18&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M439,-86.799C439,-75.1626 439,-59.5479 439,-46.2368\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"442.5,-46.1754 439,-36.1754 435.5,-46.1755 442.5,-46.1754\"/>\n",
       "<text text-anchor=\"middle\" x=\"464\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">nummod</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;16 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>19&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M405.858,-173.799C397.319,-161.702 385.745,-145.305 376.118,-131.667\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"378.75,-129.327 370.124,-123.175 373.031,-133.364 378.75,-129.327\"/>\n",
       "<text text-anchor=\"middle\" x=\"404\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;18 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>19&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M422.25,-173.799C425.125,-162.163 428.982,-146.548 432.271,-133.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"435.756,-133.723 434.757,-123.175 428.96,-132.044 435.756,-133.723\"/>\n",
       "<text text-anchor=\"middle\" x=\"467\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod:npmod</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node23\" class=\"node\"><title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"535\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">24 (by)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;24 -->\n",
       "<g id=\"edge22\" class=\"edge\"><title>26&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M652.772,-173.869C639.668,-168.616 625.588,-162.494 613,-156 597.638,-148.075 581.429,-137.994 567.791,-128.962\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"569.487,-125.885 559.235,-123.212 565.583,-131.694 569.487,-125.885\"/>\n",
       "<text text-anchor=\"middle\" x=\"625\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node24\" class=\"node\"><title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"623\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">25 (George)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;25 -->\n",
       "<g id=\"edge23\" class=\"edge\"><title>26&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M671.555,-173.873C664.323,-168.701 656.929,-162.619 651,-156 644.797,-149.076 639.392,-140.545 635.029,-132.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"638.057,-130.736 630.4,-123.413 631.819,-133.913 638.057,-130.736\"/>\n",
       "<text text-anchor=\"middle\" x=\"680\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node25\" class=\"node\"><title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"730\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">31 (volunteer)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;31 -->\n",
       "<g id=\"edge24\" class=\"edge\"><title>26&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M706.071,-173.799C710.219,-162.047 715.798,-146.238 720.526,-132.842\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"723.91,-133.77 723.938,-123.175 717.309,-131.44 723.91,-133.77\"/>\n",
       "<text text-anchor=\"middle\" x=\"732\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">appos</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node29\" class=\"node\"><title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"822\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">33 (in)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;33 -->\n",
       "<g id=\"edge28\" class=\"edge\"><title>36&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M822,-173.799C822,-162.163 822,-146.548 822,-133.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"825.5,-133.175 822,-123.175 818.5,-133.175 825.5,-133.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"834\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node30\" class=\"node\"><title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"910\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">34 (Sanford)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;34 -->\n",
       "<g id=\"edge29\" class=\"edge\"><title>36&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M839.808,-173.799C852.69,-161.356 870.282,-144.364 884.631,-130.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"887.457,-132.64 892.218,-123.175 882.594,-127.605 887.457,-132.64\"/>\n",
       "<text text-anchor=\"middle\" x=\"900\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\"><title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"626\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">28 (a)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;28 -->\n",
       "<g id=\"edge25\" class=\"edge\"><title>31&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M708.954,-86.799C693.448,-74.1257 672.168,-56.7335 655.044,-42.7377\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"656.972,-39.7938 647.015,-36.1754 652.543,-45.2138 656.972,-39.7938\"/>\n",
       "<text text-anchor=\"middle\" x=\"693.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node27\" class=\"node\"><title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"730\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">29 (neighborhood)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge26\" class=\"edge\"><title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M730,-86.799C730,-75.1626 730,-59.5479 730,-46.2368\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"733.5,-46.1754 730,-36.1754 726.5,-46.1755 733.5,-46.1754\"/>\n",
       "<text text-anchor=\"middle\" x=\"759\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node28\" class=\"node\"><title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"845\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">30 (watch)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge27\" class=\"edge\"><title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M763.745,-86.9012C773.181,-81.6054 783.244,-75.453 792,-69 802.239,-61.4544 812.611,-52.1229 821.432,-43.5725\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"824.105,-45.8504 828.742,-36.324 819.177,-40.8797 824.105,-45.8504\"/>\n",
       "<text text-anchor=\"middle\" x=\"838\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7feff7339630>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(depParses[3])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do a dependency parse on the reddit sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topPostDepParse = list(stanford.depParser.parse_sents(redditTopScores['sentences'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a few seconds, but now lets look at the parse tree from one of the processed sentences.\n",
    "\n",
    "The sentence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So anyway , I get a call from an older gentleman who 's quite bitter and mean right off the bat ( does n't like that I asked for his address / telephone number to verify the account , hates that he has to speak with a machine before reaching an agent , etc . ) .\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 7\n",
    "print(' '.join(redditTopScores['sentences'][0][targetSentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which leads to a very rich dependancy tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"1107pt\" height=\"827pt\"\n",
       " viewBox=\"0.00 0.00 1107.00 827.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 823)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-823 1103,-823 1103,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"195\" y=\"-797.3\" font-family=\"Times,serif\" font-size=\"14.00\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\"><title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"195\" y=\"-710.3\" font-family=\"Times,serif\" font-size=\"14.00\">5 (get)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M195,-782.799C195,-771.163 195,-755.548 195,-742.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"198.5,-742.175 195,-732.175 191.5,-742.175 198.5,-742.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-753.8\" font-family=\"Times,serif\" font-size=\"14.00\">root</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\"><title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">1 (So)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>5&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.995,-707.618C144.938,-702.29 111.414,-692.766 85,-678 72.7302,-671.141 60.7017,-661.418 50.8082,-652.367\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.0351,-649.656 43.3678,-645.321 48.2216,-654.738 53.0351,-649.656\"/>\n",
       "<text text-anchor=\"middle\" x=\"107.5\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">advmod</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\"><title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"111\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">2 (anyway)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>5&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.754,-696.008C160.287,-690.714 152.474,-684.536 146,-678 138.881,-670.813 132.204,-662.033 126.63,-653.847\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"129.446,-651.758 121.045,-645.302 123.586,-655.587 129.446,-651.758\"/>\n",
       "<text text-anchor=\"middle\" x=\"168.5\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">advmod</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"195\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">4 (I)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M195,-695.799C195,-684.163 195,-668.548 195,-655.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"198.5,-655.175 195,-645.175 191.5,-655.175 198.5,-655.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"210\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node6\" class=\"node\"><title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"268\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">7 (call)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M212.33,-695.905C217.796,-690.318 223.769,-684.004 229,-678 235.87,-670.114 243.012,-661.196 249.278,-653.097\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"252.15,-655.102 255.442,-645.031 246.589,-650.851 252.15,-655.102\"/>\n",
       "<text text-anchor=\"middle\" x=\"254.5\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">dobj</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node7\" class=\"node\"><title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"615\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">34 (number)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;34 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>5&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M222.234,-707.488C290.934,-693.585 472.218,-656.896 562.445,-638.636\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"563.353,-642.023 572.46,-636.609 561.964,-635.163 563.353,-642.023\"/>\n",
       "<text text-anchor=\"middle\" x=\"441\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">dep</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\"><title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"173\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">6 (a)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M248.775,-608.799C234.74,-596.241 215.526,-579.049 199.958,-565.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"201.982,-562.235 192.196,-558.175 197.315,-567.452 201.982,-562.235\"/>\n",
       "<text text-anchor=\"middle\" x=\"235.5\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node9\" class=\"node\"><title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"268\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">11 (gentleman)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;11 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>7&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M268,-608.799C268,-597.163 268,-581.548 268,-568.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"271.5,-568.175 268,-558.175 264.5,-568.175 271.5,-568.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"284\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node25\" class=\"node\"><title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"566\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">25 (like)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;25 -->\n",
       "<g id=\"edge30\" class=\"edge\"><title>34&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M605.084,-608.799C598.177,-596.817 588.838,-580.617 581.021,-567.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"583.928,-565.091 575.901,-558.175 577.863,-568.587 583.928,-565.091\"/>\n",
       "<text text-anchor=\"middle\" x=\"604\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">dep</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node32\" class=\"node\"><title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"663\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">33 (telephone)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;33 -->\n",
       "<g id=\"edge31\" class=\"edge\"><title>34&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M624.714,-608.799C631.48,-596.817 640.628,-580.617 648.286,-567.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"651.431,-568.604 653.301,-558.175 645.336,-565.162 651.431,-568.604\"/>\n",
       "<text text-anchor=\"middle\" x=\"671\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node33\" class=\"node\"><title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"782\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">36 (verify)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;36 -->\n",
       "<g id=\"edge32\" class=\"edge\"><title>34&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M657.848,-611.093C672.736,-605.407 689.363,-598.479 704,-591 719.51,-583.075 735.854,-572.909 749.552,-563.816\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"751.8,-566.521 758.136,-558.029 747.887,-560.717 751.8,-566.521\"/>\n",
       "<text text-anchor=\"middle\" x=\"736\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">acl</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\"><title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"110\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>11&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M230.934,-521.975C219.325,-516.444 206.537,-510.145 195,-504 178.857,-495.402 161.385,-485.273 146.505,-476.388\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"147.984,-473.193 137.61,-471.041 144.377,-479.193 147.984,-473.193\"/>\n",
       "<text text-anchor=\"middle\" x=\"207\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\"><title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"187\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">9 (an)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>11&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M251.608,-521.799C239.751,-509.356 223.559,-492.364 210.351,-478.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"212.8,-476 203.367,-471.175 207.732,-480.829 212.8,-476\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node12\" class=\"node\"><title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"268\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">10 (older)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M268,-521.799C268,-510.163 268,-494.548 268,-481.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"271.5,-481.175 268,-471.175 264.5,-481.175 271.5,-481.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"283.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node13\" class=\"node\"><title>15</title>\n",
       "<text text-anchor=\"middle\" x=\"357\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">15 (bitter)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;15 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>11&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M286.01,-521.799C299.16,-509.241 317.16,-492.049 331.745,-478.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"334.202,-480.613 339.016,-471.175 329.367,-475.551 334.202,-480.613\"/>\n",
       "<text text-anchor=\"middle\" x=\"340\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node14\" class=\"node\"><title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"158\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">12 (who)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;12 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>15&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M321.234,-438.183C318.122,-437.074 315.013,-435.998 312,-435 284.356,-425.847 275.93,-428.081 249,-417 230.293,-409.302 210.5,-398.746 194.169,-389.316\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"195.877,-386.26 185.481,-384.218 192.334,-392.297 195.877,-386.26\"/>\n",
       "<text text-anchor=\"middle\" x=\"264\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node15\" class=\"node\"><title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"236\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">13 (&#39;s)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;13 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>15&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M329.85,-434.899C321.189,-429.312 311.619,-423 303,-417 290.73,-408.459 277.488,-398.697 266.042,-390.079\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"268.15,-387.285 258.064,-384.04 263.925,-392.866 268.15,-387.285\"/>\n",
       "<text text-anchor=\"middle\" x=\"313\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">cop</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node16\" class=\"node\"><title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"316\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">14 (quite)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;14 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>15&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M343.809,-434.773C339.989,-429.285 336.036,-423.06 333,-417 329.345,-409.704 326.177,-401.438 323.599,-393.758\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"326.907,-392.612 320.558,-384.13 320.232,-394.72 326.907,-392.612\"/>\n",
       "<text text-anchor=\"middle\" x=\"355.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">advmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\"><title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"400\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">16 (and)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\"><title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M367.908,-434.799C371.295,-429.207 374.939,-422.916 378,-417 381.861,-409.538 385.684,-401.252 389.034,-393.607\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"392.268,-394.946 392.99,-384.375 385.834,-392.188 392.268,-394.946\"/>\n",
       "<text text-anchor=\"middle\" x=\"392.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">cc</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\"><title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"486\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">17 (mean)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\"><title>15&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M383.105,-434.799C402.776,-421.838 429.937,-403.941 451.415,-389.789\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"453.509,-392.6 459.934,-384.175 449.658,-386.755 453.509,-392.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"442\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">conj</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\"><title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"445\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">18 (right)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge18\" class=\"edge\"><title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M465.866,-347.721C460.936,-342.509 456.171,-336.444 453,-330 449.584,-323.059 447.571,-314.982 446.401,-307.369\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"449.878,-306.97 445.274,-297.428 442.922,-307.759 449.878,-306.97\"/>\n",
       "<text text-anchor=\"middle\" x=\"475.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">advmod</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node20\" class=\"node\"><title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"527\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">21 (bat)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;21 -->\n",
       "<g id=\"edge19\" class=\"edge\"><title>17&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M494.297,-347.799C500.021,-335.932 507.741,-319.928 514.242,-306.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"517.523,-307.703 518.715,-297.175 511.218,-304.662 517.523,-307.703\"/>\n",
       "<text text-anchor=\"middle\" x=\"525\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node21\" class=\"node\"><title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"480\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">19 (off)</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;19 -->\n",
       "<g id=\"edge20\" class=\"edge\"><title>21&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M517.489,-260.799C510.863,-248.817 501.906,-232.617 494.408,-219.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"497.399,-217.233 489.497,-210.175 491.273,-220.62 497.399,-217.233\"/>\n",
       "<text text-anchor=\"middle\" x=\"519\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node22\" class=\"node\"><title>20</title>\n",
       "<text text-anchor=\"middle\" x=\"557\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">20 (the)</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;20 -->\n",
       "<g id=\"edge21\" class=\"edge\"><title>21&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M533.071,-260.799C537.219,-249.047 542.798,-233.238 547.526,-219.842\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"550.91,-220.77 550.938,-210.175 544.309,-218.44 550.91,-220.77\"/>\n",
       "<text text-anchor=\"middle\" x=\"552.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node23\" class=\"node\"><title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"486\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">23 (does)</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node24\" class=\"node\"><title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"566\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">24 (n&#39;t)</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;23 -->\n",
       "<g id=\"edge22\" class=\"edge\"><title>25&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M549.811,-521.799C538.1,-509.356 522.108,-492.364 509.063,-478.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"511.568,-476.059 502.165,-471.175 506.47,-480.856 511.568,-476.059\"/>\n",
       "<text text-anchor=\"middle\" x=\"541\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">aux</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;24 -->\n",
       "<g id=\"edge23\" class=\"edge\"><title>25&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M566,-521.799C566,-510.163 566,-494.548 566,-481.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"569.5,-481.175 566,-471.175 562.5,-481.175 569.5,-481.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"576\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">neg</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\"><title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"649\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">28 (asked)</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;28 -->\n",
       "<g id=\"edge24\" class=\"edge\"><title>25&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M582.796,-521.799C594.946,-509.356 611.538,-492.364 625.072,-478.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"627.746,-480.775 632.229,-471.175 622.738,-475.885 627.746,-480.775\"/>\n",
       "<text text-anchor=\"middle\" x=\"631.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">ccomp</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\"><title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"572\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">26 (that)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;26 -->\n",
       "<g id=\"edge25\" class=\"edge\"><title>28&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M632.431,-434.663C627.14,-429.064 621.287,-422.803 616,-417 608.641,-408.922 600.72,-399.995 593.654,-391.946\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"596.144,-389.476 586.925,-384.254 590.875,-394.085 596.144,-389.476\"/>\n",
       "<text text-anchor=\"middle\" x=\"630.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\"><title>27</title>\n",
       "<text text-anchor=\"middle\" x=\"649\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">27 (I)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;27 -->\n",
       "<g id=\"edge26\" class=\"edge\"><title>28&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M649,-434.799C649,-423.163 649,-407.548 649,-394.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"652.5,-394.175 649,-384.175 645.5,-394.175 652.5,-394.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"664\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node29\" class=\"node\"><title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"736\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">31 (address)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;31 -->\n",
       "<g id=\"edge27\" class=\"edge\"><title>28&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M666.606,-434.799C679.341,-422.356 696.733,-405.364 710.919,-391.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"713.714,-393.667 718.42,-384.175 708.822,-388.66 713.714,-393.667\"/>\n",
       "<text text-anchor=\"middle\" x=\"713\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\"><title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"666\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">29 (for)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge28\" class=\"edge\"><title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M721.834,-347.799C711.777,-335.587 698.111,-318.992 686.817,-305.278\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"689.203,-302.67 680.144,-297.175 683.8,-307.12 689.203,-302.67\"/>\n",
       "<text text-anchor=\"middle\" x=\"717\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node31\" class=\"node\"><title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"743\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">30 (his)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge29\" class=\"edge\"><title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M737.417,-347.799C738.375,-336.163 739.661,-320.548 740.757,-307.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"744.253,-307.429 741.586,-297.175 737.277,-306.854 744.253,-307.429\"/>\n",
       "<text text-anchor=\"middle\" x=\"769\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod:poss</text>\n",
       "</g>\n",
       "<!-- 35 -->\n",
       "<g id=\"node34\" class=\"node\"><title>35</title>\n",
       "<text text-anchor=\"middle\" x=\"782\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">35 (to)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;35 -->\n",
       "<g id=\"edge33\" class=\"edge\"><title>36&#45;&gt;35</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M782,-521.799C782,-510.163 782,-494.548 782,-481.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"785.5,-481.175 782,-471.175 778.5,-481.175 785.5,-481.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"796.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 38 -->\n",
       "<g id=\"node35\" class=\"node\"><title>38</title>\n",
       "<text text-anchor=\"middle\" x=\"878\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">38 (account)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;38 -->\n",
       "<g id=\"edge34\" class=\"edge\"><title>36&#45;&gt;38</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M801.427,-521.799C815.61,-509.241 835.027,-492.049 850.758,-478.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"853.435,-480.425 858.602,-471.175 848.795,-475.184 853.435,-480.425\"/>\n",
       "<text text-anchor=\"middle\" x=\"847.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">dobj</text>\n",
       "</g>\n",
       "<!-- 37 -->\n",
       "<g id=\"node36\" class=\"node\"><title>37</title>\n",
       "<text text-anchor=\"middle\" x=\"825\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">37 (the)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;37 -->\n",
       "<g id=\"edge35\" class=\"edge\"><title>38&#45;&gt;37</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M867.275,-434.799C859.732,-422.702 849.508,-406.305 841.004,-392.667\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"843.97,-390.809 835.709,-384.175 838.03,-394.513 843.97,-390.809\"/>\n",
       "<text text-anchor=\"middle\" x=\"862.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 40 -->\n",
       "<g id=\"node37\" class=\"node\"><title>40</title>\n",
       "<text text-anchor=\"middle\" x=\"908\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">40 (hates)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;40 -->\n",
       "<g id=\"edge36\" class=\"edge\"><title>38&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M884.071,-434.799C888.219,-423.047 893.798,-407.238 898.526,-393.842\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"901.91,-394.77 901.938,-384.175 895.309,-392.44 901.91,-394.77\"/>\n",
       "<text text-anchor=\"middle\" x=\"906\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">conj</text>\n",
       "</g>\n",
       "<!-- 54 -->\n",
       "<g id=\"node38\" class=\"node\"><title>54</title>\n",
       "<text text-anchor=\"middle\" x=\"991\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">54 (etc)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;54 -->\n",
       "<g id=\"edge37\" class=\"edge\"><title>38&#45;&gt;54</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M900.867,-434.799C917.868,-422.01 941.258,-404.417 959.949,-390.357\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"962.279,-392.984 968.167,-384.175 958.071,-387.39 962.279,-392.984\"/>\n",
       "<text text-anchor=\"middle\" x=\"953\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">conj</text>\n",
       "</g>\n",
       "<!-- 43 -->\n",
       "<g id=\"node39\" class=\"node\"><title>43</title>\n",
       "<text text-anchor=\"middle\" x=\"827\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">43 (has)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;43 -->\n",
       "<g id=\"edge38\" class=\"edge\"><title>40&#45;&gt;43</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M891.608,-347.799C879.751,-335.356 863.559,-318.364 850.351,-304.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"852.8,-302 843.367,-297.175 847.732,-306.829 852.8,-302\"/>\n",
       "<text text-anchor=\"middle\" x=\"890.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">ccomp</text>\n",
       "</g>\n",
       "<!-- 50 -->\n",
       "<g id=\"node40\" class=\"node\"><title>50</title>\n",
       "<text text-anchor=\"middle\" x=\"970\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">50 (reaching)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;50 -->\n",
       "<g id=\"edge39\" class=\"edge\"><title>40&#45;&gt;50</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M920.547,-347.799C929.455,-335.587 941.559,-318.992 951.562,-305.278\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"954.407,-307.317 957.472,-297.175 948.751,-303.192 954.407,-307.317\"/>\n",
       "<text text-anchor=\"middle\" x=\"957\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">advcl</text>\n",
       "</g>\n",
       "<!-- 41 -->\n",
       "<g id=\"node41\" class=\"node\"><title>41</title>\n",
       "<text text-anchor=\"middle\" x=\"715\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">41 (that)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;41 -->\n",
       "<g id=\"edge40\" class=\"edge\"><title>43&#45;&gt;41</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M796.722,-260.854C787.882,-255.466 778.356,-249.277 770,-243 759.521,-235.128 748.7,-225.633 739.458,-217.041\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"741.778,-214.419 732.107,-210.092 736.969,-219.505 741.778,-214.419\"/>\n",
       "<text text-anchor=\"middle\" x=\"784.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 42 -->\n",
       "<g id=\"node42\" class=\"node\"><title>42</title>\n",
       "<text text-anchor=\"middle\" x=\"793\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">42 (he)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;42 -->\n",
       "<g id=\"edge41\" class=\"edge\"><title>43&#45;&gt;42</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M817.449,-260.821C814.535,-255.23 811.456,-248.935 809,-243 805.981,-235.705 803.173,-227.622 800.779,-220.119\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"804.07,-218.91 797.79,-210.377 797.378,-220.964 804.07,-218.91\"/>\n",
       "<text text-anchor=\"middle\" x=\"824\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 45 -->\n",
       "<g id=\"node43\" class=\"node\"><title>45</title>\n",
       "<text text-anchor=\"middle\" x=\"876\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">45 (speak)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;45 -->\n",
       "<g id=\"edge42\" class=\"edge\"><title>43&#45;&gt;45</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M836.916,-260.799C843.823,-248.817 853.162,-232.617 860.979,-219.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"864.137,-220.587 866.099,-210.175 858.072,-217.091 864.137,-220.587\"/>\n",
       "<text text-anchor=\"middle\" x=\"873\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">xcomp</text>\n",
       "</g>\n",
       "<!-- 49 -->\n",
       "<g id=\"node48\" class=\"node\"><title>49</title>\n",
       "<text text-anchor=\"middle\" x=\"970\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">49 (before)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;49 -->\n",
       "<g id=\"edge47\" class=\"edge\"><title>50&#45;&gt;49</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M970,-260.799C970,-249.163 970,-233.548 970,-220.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"973.5,-220.175 970,-210.175 966.5,-220.175 973.5,-220.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"984.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 52 -->\n",
       "<g id=\"node49\" class=\"node\"><title>52</title>\n",
       "<text text-anchor=\"middle\" x=\"1063\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">52 (agent)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;52 -->\n",
       "<g id=\"edge48\" class=\"edge\"><title>50&#45;&gt;52</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M988.82,-260.799C1002.56,-248.241 1021.37,-231.049 1036.61,-217.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1039.19,-219.505 1044.21,-210.175 1034.47,-214.338 1039.19,-219.505\"/>\n",
       "<text text-anchor=\"middle\" x=\"1034.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">dobj</text>\n",
       "</g>\n",
       "<!-- 44 -->\n",
       "<g id=\"node44\" class=\"node\"><title>44</title>\n",
       "<text text-anchor=\"middle\" x=\"850\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">44 (to)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;44 -->\n",
       "<g id=\"edge43\" class=\"edge\"><title>45&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M870.739,-173.799C867.144,-162.047 862.308,-146.238 858.211,-132.842\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"861.526,-131.714 855.254,-123.175 854.832,-133.762 861.526,-131.714\"/>\n",
       "<text text-anchor=\"middle\" x=\"878.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 48 -->\n",
       "<g id=\"node45\" class=\"node\"><title>48</title>\n",
       "<text text-anchor=\"middle\" x=\"940\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">48 (machine)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;48 -->\n",
       "<g id=\"edge44\" class=\"edge\"><title>45&#45;&gt;48</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M888.951,-173.799C898.147,-161.587 910.641,-144.992 920.967,-131.278\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"923.849,-133.269 927.068,-123.175 918.257,-129.059 923.849,-133.269\"/>\n",
       "<text text-anchor=\"middle\" x=\"927\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 46 -->\n",
       "<g id=\"node46\" class=\"node\"><title>46</title>\n",
       "<text text-anchor=\"middle\" x=\"900\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">46 (with)</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;46 -->\n",
       "<g id=\"edge45\" class=\"edge\"><title>48&#45;&gt;46</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M931.905,-86.799C926.321,-74.9322 918.79,-58.9279 912.446,-45.4488\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"915.507,-43.7333 908.083,-36.1754 909.174,-46.7139 915.507,-43.7333\"/>\n",
       "<text text-anchor=\"middle\" x=\"934\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 47 -->\n",
       "<g id=\"node47\" class=\"node\"><title>47</title>\n",
       "<text text-anchor=\"middle\" x=\"979\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">47 (a)</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;47 -->\n",
       "<g id=\"edge46\" class=\"edge\"><title>48&#45;&gt;47</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M947.892,-86.799C953.337,-74.9322 960.68,-58.9279 966.865,-45.4488\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"970.13,-46.724 971.12,-36.1754 963.768,-43.8048 970.13,-46.724\"/>\n",
       "<text text-anchor=\"middle\" x=\"969.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 51 -->\n",
       "<g id=\"node50\" class=\"node\"><title>51</title>\n",
       "<text text-anchor=\"middle\" x=\"1063\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">51 (an)</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;51 -->\n",
       "<g id=\"edge49\" class=\"edge\"><title>52&#45;&gt;51</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1063,-173.799C1063,-162.163 1063,-146.548 1063,-133.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1066.5,-133.175 1063,-123.175 1059.5,-133.175 1066.5,-133.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"1071.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7feff71ea6d8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(topPostDepParse[targetSentence])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, parse a (modest) subset of your corpus of interest. How deep are the phrase structure and dependency parse trees nested? How does parse depth relate to perceived sentence complexity? What are five things you can extract from these parses for subsequent analysis? (e.g., nouns collocated in a noun phrase; adjectives that modify a noun; etc.) Capture these sets of things for a focal set of words (e.g., \"Bush\", \"Obama\", \"Trump\"). What do they reveal about the roles that these entities are perceived to play in the social world inscribed by your texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Contents</th>\n",
       "      <th>Copyright Year</th>\n",
       "      <th>Journal Title</th>\n",
       "      <th>Titles</th>\n",
       "      <th>sentences</th>\n",
       "      <th>POS_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The study of animal communication is a complex...</td>\n",
       "      <td>2011</td>\n",
       "      <td>plos one</td>\n",
       "      <td>UV-Deprived Coloration Reduces Success in Mate...</td>\n",
       "      <td>[[The, study, of, animal, communication, is, a...</td>\n",
       "      <td>[[(The, DT), (study, NN), (of, IN), (animal, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aneurysms in general represent a Damocles swor...</td>\n",
       "      <td>2017</td>\n",
       "      <td>plos one</td>\n",
       "      <td>Metabolomic profiling of ascending thoracic ao...</td>\n",
       "      <td>[[Aneurysms, in, general, represent, a, Damocl...</td>\n",
       "      <td>[[(Aneurysms, NNS), (in, IN), (general, JJ), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prognostic information about life expectancy i...</td>\n",
       "      <td>2013</td>\n",
       "      <td>plos one</td>\n",
       "      <td>Predictive Value of a Profile of Routine Blood...</td>\n",
       "      <td>[[Prognostic, information, about, life, expect...</td>\n",
       "      <td>[[(Prognostic, JJ), (information, NN), (about,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Interleukin (IL)-23 has been associated with t...</td>\n",
       "      <td>2017</td>\n",
       "      <td>plos one</td>\n",
       "      <td>Continuous IL-23 stimulation drives ILC3 deple...</td>\n",
       "      <td>[[Interleukin, (, IL, ), -23, has, been, assoc...</td>\n",
       "      <td>[[(Interleukin, NN), ((, CD), (IL, NN), (), NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Labor represents a stress test for the fetus. ...</td>\n",
       "      <td>2014</td>\n",
       "      <td>plos one</td>\n",
       "      <td>Assessment of Coupling between Trans-Abdominal...</td>\n",
       "      <td>[[Labor, represents, a, stress, test, for, the...</td>\n",
       "      <td>[[(Labor, NN), (represents, VBZ), (a, DT), (st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Article Contents Copyright Year  \\\n",
       "0  The study of animal communication is a complex...           2011   \n",
       "1  Aneurysms in general represent a Damocles swor...           2017   \n",
       "2  Prognostic information about life expectancy i...           2013   \n",
       "3  Interleukin (IL)-23 has been associated with t...           2017   \n",
       "4  Labor represents a stress test for the fetus. ...           2014   \n",
       "\n",
       "  Journal Title                                             Titles  \\\n",
       "0      plos one  UV-Deprived Coloration Reduces Success in Mate...   \n",
       "1      plos one  Metabolomic profiling of ascending thoracic ao...   \n",
       "2      plos one  Predictive Value of a Profile of Routine Blood...   \n",
       "3      plos one  Continuous IL-23 stimulation drives ILC3 deple...   \n",
       "4      plos one  Assessment of Coupling between Trans-Abdominal...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [[The, study, of, animal, communication, is, a...   \n",
       "1  [[Aneurysms, in, general, represent, a, Damocl...   \n",
       "2  [[Prognostic, information, about, life, expect...   \n",
       "3  [[Interleukin, (, IL, ), -23, has, been, assoc...   \n",
       "4  [[Labor, represents, a, stress, test, for, the...   \n",
       "\n",
       "                                           POS_sents  \n",
       "0  [[(The, DT), (study, NN), (of, IN), (animal, N...  \n",
       "1  [[(Aneurysms, NNS), (in, IN), (general, JJ), (...  \n",
       "2  [[(Prognostic, JJ), (information, NN), (about,...  \n",
       "3  [[(Interleukin, NN), ((, CD), (IL, NN), (), NN...  \n",
       "4  [[(Labor, NN), (represents, VBZ), (a, DT), (st...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "plos_DepParse = list(stanford.depParser.parse_sents(plos_df['sentences'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The study of animal communication is a complex science addressing a wide range of multi-layered questions, such as how a signal is emitted (e.g., visually, acoustically etc), how it is perceived (e.g. the spectral range and sensitivity of colour vision), and of course the adaptive reasons for signaling (e.g., deterring rivals, attracting mates). Any and all of these factors interact to mold signal selection in the wild, and mediate the type and degree of honest indication of some aspect of sender ‘quality’, which is expected in any evolutionarily stable signal trait [1]–[4].Early work suggested that ‘badges of status’ would be beneficial to both signalers and receivers, since they would cut costs of contests to both contestants if an outcome would be predictable [5]. Subsequent work has debated whether merely ‘social costs’, in the absence of developmental costs, really are sufficient to guarantee honest and evolutionarily stable signaling, conveying aspects of ‘quality’, such as fighting or parental ability [6], [7]. Nevertheless, there is no lack of examples of seemingly ‘cheap’ yet adaptive badges in the recent literature, such as wing epaulettes in birds [8] and badges of bright colours in lizards [9].The bright green colour ‘badge’ of our model species, the Swedish sand lizard (Lacerta agilis), shows spectral reflectance peaks in both green (ca 540 nm) and the UV (ca 340 nm). However, when we first investigated these traits in the mid 90's, neither the UV-component nor other aspects of colour as such (spectral shape) were identified; in particular, we dismissed the UV effect based on data from males caught at spring emergence from hibernation (with the rationale that signals present at this time are the most likely to reflect early male resource holding power when core areas of male home ranges are contested [9]). However, this ignores that the exuvia may not show the same spectroradiometry characteristics as a newly shed animal at maximal brightness. Instead, our signaling work on sand lizards has since mostly focused on the link between badge size (proportion green colour on a male's body sides), signaling, and fitness parameters in field and laboratory studies [9]–[12]. This work showed, for example, that badges contribute to mate acquisition; in smaller than average males experimental increment of badge size increased mate acquisition by 400 percent [12].Given the renewed interest in UV signaling in both vertebrates and invertebrates (e.g., [13]–[21]), we revisited this research area in 2007. A pilot study confirmed that recently shed males indeed showed a much stronger reflectance peak in the UV spectrum (ca 340 nm, Fig. 1). This agrees with reflectance data from male Lacerta agilis in Pyrenees populations during the mating season [19]. We therefore designed an experiment in 2008 to test the proposition that UV blockage would interfere with rival and partner communication and compromise mate acquisition.Note the considerable UV-reduction also after 30 days in the wild. Spectra are set to equal brightness, in order to see spectral shape (i.e., colour) more precisely.The field work in this population (Asketunnan, Sweden ∼N57°22′ E11°58′) follows a well-established protocol that has been reported on in previous work (e.g., [9]–[12], [22]). In short, sand lizards (Lacerta agilis) are small (to 20 g), ground-dwelling lizards. Eighty five males and eighty females are individually marked short term by putting a uniquely numbered cloth tape on their backs. Males observed courting, copulating or mate guarding females were classified as partners. The sex ratio in the Asketunnan population is approximately 1∶1, the capture rate of adults is >90% and, thus, the observations made of the lizards in the current paper are based on nearly complete coverage of the adult population. That said, scored mating success of adult males is known to covary with the number of times males are observed, which was therefore controlled for in our analyses. All adults were weighed (to the nearest 0.1 g), measured (snout-vent and total length to the nearest 1 mm), and a 50 µl blood sample was taken from vena angularis (in the corner of the mouth) of both males and females and stored in 70% alcohol for later molecular genetic analysis. Males and females were then released at the place of capture and monitored during the mating season (ca seven weeks) every day that weather permitted.Females were immediately released at the place of captured. Males were accumulated at daily field captures over a ten day period and stored at +8°C in a constant temperature room, awaiting a synchronized release of all males immediately after being weighed, measured, marked and treated with UV blocker (released 2 May 2008). Representative radiospectrometric analysis of UV blockage effects were performed at release and after three weeks (Fig. 1) to verify that our UV blockage had the desired long-term effects. The second measure after three weeks was virtually indistinguishable from the first (Fig. 1). The UV blockage was performed by gently rubbing +50 SPF (‘sun protection factor’; Vichy Laboratoires, Capital Soleil, Very High Protection] on every second male (n\\u200a=\\u200a43, for controls, n\\u200a=\\u200a42) in an Excel size-sorted data set of the captured males in storage. This ensured that UV-blocked and control males did not differ in snout-vent length, mass or body condition (p>0.14 for all three of these traits). Thereafter every male was sprayed with a vapour-permeable spray dressing, used to treat superficial human wounds (Smith & Nephew, Hull, England). Neither the storage at cool temperatures nor the spray dressing have any detrimental effects [12], and the UV block was developed for humans and appeared biologically inert on lizard skin (no apparent fading or discoloring was observed).After the morphology data had been collected and the lizards treated, they were released at random places of capture (i.e., randomizing sites that were, at capture, potentially further or closer from females) and monitored for associations with partners (facilitated by the prolonged mate guarding, [22]) every day of the mating season when the weather permitted lizard activity (3 May–20 June, number of observation days, n\\u200a=\\u200a26). Thus, our procedure also eliminates variation in male spring emergence (since all males are released simultaneously). Our work was approved by the Animal Ethics Committee, University of Gothenburg.Our statistical analyses involved two approaches: (a) we first performed a homogeneity of slopes regression analysis with number of partners as response variable and treatment (UV-blocked vs. controls), number of observations of a male and its interaction with treatment, and male snout-vent length as covariate. However, because of some non-normality of the data (over-representation of zero pairing success), we (b) also performed an analysis more robust to deviation from normality using a logistic regression with an ordered cumulative logit model with the same trait variables.There was no difference in the mean number of observations of UV-reduced and control males (mean number of re-observations, 2.1±0.24, range 1 to 8, and 1.93±0.24, range 1 to 9, for control and UV-reduced males, respectively; T-test, t\\u200a=\\u200a0.61, P\\u200a=\\u200a0.54). Across treatment and control males, the number of observations of a male after release was correlated with the number of times he was seen courting a female (rs\\u200a=\\u200a0.49, P<0.0001, N\\u200a=\\u200a85). We therefore incorporated male number of re-sightings in our analysis of treatment effects on number of females paired. UV-blocked males had an average of 0.12 female pairing observations per male (±0.049, SE, N\\u200a=\\u200a43), whereas the corresponding number for control males was three times as high (0.31±0.12, N\\u200a=\\u200a42). The regression analysis was globally significant (F3, 81\\u200a=\\u200a40.7, P<0.0001, R2\\u200a=\\u200a0.60), and had significant independent effects of treatment (F\\u200a=\\u200a15.4, P<0.0002, d.f.\\u200a=\\u200a1), number of observations (F\\u200a=\\u200a82.9, P<0.0001, d.f.\\u200a=\\u200a1), and their interaction (F\\u200a=\\u200a39.7, P<0.0001; Fig. 2). Body size (SVL) was backwards eliminated from the final model (P>0.25). Our cumulative, ordered logistic regression largely agreed with these results (Global model Likelihood ratio X2\\u200a=\\u200a35.0, P<0.0001, d.f.\\u200a=\\u200a4). The number of observations of a male significantly affected the number of females he was observed with (Wald X2\\u200a=\\u200a15.06, P\\u200a=\\u200a0.0001), the treatment x observation interaction remained significant (Wald X2\\u200a=\\u200a6.03, P\\u200a=\\u200a0.014), while the treatment effect per se fell just short of significant (Wald X2\\u200a=\\u200a3.07, P\\u200a=\\u200a0.079; Fig. 2).Increment symbol size represents increasing number of observations of males from 1 (smallest) to 24 (largest).Our results show slight discrepancy between the logistic regression and the linear multiple regression analysis. However, we know from previous work [22] that the number of observations per male influences estimates of mate acquisition (number of females seen courting), and that this effect is modified by UV reduced signaling. Thus, it can be argued that the significant interaction term in both analyses is the correct unit of analysis, and it is significant in both cases. How robust are these results? The current study is specifically aimed at analyzing mate acquisition success in relation to UV blockage. Thus, analyzing access to females is a more appropriate level of analysis than tallying molecularly assigned offspring, since this level would include sperm competition and cryptic female choice effects [9], [23]. Regardless, male access to females is tightly correlated with probability of paternity for a given female [24] and hence our analysis should represent fitness consequences of UV-signaling, independent of badge signaling.Our previous work [9] shows the effect of the area of nuptial coloration for successful mate acquisition and that males are likely to use both badge cues and other coloration for assessing rival fighting ability and to avoid repeating contests with other males [9]–[11]. How UV signaling adds additional information, or makes already described traits such as size and fighting ability more (or less) easily or accurately perceived cannot be deduced from the current experiment. However, since UV blocking has an effect on mate acquisition across the male size distribution when badge and other colour traits are unmanipulated, this seems to suggest that UV signaling is universally employed in all males and perhaps more important for conveying mere presence than fighting ability [9].Our results and interpretations also agree with those of two previous studies on the role of UV in lizard communication. Stapley and Whiting [25] showed with a field experiment that males with reduced UV signals in Platysaurus broadleyi were more likely to be challenged by rivals, and Bajer et al. [26] showed that male green lizards (Lacerta viridis) with reduced UV signals were less spatially associated with by females. In sand lizards, we have never been able to demonstrate that there is female choice on male colour traits whereas there are strong effects of male green badges on male contest behavoiurs [9], [10]. Thus, we conclude that male UV reduction in this species compromises mate acquisition but that it is unresolved in free-ranging animals whether this is a combined effect of male-male rivalry and female choice on UV components of signalling.In summary, our field experiment demonstrates a technique for long-term elimination of UV signaling in free-ranging lizards, which reduces success in mate acquisition, probably through reduced deterrence of rivals.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plos_df['Article Contents'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plos_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-179bd97d9754>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#This prints the last sentence in the given article. The second article is weird because some of the tag seems to have slipped in.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtargetArticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlast_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplos_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargetArticle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplos_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargetArticle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlast_sent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plos_df' is not defined"
     ]
    }
   ],
   "source": [
    "#This prints the last sentence in the given article. The second article is weird because some of the tag seems to have slipped in. \n",
    "targetArticle = 1\n",
    "last_sent = ((len(plos_df['sentences'].iloc[targetArticle]))-1)\n",
    "print(last_sent)\n",
    "print((plos_df['sentences'].iloc[targetArticle])[last_sent])\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plos_df['sentences'].iloc[targetArticle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"1813pt\" height=\"914pt\"\n",
       " viewBox=\"0.00 0.00 1812.50 914.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 910)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-910 1808.5,-910 1808.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"505.5\" y=\"-884.3\" font-family=\"Times,serif\" font-size=\"14.00\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 45 -->\n",
       "<g id=\"node2\" class=\"node\"><title>45</title>\n",
       "<text text-anchor=\"middle\" x=\"505.5\" y=\"-797.3\" font-family=\"Times,serif\" font-size=\"14.00\">45 (demonstrates)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;45 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;45</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M505.5,-869.799C505.5,-858.163 505.5,-842.548 505.5,-829.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"509,-829.175 505.5,-819.175 502,-829.175 509,-829.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"516.5\" y=\"-840.8\" font-family=\"Times,serif\" font-size=\"14.00\">root</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\"><title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"290.5\" y=\"-710.3\" font-family=\"Times,serif\" font-size=\"14.00\">1 (Thus)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;1 -->\n",
       "<g id=\"edge41\" class=\"edge\"><title>45&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M447.972,-786.448C427.019,-780.781 403.364,-773.516 382.5,-765 363.613,-757.291 343.606,-746.733 327.09,-737.305\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"328.71,-734.198 318.303,-732.208 325.198,-740.253 328.71,-734.198\"/>\n",
       "<text text-anchor=\"middle\" x=\"405\" y=\"-753.8\" font-family=\"Times,serif\" font-size=\"14.00\">advmod</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"382.5\" y=\"-710.3\" font-family=\"Times,serif\" font-size=\"14.00\">4 (conclude)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;4 -->\n",
       "<g id=\"edge42\" class=\"edge\"><title>45&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M479.224,-782.921C470.659,-777.281 461.134,-770.932 452.5,-765 439.92,-756.358 426.229,-746.647 414.318,-738.099\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"416.162,-735.114 406.001,-732.112 412.072,-740.795 416.162,-735.114\"/>\n",
       "<text text-anchor=\"middle\" x=\"477\" y=\"-753.8\" font-family=\"Times,serif\" font-size=\"14.00\">parataxis</text>\n",
       "</g>\n",
       "<!-- 44 -->\n",
       "<g id=\"node44\" class=\"node\"><title>44</title>\n",
       "<text text-anchor=\"middle\" x=\"505.5\" y=\"-710.3\" font-family=\"Times,serif\" font-size=\"14.00\">44 (experiment)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;44 -->\n",
       "<g id=\"edge43\" class=\"edge\"><title>45&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M505.5,-782.799C505.5,-771.163 505.5,-755.548 505.5,-742.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"509,-742.175 505.5,-732.175 502,-742.175 509,-742.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"520.5\" y=\"-753.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 47 -->\n",
       "<g id=\"node45\" class=\"node\"><title>47</title>\n",
       "<text text-anchor=\"middle\" x=\"643.5\" y=\"-710.3\" font-family=\"Times,serif\" font-size=\"14.00\">47 (technique)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;47 -->\n",
       "<g id=\"edge44\" class=\"edge\"><title>45&#45;&gt;47</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M533.426,-782.799C554.563,-769.78 583.784,-751.781 606.808,-737.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"608.936,-740.4 615.615,-732.175 605.265,-734.44 608.936,-740.4\"/>\n",
       "<text text-anchor=\"middle\" x=\"596\" y=\"-753.8\" font-family=\"Times,serif\" font-size=\"14.00\">dobj</text>\n",
       "</g>\n",
       "<!-- 68 -->\n",
       "<g id=\"node46\" class=\"node\"><title>68</title>\n",
       "<text text-anchor=\"middle\" x=\"963.5\" y=\"-710.3\" font-family=\"Times,serif\" font-size=\"14.00\">68 (deterrence)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;68 -->\n",
       "<g id=\"edge45\" class=\"edge\"><title>45&#45;&gt;68</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M563.042,-789.321C650.372,-773.113 815.542,-742.459 904.012,-726.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"904.697,-729.473 913.89,-724.207 903.419,-722.591 904.697,-729.473\"/>\n",
       "<text text-anchor=\"middle\" x=\"778.5\" y=\"-753.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"269.5\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">3 (we)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;3 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>4&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M359.633,-695.799C342.632,-683.01 319.242,-665.417 300.551,-651.357\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"302.429,-648.39 292.333,-645.175 298.221,-653.984 302.429,-648.39\"/>\n",
       "<text text-anchor=\"middle\" x=\"348.5\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node6\" class=\"node\"><title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"372.5\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">12 (compromises)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;12 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>4&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M380.476,-695.799C379.107,-684.163 377.27,-668.548 375.704,-655.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"379.165,-654.698 374.521,-645.175 372.213,-655.516 379.165,-654.698\"/>\n",
       "<text text-anchor=\"middle\" x=\"397\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">ccomp</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\"><title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">5 (that)</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;5 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>12&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M314.218,-611.194C278.777,-602.177 239.555,-592.166 235.5,-591 192.191,-578.542 178.943,-575.731 133.226,-558.084\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"134.22,-554.715 123.631,-554.35 131.681,-561.239 134.22,-554.715\"/>\n",
       "<text text-anchor=\"middle\" x=\"250\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\"><title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"185.5\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">8 (reduction)</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;8 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>12&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M334.658,-608.799C305.159,-595.39 264.039,-576.7 232.443,-562.338\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"233.518,-558.982 222.966,-558.03 230.622,-565.355 233.518,-558.982\"/>\n",
       "<text text-anchor=\"middle\" x=\"305.5\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node14\" class=\"node\"><title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"298.5\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">14 (acquisition)</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M357.525,-608.799C346.793,-596.471 332.173,-579.679 320.169,-565.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"322.659,-563.42 313.453,-558.175 317.379,-568.016 322.659,-563.42\"/>\n",
       "<text text-anchor=\"middle\" x=\"353\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">dobj</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node15\" class=\"node\"><title>15</title>\n",
       "<text text-anchor=\"middle\" x=\"397.5\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">15 (but)</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;15 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>12&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M377.559,-608.799C381.015,-597.047 385.665,-581.238 389.605,-567.842\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"392.984,-568.757 392.448,-558.175 386.269,-566.781 392.984,-568.757\"/>\n",
       "<text text-anchor=\"middle\" x=\"393\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">cc</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node16\" class=\"node\"><title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"510.5\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">19 (unresolved)</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;19 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>12&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M400.426,-608.799C421.563,-595.78 450.784,-577.781 473.808,-563.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"475.936,-566.4 482.615,-558.175 472.265,-560.44 475.936,-566.4\"/>\n",
       "<text text-anchor=\"middle\" x=\"462.5\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">conj</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\"><title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"31.5\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">6 (male)</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\"><title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">7 (UV)</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;6 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>8&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.891,-524.833C117.036,-516.591 89.7966,-507.211 84.5,-504 73.1753,-497.135 62.2496,-487.524 53.2905,-478.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"55.718,-476.035 46.2617,-471.257 50.6745,-480.889 55.718,-476.035\"/>\n",
       "<text text-anchor=\"middle\" x=\"100\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;7 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>8&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M149.529,-521.886C141.825,-516.951 134.277,-510.976 128.5,-504 123.003,-497.362 118.994,-488.965 116.118,-480.957\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"119.385,-479.678 113.052,-471.183 112.705,-481.773 119.385,-479.678\"/>\n",
       "<text text-anchor=\"middle\" x=\"157.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node11\" class=\"node\"><title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"196.5\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">11 (species)</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;11 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>8&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M187.726,-521.799C189.232,-510.163 191.253,-494.548 192.975,-481.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.465,-481.542 194.277,-471.175 189.523,-480.644 196.465,-481.542\"/>\n",
       "<text text-anchor=\"middle\" x=\"208.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node12\" class=\"node\"><title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"169.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">9 (in)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;9 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>11&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M191.036,-434.799C187.303,-423.047 182.282,-407.238 178.026,-393.842\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181.319,-392.647 174.956,-384.175 174.647,-394.766 181.319,-392.647\"/>\n",
       "<text text-anchor=\"middle\" x=\"197.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node13\" class=\"node\"><title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"245.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">10 (this)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M206.416,-434.799C213.323,-422.817 222.662,-406.617 230.479,-393.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"233.637,-394.587 235.599,-384.175 227.572,-391.091 233.637,-394.587\"/>\n",
       "<text text-anchor=\"middle\" x=\"233\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node17\" class=\"node\"><title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"290.5\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">13 (mate)</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;13 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>14&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M296.881,-521.799C295.786,-510.163 294.316,-494.548 293.063,-481.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"296.538,-480.803 292.117,-471.175 289.569,-481.459 296.538,-480.803\"/>\n",
       "<text text-anchor=\"middle\" x=\"324.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node18\" class=\"node\"><title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"375.5\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">16 (that)</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;16 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>19&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M473.66,-521.842C463.064,-516.5 451.636,-510.339 441.5,-504 428.75,-496.026 415.4,-486.22 404.068,-477.411\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"406.231,-474.659 396.212,-471.213 401.895,-480.154 406.231,-474.659\"/>\n",
       "<text text-anchor=\"middle\" x=\"456\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node19\" class=\"node\"><title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">17 (it)</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;17 -->\n",
       "<g id=\"edge16\" class=\"edge\"><title>19&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M496.684,-521.814C492.341,-516.223 487.609,-509.929 483.5,-504 478.231,-496.398 472.811,-487.917 468.005,-480.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"470.882,-478.125 462.692,-471.403 464.903,-481.765 470.882,-478.125\"/>\n",
       "<text text-anchor=\"middle\" x=\"498.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node20\" class=\"node\"><title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"524.5\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">18 (is)</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;18 -->\n",
       "<g id=\"edge17\" class=\"edge\"><title>19&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M513.333,-521.799C515.25,-510.163 517.822,-494.548 520.014,-481.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"523.499,-481.611 521.671,-471.175 516.592,-480.474 523.499,-481.611\"/>\n",
       "<text text-anchor=\"middle\" x=\"528.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">cop</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node21\" class=\"node\"><title>22</title>\n",
       "<text text-anchor=\"middle\" x=\"612.5\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">22 (animals)</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;22 -->\n",
       "<g id=\"edge18\" class=\"edge\"><title>19&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M531.141,-521.799C546.349,-509.126 567.22,-491.734 584.015,-477.738\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"586.448,-480.266 591.889,-471.175 581.967,-474.889 586.448,-480.266\"/>\n",
       "<text text-anchor=\"middle\" x=\"584.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node22\" class=\"node\"><title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"814.5\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">28 (effect)</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;28 -->\n",
       "<g id=\"edge19\" class=\"edge\"><title>19&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M561.726,-524.677C619.233,-508.598 711.766,-482.725 767.49,-467.144\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"768.762,-470.423 777.45,-464.359 766.877,-463.682 768.762,-470.423\"/>\n",
       "<text text-anchor=\"middle\" x=\"700\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">ccomp</text>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node23\" class=\"node\"><title>20</title>\n",
       "<text text-anchor=\"middle\" x=\"379.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">20 (in)</text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;20 -->\n",
       "<g id=\"edge20\" class=\"edge\"><title>22&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M569.418,-437.747C566.405,-436.802 563.41,-435.878 560.5,-435 531.802,-426.34 523.852,-426.733 495.5,-417 461.822,-405.438 452.086,-400.328 415.82,-384.086\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"417.119,-380.833 406.56,-379.956 414.267,-387.226 417.119,-380.833\"/>\n",
       "<text text-anchor=\"middle\" x=\"507.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node24\" class=\"node\"><title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"479.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">21 (free&#45;ranging)</text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;21 -->\n",
       "<g id=\"edge21\" class=\"edge\"><title>22&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M574.656,-434.864C564.007,-429.568 552.584,-423.425 542.5,-417 530.206,-409.168 517.463,-399.409 506.681,-390.597\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"508.785,-387.795 498.858,-384.091 504.309,-393.177 508.785,-387.795\"/>\n",
       "<text text-anchor=\"middle\" x=\"558\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node25\" class=\"node\"><title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"595.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">23 (whether)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;23 -->\n",
       "<g id=\"edge22\" class=\"edge\"><title>28&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M777.5,-442.292C754.713,-435.932 725.068,-426.97 699.5,-417 678.423,-408.782 655.705,-398.085 636.894,-388.695\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"638.226,-385.447 627.722,-384.067 635.073,-391.696 638.226,-385.447\"/>\n",
       "<text text-anchor=\"middle\" x=\"714\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node26\" class=\"node\"><title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"687.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">24 (this)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;24 -->\n",
       "<g id=\"edge23\" class=\"edge\"><title>28&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M782.122,-434.862C772.424,-429.427 761.879,-423.209 752.5,-417 740.297,-408.921 727.413,-399.268 716.363,-390.608\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"718.322,-387.694 708.313,-384.221 713.972,-393.178 718.322,-387.694\"/>\n",
       "<text text-anchor=\"middle\" x=\"767.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node27\" class=\"node\"><title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"763.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">25 (is)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;25 -->\n",
       "<g id=\"edge24\" class=\"edge\"><title>28&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M804.179,-434.799C796.99,-422.817 787.27,-406.617 779.134,-393.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"781.951,-390.95 773.805,-384.175 775.949,-394.551 781.951,-390.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"802.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">cop</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node28\" class=\"node\"><title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"835.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">26 (a)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;26 -->\n",
       "<g id=\"edge25\" class=\"edge\"><title>28&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M818.75,-434.799C821.625,-423.163 825.482,-407.548 828.771,-394.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"832.256,-394.723 831.257,-384.175 825.46,-393.044 832.256,-394.723\"/>\n",
       "<text text-anchor=\"middle\" x=\"835\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node29\" class=\"node\"><title>27</title>\n",
       "<text text-anchor=\"middle\" x=\"929.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">27 (combined)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;27 -->\n",
       "<g id=\"edge26\" class=\"edge\"><title>28&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M837.772,-434.799C855.152,-421.953 879.092,-404.258 898.156,-390.167\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"900.301,-392.934 906.263,-384.175 896.14,-387.305 900.301,-392.934\"/>\n",
       "<text text-anchor=\"middle\" x=\"895\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node30\" class=\"node\"><title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"1035.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">31 (rivalry)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;31 -->\n",
       "<g id=\"edge27\" class=\"edge\"><title>28&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M851.719,-439.519C870.614,-433.046 893.876,-424.864 914.5,-417 938.366,-407.9 964.594,-397.156 986.512,-387.963\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"988.079,-391.101 995.937,-383.994 985.363,-384.649 988.079,-391.101\"/>\n",
       "<text text-anchor=\"middle\" x=\"967.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 40 -->\n",
       "<g id=\"node31\" class=\"node\"><title>40</title>\n",
       "<text text-anchor=\"middle\" x=\"1247.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">40 (summary)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;40 -->\n",
       "<g id=\"edge28\" class=\"edge\"><title>28&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M851.792,-444.679C927.444,-429.829 1099.41,-396.07 1189.7,-378.347\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1190.62,-381.732 1199.76,-376.371 1189.27,-374.863 1190.62,-381.732\"/>\n",
       "<text text-anchor=\"middle\" x=\"1073.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node32\" class=\"node\"><title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"751.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">29 (of)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge29\" class=\"edge\"><title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M995.804,-350.689C992.672,-349.725 989.544,-348.815 986.5,-348 945.41,-337 933.747,-340.394 892.5,-330 847.938,-318.771 834.64,-315.242 788.248,-297.086\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"789.435,-293.792 778.848,-293.387 786.872,-300.306 789.435,-293.792\"/>\n",
       "<text text-anchor=\"middle\" x=\"904.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node33\" class=\"node\"><title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"847.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">30 (male&#45;male)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge30\" class=\"edge\"><title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M995.998,-349.05C981.887,-343.218 965.916,-336.463 951.5,-330 931.483,-321.026 909.562,-310.581 891.029,-301.565\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"892.472,-298.375 881.951,-297.132 889.401,-304.665 892.472,-298.375\"/>\n",
       "<text text-anchor=\"middle\" x=\"967\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 32 -->\n",
       "<g id=\"node34\" class=\"node\"><title>32</title>\n",
       "<text text-anchor=\"middle\" x=\"947.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">32 (and)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;32 -->\n",
       "<g id=\"edge31\" class=\"edge\"><title>31&#45;&gt;32</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1017.69,-347.799C1004.81,-335.356 987.218,-318.364 972.869,-304.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"974.906,-301.605 965.282,-297.175 970.043,-306.64 974.906,-301.605\"/>\n",
       "<text text-anchor=\"middle\" x=\"1004\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">cc</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node35\" class=\"node\"><title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"1035.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">34 (choice)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;34 -->\n",
       "<g id=\"edge32\" class=\"edge\"><title>31&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1035.5,-347.799C1035.5,-336.163 1035.5,-320.548 1035.5,-307.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1039,-307.175 1035.5,-297.175 1032,-307.175 1039,-307.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"1047.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">conj</text>\n",
       "</g>\n",
       "<!-- 37 -->\n",
       "<g id=\"node36\" class=\"node\"><title>37</title>\n",
       "<text text-anchor=\"middle\" x=\"1147.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">37 (components)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;37 -->\n",
       "<g id=\"edge33\" class=\"edge\"><title>31&#45;&gt;37</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1058.16,-347.799C1075.02,-335.01 1098.2,-317.417 1116.72,-303.357\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1119.02,-306.009 1124.87,-297.175 1114.79,-300.433 1119.02,-306.009\"/>\n",
       "<text text-anchor=\"middle\" x=\"1114.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 38 -->\n",
       "<g id=\"node40\" class=\"node\"><title>38</title>\n",
       "<text text-anchor=\"middle\" x=\"1247.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">38 (of)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;38 -->\n",
       "<g id=\"edge37\" class=\"edge\"><title>40&#45;&gt;38</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1247.5,-347.799C1247.5,-336.163 1247.5,-320.548 1247.5,-307.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1251,-307.175 1247.5,-297.175 1244,-307.175 1251,-307.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"1259.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 39 -->\n",
       "<g id=\"node41\" class=\"node\"><title>39</title>\n",
       "<text text-anchor=\"middle\" x=\"1348.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">39 (signalling.In)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;39 -->\n",
       "<g id=\"edge38\" class=\"edge\"><title>40&#45;&gt;39</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1267.94,-347.799C1283,-335.126 1303.66,-317.734 1320.29,-303.738\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1322.69,-306.292 1328.09,-297.175 1318.19,-300.937 1322.69,-306.292\"/>\n",
       "<text text-anchor=\"middle\" x=\"1332.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node37\" class=\"node\"><title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"1035.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">33 (female)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;33 -->\n",
       "<g id=\"edge34\" class=\"edge\"><title>34&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1035.5,-260.799C1035.5,-249.163 1035.5,-233.548 1035.5,-220.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1039,-220.175 1035.5,-210.175 1032,-220.175 1039,-220.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"1064.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 35 -->\n",
       "<g id=\"node38\" class=\"node\"><title>35</title>\n",
       "<text text-anchor=\"middle\" x=\"1134.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">35 (on)</text>\n",
       "</g>\n",
       "<!-- 37&#45;&gt;35 -->\n",
       "<g id=\"edge35\" class=\"edge\"><title>37&#45;&gt;35</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1144.87,-260.799C1143.09,-249.163 1140.7,-233.548 1138.67,-220.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1142.1,-219.531 1137.13,-210.175 1135.18,-220.59 1142.1,-219.531\"/>\n",
       "<text text-anchor=\"middle\" x=\"1154.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node39\" class=\"node\"><title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"1211.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">36 (UV)</text>\n",
       "</g>\n",
       "<!-- 37&#45;&gt;36 -->\n",
       "<g id=\"edge36\" class=\"edge\"><title>37&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1160.45,-260.799C1169.65,-248.587 1182.14,-231.992 1192.47,-218.278\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1195.35,-220.269 1198.57,-210.175 1189.76,-216.059 1195.35,-220.269\"/>\n",
       "<text text-anchor=\"middle\" x=\"1212.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 42 -->\n",
       "<g id=\"node42\" class=\"node\"><title>42</title>\n",
       "<text text-anchor=\"middle\" x=\"479.5\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">42 (our)</text>\n",
       "</g>\n",
       "<!-- 43 -->\n",
       "<g id=\"node43\" class=\"node\"><title>43</title>\n",
       "<text text-anchor=\"middle\" x=\"562.5\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">43 (field)</text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;42 -->\n",
       "<g id=\"edge39\" class=\"edge\"><title>44&#45;&gt;42</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M490.265,-695.983C486.333,-690.599 482.615,-684.379 480.5,-678 478.123,-670.83 477.246,-662.759 477.124,-655.227\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"480.627,-655.173 477.377,-645.088 473.629,-654.998 480.627,-655.173\"/>\n",
       "<text text-anchor=\"middle\" x=\"510.5\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod:poss</text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;43 -->\n",
       "<g id=\"edge40\" class=\"edge\"><title>44&#45;&gt;43</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M525.056,-695.988C530.511,-690.604 536.133,-684.383 540.5,-678 545.394,-670.845 549.598,-662.425 552.981,-654.556\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"556.332,-655.603 556.817,-645.019 549.838,-652.991 556.332,-655.603\"/>\n",
       "<text text-anchor=\"middle\" x=\"578.5\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 46 -->\n",
       "<g id=\"node47\" class=\"node\"><title>46</title>\n",
       "<text text-anchor=\"middle\" x=\"643.5\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">46 (a)</text>\n",
       "</g>\n",
       "<!-- 47&#45;&gt;46 -->\n",
       "<g id=\"edge46\" class=\"edge\"><title>47&#45;&gt;46</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M643.5,-695.799C643.5,-684.163 643.5,-668.548 643.5,-655.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"647,-655.175 643.5,-645.175 640,-655.175 647,-655.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"652\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 50 -->\n",
       "<g id=\"node48\" class=\"node\"><title>50</title>\n",
       "<text text-anchor=\"middle\" x=\"741.5\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">50 (elimination)</text>\n",
       "</g>\n",
       "<!-- 47&#45;&gt;50 -->\n",
       "<g id=\"edge47\" class=\"edge\"><title>47&#45;&gt;50</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M663.332,-695.799C677.943,-683.126 697.995,-665.734 714.132,-651.738\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"716.437,-654.372 721.698,-645.175 711.85,-649.084 716.437,-654.372\"/>\n",
       "<text text-anchor=\"middle\" x=\"714.5\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 65 -->\n",
       "<g id=\"node63\" class=\"node\"><title>65</title>\n",
       "<text text-anchor=\"middle\" x=\"857.5\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">65 (probably)</text>\n",
       "</g>\n",
       "<!-- 68&#45;&gt;65 -->\n",
       "<g id=\"edge62\" class=\"edge\"><title>68&#45;&gt;65</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M938.469,-695.846C930.67,-690.309 922.12,-684.039 914.5,-678 904.183,-669.824 893.229,-660.406 883.71,-651.964\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"885.882,-649.211 876.096,-645.151 881.214,-654.428 885.882,-649.211\"/>\n",
       "<text text-anchor=\"middle\" x=\"937\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">advmod</text>\n",
       "</g>\n",
       "<!-- 66 -->\n",
       "<g id=\"node64\" class=\"node\"><title>66</title>\n",
       "<text text-anchor=\"middle\" x=\"963.5\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">66 (through)</text>\n",
       "</g>\n",
       "<!-- 68&#45;&gt;66 -->\n",
       "<g id=\"edge63\" class=\"edge\"><title>68&#45;&gt;66</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M963.5,-695.799C963.5,-684.163 963.5,-668.548 963.5,-655.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"967,-655.175 963.5,-645.175 960,-655.175 967,-655.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"975.5\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 67 -->\n",
       "<g id=\"node65\" class=\"node\"><title>67</title>\n",
       "<text text-anchor=\"middle\" x=\"1067.5\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">67 (reduced)</text>\n",
       "</g>\n",
       "<!-- 68&#45;&gt;67 -->\n",
       "<g id=\"edge64\" class=\"edge\"><title>68&#45;&gt;67</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M984.546,-695.799C1000.05,-683.126 1021.33,-665.734 1038.46,-651.738\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1040.96,-654.214 1046.49,-645.175 1036.53,-648.794 1040.96,-654.214\"/>\n",
       "<text text-anchor=\"middle\" x=\"1038\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 70 -->\n",
       "<g id=\"node66\" class=\"node\"><title>70</title>\n",
       "<text text-anchor=\"middle\" x=\"1165.5\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">70 (rivals)</text>\n",
       "</g>\n",
       "<!-- 68&#45;&gt;70 -->\n",
       "<g id=\"edge65\" class=\"edge\"><title>68&#45;&gt;70</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1011.6,-695.902C1026.41,-690.419 1042.71,-684.165 1057.5,-678 1078.64,-669.186 1101.73,-658.629 1121.11,-649.492\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1122.76,-652.581 1130.3,-645.134 1119.76,-646.256 1122.76,-652.581\"/>\n",
       "<text text-anchor=\"middle\" x=\"1106.5\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 48 -->\n",
       "<g id=\"node49\" class=\"node\"><title>48</title>\n",
       "<text text-anchor=\"middle\" x=\"693.5\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">48 (for)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;48 -->\n",
       "<g id=\"edge48\" class=\"edge\"><title>50&#45;&gt;48</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M731.786,-608.799C725.02,-596.817 715.872,-580.617 708.214,-567.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"711.164,-565.162 703.199,-558.175 705.069,-568.604 711.164,-565.162\"/>\n",
       "<text text-anchor=\"middle\" x=\"732.5\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 49 -->\n",
       "<g id=\"node50\" class=\"node\"><title>49</title>\n",
       "<text text-anchor=\"middle\" x=\"789.5\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">49 (long&#45;term)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;49 -->\n",
       "<g id=\"edge49\" class=\"edge\"><title>50&#45;&gt;49</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M751.214,-608.799C757.98,-596.817 767.128,-580.617 774.786,-567.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"777.931,-568.604 779.801,-558.175 771.836,-565.162 777.931,-568.604\"/>\n",
       "<text text-anchor=\"middle\" x=\"784\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 52 -->\n",
       "<g id=\"node51\" class=\"node\"><title>52</title>\n",
       "<text text-anchor=\"middle\" x=\"1040.5\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">52 (UV)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;52 -->\n",
       "<g id=\"edge50\" class=\"edge\"><title>50&#45;&gt;52</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M794.267,-610.999C852.857,-594.343 946.184,-567.812 999.53,-552.647\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1000.66,-555.963 1009.33,-549.862 998.75,-549.23 1000.66,-555.963\"/>\n",
       "<text text-anchor=\"middle\" x=\"925.5\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 51 -->\n",
       "<g id=\"node52\" class=\"node\"><title>51</title>\n",
       "<text text-anchor=\"middle\" x=\"1040.5\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">51 (of)</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;51 -->\n",
       "<g id=\"edge51\" class=\"edge\"><title>52&#45;&gt;51</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1040.5,-521.799C1040.5,-510.163 1040.5,-494.548 1040.5,-481.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1044,-481.175 1040.5,-471.175 1037,-481.175 1044,-481.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"1052.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 53 -->\n",
       "<g id=\"node53\" class=\"node\"><title>53</title>\n",
       "<text text-anchor=\"middle\" x=\"1209.5\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">53 (signaling)</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;53 -->\n",
       "<g id=\"edge52\" class=\"edge\"><title>52&#45;&gt;53</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1071.89,-523.211C1098.51,-509.825 1136.94,-490.493 1166.43,-475.661\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1168.31,-478.636 1175.67,-471.016 1165.16,-472.383 1168.31,-478.636\"/>\n",
       "<text text-anchor=\"middle\" x=\"1143.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">acl</text>\n",
       "</g>\n",
       "<!-- 56 -->\n",
       "<g id=\"node54\" class=\"node\"><title>56</title>\n",
       "<text text-anchor=\"middle\" x=\"1499.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">56 (lizards)</text>\n",
       "</g>\n",
       "<!-- 53&#45;&gt;56 -->\n",
       "<g id=\"edge53\" class=\"edge\"><title>53&#45;&gt;56</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1255.77,-438.438C1308.91,-422.862 1395.75,-397.41 1450.15,-381.465\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1451.3,-384.774 1459.92,-378.602 1449.33,-378.056 1451.3,-384.774\"/>\n",
       "<text text-anchor=\"middle\" x=\"1387.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 54 -->\n",
       "<g id=\"node55\" class=\"node\"><title>54</title>\n",
       "<text text-anchor=\"middle\" x=\"1449.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">54 (in)</text>\n",
       "</g>\n",
       "<!-- 56&#45;&gt;54 -->\n",
       "<g id=\"edge54\" class=\"edge\"><title>56&#45;&gt;54</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1489.38,-347.799C1482.33,-335.817 1472.8,-319.617 1464.83,-306.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1467.69,-304.02 1459.6,-297.175 1461.66,-307.569 1467.69,-304.02\"/>\n",
       "<text text-anchor=\"middle\" x=\"1488.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 55 -->\n",
       "<g id=\"node56\" class=\"node\"><title>55</title>\n",
       "<text text-anchor=\"middle\" x=\"1549.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">55 (free&#45;ranging)</text>\n",
       "</g>\n",
       "<!-- 56&#45;&gt;55 -->\n",
       "<g id=\"edge55\" class=\"edge\"><title>56&#45;&gt;55</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1509.62,-347.799C1516.67,-335.817 1526.2,-319.617 1534.17,-306.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1537.34,-307.569 1539.4,-297.175 1531.31,-304.02 1537.34,-307.569\"/>\n",
       "<text text-anchor=\"middle\" x=\"1542\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 59 -->\n",
       "<g id=\"node57\" class=\"node\"><title>59</title>\n",
       "<text text-anchor=\"middle\" x=\"1664.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">59 (reduces)</text>\n",
       "</g>\n",
       "<!-- 56&#45;&gt;59 -->\n",
       "<g id=\"edge56\" class=\"edge\"><title>56&#45;&gt;59</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1532.89,-347.799C1558.69,-334.506 1594.58,-316.021 1622.36,-301.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1624.15,-304.721 1631.44,-297.03 1620.95,-298.498 1624.15,-304.721\"/>\n",
       "<text text-anchor=\"middle\" x=\"1613.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 58 -->\n",
       "<g id=\"node58\" class=\"node\"><title>58</title>\n",
       "<text text-anchor=\"middle\" x=\"1626.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">58 (which)</text>\n",
       "</g>\n",
       "<!-- 59&#45;&gt;58 -->\n",
       "<g id=\"edge57\" class=\"edge\"><title>59&#45;&gt;58</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1656.81,-260.799C1651.5,-248.932 1644.35,-232.928 1638.32,-219.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1641.46,-217.876 1634.18,-210.175 1635.06,-220.733 1641.46,-217.876\"/>\n",
       "<text text-anchor=\"middle\" x=\"1662.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 60 -->\n",
       "<g id=\"node59\" class=\"node\"><title>60</title>\n",
       "<text text-anchor=\"middle\" x=\"1724.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">60 (success)</text>\n",
       "</g>\n",
       "<!-- 59&#45;&gt;60 -->\n",
       "<g id=\"edge58\" class=\"edge\"><title>59&#45;&gt;60</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1676.64,-260.799C1685.18,-248.702 1696.76,-232.305 1706.38,-218.667\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1709.47,-220.364 1712.38,-210.175 1703.75,-216.327 1709.47,-220.364\"/>\n",
       "<text text-anchor=\"middle\" x=\"1710\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">dobj</text>\n",
       "</g>\n",
       "<!-- 63 -->\n",
       "<g id=\"node60\" class=\"node\"><title>63</title>\n",
       "<text text-anchor=\"middle\" x=\"1724.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">63 (acquisition)</text>\n",
       "</g>\n",
       "<!-- 60&#45;&gt;63 -->\n",
       "<g id=\"edge59\" class=\"edge\"><title>60&#45;&gt;63</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1724.5,-173.799C1724.5,-162.163 1724.5,-146.548 1724.5,-133.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1728,-133.175 1724.5,-123.175 1721,-133.175 1728,-133.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"1740.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 61 -->\n",
       "<g id=\"node61\" class=\"node\"><title>61</title>\n",
       "<text text-anchor=\"middle\" x=\"1684.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">61 (in)</text>\n",
       "</g>\n",
       "<!-- 63&#45;&gt;61 -->\n",
       "<g id=\"edge60\" class=\"edge\"><title>63&#45;&gt;61</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1716.41,-86.799C1710.82,-74.9322 1703.29,-58.9279 1696.95,-45.4488\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1700.01,-43.7333 1692.58,-36.1754 1693.67,-46.7139 1700.01,-43.7333\"/>\n",
       "<text text-anchor=\"middle\" x=\"1718.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 62 -->\n",
       "<g id=\"node62\" class=\"node\"><title>62</title>\n",
       "<text text-anchor=\"middle\" x=\"1764.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">62 (mate)</text>\n",
       "</g>\n",
       "<!-- 63&#45;&gt;62 -->\n",
       "<g id=\"edge61\" class=\"edge\"><title>63&#45;&gt;62</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1732.59,-86.799C1738.18,-74.9322 1745.71,-58.9279 1752.05,-45.4488\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1755.33,-46.7139 1756.42,-36.1754 1748.99,-43.7333 1755.33,-46.7139\"/>\n",
       "<text text-anchor=\"middle\" x=\"1775.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 69 -->\n",
       "<g id=\"node67\" class=\"node\"><title>69</title>\n",
       "<text text-anchor=\"middle\" x=\"1165.5\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">69 (of)</text>\n",
       "</g>\n",
       "<!-- 70&#45;&gt;69 -->\n",
       "<g id=\"edge66\" class=\"edge\"><title>70&#45;&gt;69</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1165.5,-608.799C1165.5,-597.163 1165.5,-581.548 1165.5,-568.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1169,-568.175 1165.5,-558.175 1162,-568.175 1169,-568.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"1177.5\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f85a747cac8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetSentence = last_sent\n",
    "try:\n",
    "    graph = graphviz.Source(list(plos_DepParse[targetSentence])[targetArticle].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'last_sent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-38a4ab7bfafc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtargetSentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_sent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplos_DepParse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargetSentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargetArticle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You likely have to rerun the depParses\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'last_sent' is not defined"
     ]
    }
   ],
   "source": [
    "targetSentence = last_sent\n",
    "try:\n",
    "    graph = graphviz.Source(list(plos_DepParse[targetSentence])[targetArticle].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am interested in how scientific findings are represented in articles, in particular the means by which scientists articulate the credibility of their findings. My intution is that there are a few varied strategies that can be deployed and that they depend upon different linguistic strategies. In the context of the above analysis I think that it is important to examine how particular verbs such as demonstrates, reveals, establishes, are modified by adverbs. Also it might be interesting to examine how the investigation is represented in the nouns the scientists chooses, from say 'this investigation' vs. 'this study' or 'this analysis'. There are further pieces of information that are needed to corroborate and variance that might be important in when and where such particular words are  usde and their implications for the representation of scientific findings. \n",
    "\n",
    "For the dependecy trees I wanted to look at the last sentences of the articles, figuring these would be spots where the authors would make an effort to summarize the evidence in as convincing way as possible. The senteces are naturally complex, at least visually in the dependecy parsing graph. To the end of understanding how scientific findings are represented, and here I am referring to the first graph above, I think it will be important to identify the ways in which the chief mechanism the author's are studying is represented. In the first graph the authors refer to an 'effect', modifying it in a number of conservative ways before representing the effect they are referring to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction\n",
    "\n",
    "Information extraction approaches typically (as here, with Stanford's Open IE engine) ride atop the dependency parse of a sentence. They are a pre-coded example of the type analyzed in the prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.8 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 9.546 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [11.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0318 seconds]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(text)stanford.startCoreServer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openIE()` prints everything stanford core produces and we can see from looking at it that initializing the dependency parser takes most of the time, so calling the function will always take at least 12 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [certainty, subject, verb, object]\n",
       "Index: []"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No buffalos (because there were no verbs), but the rest is somewhat promising. Note, however, that it abandoned the key theme of the sentence about the tragic Trayvon Martin death (\"fatally shot\"), likely because it was buried so deeply within the complex phrase structure. This is obviously a challenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">How would you extract relevant information about the Trayvon Martin sentence directly from the dependency parse (above)? Code an example here. (For instance, what compound nouns show up with what verb phrases within the sentence?) How could these approaches inform your research project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither the openIE method or the Stanford.StarCoreServer link worked for me. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also look for subject, object, target triples in one of the reddit stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.9 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 13.419 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [15.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0355 seconds]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(redditTopScores['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [certainty, subject, verb, object]\n",
       "Index: []"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's almost 200 triples in only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(redditTopScores['sentences'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(s) for s in redditTopScores['sentences'][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find at the most common subject in this story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieDF['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I is followed by various male pronouns and compound nouns (e.g., \"old man\"). 'I' occures most often with the following verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieDF[ieDF['subject'] == 'I']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieDF[ieDF['subject'] == 'I']['object'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the corenlp server. When you run this server (with the command below), you can click on the browswer link provided to experiment with it. Note that when we run the server, executing the command below, it interrupts the current jupyter process and you will not be able to run code here again (processes will \"hang\" and never finish) until you interrup the process by clicking \"Kernel\" and then \"Interrupt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server on http://10.50.221.147:16432 , please wait a few seconds\n",
      "click Kernel -> Then Interupt to stop   (･ω･｀)))                      \n",
      "Exiting (ノ≧▽≦)ノ\n"
     ]
    }
   ],
   "source": [
    "stanford.startCoreServer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:red\">*Exercise 5*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform open information extraction on a modest subset of texts relevant to your final project. Analyze the relative attachment of several subjects relative to verbs and objects and visa versa. Describe how you would select among these statements to create a database of high-value statements for your project and then do it by extracting relevant statements into a pandas dataframe."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
